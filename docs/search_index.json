[
["index.html", "Science des données biologiques 2 Préambule", " Science des données biologiques 2 Philippe Grosjean &amp; Guyliann Engels 2020-03-09 Préambule Cet ouvrage interactif est le second d’une série de trois ouvrages traitant de la science des données biologiques. L’écriture de cette suite de livres a débuté au cours de l’année académique 2018-2019. Pour l’année académique 2019-2020, cet ouvrage interactif sera le support du cours suivant : Science des données II : Analyse et modélisation, UMONS dont le responsable est Grosjean Philippe Cet ouvrage est conçu pour être utilisé de manière interactive en ligne. En effet, nous y ajoutons des vidéos, des démonstrations interactives, et des exercices sous forme de questionnaires interactifs également. Ces différents éléments ne sont, bien évidemment, utilisables qu’en ligne. Le matériel dans cet ouvrage est distribué sous licence CC BY-NC-SA 4.0. "],
["vue-generale-des-cours.html", "Vue générale des cours", " Vue générale des cours Le cours de Science des données II: analyse et modélisation est dispensé aux biologistes de troisième Bachelier en Faculté des Sciences de l’Université de Mons à partir de l’année académique 2019-2020. La matière est divisée en huit modules de 6h chacun en présentiel. Il nécessitera environ un tiers de ce temps (voir plus, en fonction de votre rythme et de votre technique d’apprentissage) en travail à domicile. Cette matière fait suite au premier cours dont le contenu est considéré comme assimilé (voir https://biodatascience-course.sciviews.org/sdd-umons/). La première moitié du cours est consacrée à la modélisation, un domaine particulièrement important de la science des données qui étend les concepts déjà vu au cours 1 d’analyse de variance et de corrélation entre deux variables. Ces quatre modules formeront aussi un socle sur lequel nous pourrons élaborer les techniques d’apprentissage machine (classification supervisée), et puis ensuite l’apprentissage profond à la base de l’intelligence artificielle qui seront abordées plus tard dans le cours 3. Cette partie est dense, mais ultra importante ! La seconde moitié s’intéressera à l’exploration des données, encore appelée analyse des données qui vise à découvrir des caractéristiques intéressantes dans des très gros jeux de données. Ces techniques sont d’autant plus utiles que les données volumineuses deviennent de plus en plus courantes en biologie. "],
["materiel-pedagogique.html", "Matériel pédagogique", " Matériel pédagogique Le matériel pédagogique, rassemblé dans ce syllabus interactif est aussi varié que possible. Vous pourrez ainsi piocher dans l’offre en fonction de vos envies et de votre profil d’apprenant pour optimiser votre travail. Vous trouverez: le présent ouvrage en ligne, des tutoriaux interactifs (réalisés avec un logiciel appelé learnr). Vous pourrez exécuter ces tutoriaux directement sur votre ordinateur, et vous aurez alors accès à des pages Web réactives contenant des explications, des exercices et des quizzs en ligne, des slides de présentations, des dépôts Github Classroom dans la section BioDataScience-Course pour réaliser et documenter vos travaux personnels. des renvois vers des documents externes en ligne, types vidéos youtube ou vimeo, des ouvrages en ligne en anglais ou en français, des blogs, des tutoriaux, des parties gratuites de cours Datacamp ou équivalents, des questions sur des sites comme “Stackoverflow” ou issues des “mailing lists” R, … Tout ce matériel est accessible à partir du site Web du cours, du présent syllabus interactif (et de Moodle pour les étudiants de l’UMONS). Ces derniers ont aussi accès au dossier SDD sur StudentTemp en Intranet à l’UMONS. Les aspects pratiques seront à réaliser en utilisant la ‘SciViews Box’, une machine virtuelle préconfigurée. Nous installerons ensemble la nouvelle version de cette SciViews Box au premier cours. Il est donc très important que vous soyez présent à ce cours, et vous pouvez venir aussi si vous le souhaitez avec votre propre ordinateur portable comme pour le cours 1. Enfin, vous pourrez poser vos questions par mail à l’adresse sdd@sciviews.org. System information sessioninfo::session_info() # ─ Session info ────────────────────────────────────────────────────────── # setting value # version R version 3.5.3 (2019-03-11) # os Ubuntu 18.04.2 LTS # system x86_64, linux-gnu # ui X11 # language (EN) # collate en_US.UTF-8 # ctype en_US.UTF-8 # tz Europe/Brussels # date 2020-03-09 # # ─ Packages ────────────────────────────────────────────────────────────── # package * version date lib source # assertthat 0.2.1 2019-03-21 [2] CRAN (R 3.5.3) # bookdown 0.9 2018-12-21 [2] CRAN (R 3.5.3) # cli 1.1.0 2019-03-19 [2] CRAN (R 3.5.3) # colorspace 1.4-1 2019-03-18 [2] CRAN (R 3.5.3) # crayon 1.3.4 2017-09-16 [2] CRAN (R 3.5.3) # digest 0.6.18 2018-10-10 [2] CRAN (R 3.5.3) # dplyr 0.8.0.1 2019-02-15 [2] CRAN (R 3.5.3) # evaluate 0.13 2019-02-12 [2] CRAN (R 3.5.3) # farver 1.1.0 2018-11-20 [2] CRAN (R 3.5.3) # gganimate 1.0.3 2019-04-02 [2] CRAN (R 3.5.3) # ggplot2 3.1.1 2019-04-07 [2] CRAN (R 3.5.3) # glue 1.3.1 2019-03-12 [2] CRAN (R 3.5.3) # gtable 0.3.0 2019-03-25 [2] CRAN (R 3.5.3) # hms 0.4.2 2018-03-10 [2] CRAN (R 3.5.3) # htmltools 0.3.6 2017-04-28 [2] CRAN (R 3.5.3) # inline 0.3.15 2018-05-18 [2] CRAN (R 3.5.3) # knitr 1.23 2019-12-04 [1] Github (yihui/knitr@b5bf077) # lazyeval 0.2.2 2019-03-15 [2] CRAN (R 3.5.3) # magick 2.0 2018-10-05 [2] CRAN (R 3.5.3) # magrittr 1.5 2014-11-22 [2] CRAN (R 3.5.3) # munsell 0.5.0 2018-06-12 [2] CRAN (R 3.5.3) # pillar 1.3.1 2018-12-15 [2] CRAN (R 3.5.3) # pkgconfig 2.0.2 2018-08-16 [2] CRAN (R 3.5.3) # plyr 1.8.4 2016-06-08 [2] CRAN (R 3.5.3) # prettyunits 1.0.2 2015-07-13 [2] CRAN (R 3.5.3) # progress 1.2.0 2018-06-14 [2] CRAN (R 3.5.3) # purrr 0.3.2 2019-03-15 [2] CRAN (R 3.5.3) # R6 2.4.0 2019-02-14 [2] CRAN (R 3.5.3) # Rcpp 1.0.1 2019-03-17 [2] CRAN (R 3.5.3) # rlang 0.3.4 2019-04-07 [2] CRAN (R 3.5.3) # rmarkdown 1.12 2019-03-14 [2] CRAN (R 3.5.3) # rstudioapi 0.10 2019-03-19 [2] CRAN (R 3.5.3) # scales 1.0.0 2018-08-09 [2] CRAN (R 3.5.3) # sessioninfo 1.1.1 2018-11-05 [2] CRAN (R 3.5.3) # stringi 1.4.3 2019-03-12 [2] CRAN (R 3.5.3) # stringr 1.4.0 2019-02-10 [2] CRAN (R 3.5.3) # tibble 2.1.1 2019-03-16 [2] CRAN (R 3.5.3) # tidyselect 0.2.5 2018-10-11 [2] CRAN (R 3.5.3) # tweenr 1.0.1 2018-12-14 [2] CRAN (R 3.5.3) # withr 2.1.2 2018-03-15 [2] CRAN (R 3.5.3) # xfun 0.6 2019-04-02 [2] CRAN (R 3.5.3) # yaml 2.2.0 2018-07-25 [2] CRAN (R 3.5.3) # # [1] /home/sv/R/x86_64-pc-linux-gnu-library/3.5 # [2] /usr/local/lib/R/site-library # [3] /usr/lib/R/site-library # [4] /usr/lib/R/library "],
["lm.html", "Module 1 Régression linéaire I", " Module 1 Régression linéaire I Objectifs Retrouver ses marques avec R, RStudio et la SciViews Box et découvrir les fonctions supplémentaires de la nouvelle version. Découvrir la régression linaire de manière intuitive. Découvrir les outils de diagnostic de la régression linéaire, en particulier l’analyse des résidus. Prérequis Avant de nous lancer tête baissée dans de la matière nouvelle, nous allons installer la dernière version de la SciViews Box. Une nouvelle version est disponible chaque année début septembre. Reportez-vous à l’appendice A pour son installation et pour la migration éventuelle de vos projets depuis la version précédente. Une fois la box installée, consacrez un petit quart d’heure à repérer les icônes nouvelles dans le Dock et dans le menu Applications. Vous retrouverez R et RStudio, mais dans des version plus récentes qui apportent également leur lot de nouveautés. Lancez RStudio et repérez ici aussi les nouveaux onglets et les nouvelles entrées de menu. Aidez-vous de l’aide en ligne ou de recherches sur le net pour vous familiariser avec ces nouvelles fonctionnalités. Une fois la nouvelle SciViews Box fonctionnelle sur votre ordinateur, vous allez réaliser une séance d’exercice couvrant les points essentiels des notions abordées dans le livre science des données biologiques partie 1, histoire de rafraîchir vos connaissances. Les tutoriaux learnr auxquels vous êtes maintenant habitués seront là pour vous aider à auto-évaluer votre progression. Pour le cours 2, ces tutoriaux sont dans le package BioDataScience2 que vous venez normalement d’installer si vous avez bien suivi toutes les instructions de configuration de votre SciViews Box (sinon, vérifiez votre configuration). Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant les bases de R : BioDataScience2::run(&quot;01a_rappel&quot;) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel dans la console R. "],
["modele.html", "1.1 Modèle", " 1.1 Modèle Qu’est-ce qu’un “modèle” en science des données et en statistique ? Il s’agit d’une représentation simplifiée sous forme mathématique du mécanisme responsable de la distribution des observations. Rassurez-vous, dans ce module, le côté mathématique du problème sera volontairement peu développé pour laisser une large place à une compréhension intuitive du modèle. Les seules notions clés à connaître ici concernent l’équation qui définit une droite quelconque dans le plan \\(xy\\) : \\[y = a \\ x + b\\] Cette équation comporte : deux variables \\(x\\) et \\(y\\) qui sont matérialisées par les axes des abscisses et des ordonnées dans le plan \\(xy\\). Ces variables prennent des valeurs bien définies pour les observations réalisées sur chaque individu du jeu de données. deux paramètres \\(a\\) et \\(b\\), respectivement la pente de la droite (\\(a\\)) et son ordonnée à l’origine (\\(b\\)). Ecrit de la sorte, \\(a\\) et \\(b\\) peuvent prendre n’importe quelle valeur et l’équation définit de manière généraliste toutes les droites possibles qui existent dans le plan \\(xy\\). Paramétrer ou paramétriser le modèle consiste à définir une et une seule droite en fixant les valeurs de \\(a\\) et de \\(b\\). Par exemple, si je décide de fixer \\(a = 0.35\\) et \\(b = -1.23\\), mon équation définit maintenant une droite bien précise dans le plan \\(xy\\) : \\[y = 0,35 \\ x - 1.23\\] La distinction entre variable et paramètre dans les équations précédentes semble difficile pour certaines personnes. C’est pourtant crucial de pouvoir le faire pour bien comprendre la suite. Alors, c’est le bon moment de relire attentivement ce qui est écrit ci-dessus et de le mémoriser avant d’aller plus avant ! 1.1.1 Pourquoi modéliser ? Le but de la modélisation consiste à découvrir l’équation mathématique de la droite (ou plus généralement, de la fonction) qui décrit au mieux la forme du nuage de points matérialisant les observations dans le plan \\(xy\\) (ou plus généralement dans un hyper-espace représenté par les différentes variables mesurées). Cette équation mathématique peut ensuite être utilisée de différentes façons, toutes plus utiles les unes que les autres : Aide à la compréhension du mécanisme sous-jacent qui a généré les données. Par exemple, si une droite représente bien la croissance pondérale d’un organisme dans le plan représenté par le logarithme du poids (P) en ordonnée et le temps (t) en abscisse, nous pourrons déduire que la croissance de cet organisme est probablement un mécanisme de type exponentiel (puisqu’une transformation inverse, c’est-à-dire logarithmique, linéarise alors le nuage de points). Attention ! Le modèle n’est pos le mécanisme sous-jacent de génération des données, mais utilisé habilement, ce modèle peut donner des indices utiles pour aider à découvrir ce mécanisme. Effectuer des prédictions. Le modèle paramétré pourra être utilisé pour prédire, par exemple, le poids probable d’un individu de la même population après un certain laps de temps. Comparer différents modèles. En présence de plusieurs populations, nous pourrons ajuster un modèle linéaire pour chacune d’elles et comparer ensuite les pentes des droites pour déterminer quelle population a le meilleur ou le moins bon taux de croissance. Explorer les relations entre variables. Sans aucunes connaissances sur le contexte qui a permit d’obtenir nos données, un modèle peut fournir des informations utiles pour orienter les recherches futures. Idéalement, un modèle devrait pouvoir servir à ces différentes applications. En pratique, comme le modèle est forcément une simplification de la réalité, des compromis doivent être concédés pour arriver à cette simplification. En fonction de son usage, les compromis possibles vont différer. Il s’en suit une spécialisation des modèles en modèles mécanistiques qui décrivent particulièrement bien le mécanisme sous-jacent (fréquents en physique, par exemple), les modèles prédictifs conçus pour calculer des nouvelles valeurs (que l’intelligence artificielle affectionne particulièrement), les modèles comparatifs, et enfin, les modèles exploratroires (utilisés dans la phase initiale de découverte et de description des données). Retenez simplement qu’un même modèle est rarement efficace sur les quatre tableaux simultanément. 1.1.2 Quand modéliser ? A chaque fois que deux ou plusieurs variables (quantitatives dans le cas de la régression) forment un nuage de points qui présente une forme particulière non sphérique, autrement dit, qu’une corrélation significative existe dans les données, un modèle peut être utile. Etant donné deux variables quantitatives, trois niveaux d’association de force croissante peuvent être définies entre ces deux variables : La corrélation quantifie juste l’allongement dans une direction préférentielle du nuage de points à l’aide des coefficients de corrélation linéaire de Pearson ou non linéaire de Spearman. Ce niveau d’association a été traité dans le module 12 du cours 1. Il est purement descriptif et n’implique aucunes autres hypothèses sur les données observées. La relation considère que la corrélation observée entre les deux variables est issue d’un mécanisme sous-jacent qui nous intéresse. Un modèle mathématique de l’association entre les deux variables matérialise de manière éventuellement simplifiée, ce mécanisme. Il permet de réaliser ensuite des calculs utiles. Nous verrons plus loin que des contraintes plus fortes doivent être supposées concernant le distribution des deux variables. La causalité précise encore le mécanisme sous-jacent dans le sens qu’elle exprime le fait que c’est la variation de l’une de ces variables qui est directement ou indirectement la cause de la variation de la seconde variable. Bien que des outils statistiques existent pour inférer une causalité (nous ne les aborderons pas dans ce cours), la causalité est plutôt étudiée via l’expérimentation : le biologiste contrôle et fait varier la variable supposée causale, toutes autres conditions par ailleurs invariables dans l’expérience. Il mesure alors et constate si la seconde variable répond ou non à ces variations1 et en déduit une causalité éventuelle. La distinction entre ces trois degrés d’association de deux variables est cruciale. Il est fréquent d’observer une confusion entre corrélation (ou relation) et causalité chez ceux qui ne comprennent pas bien la différence. Cela peut mener à des interprétations complètement erronées ! Comme ceci est à la fois crucial mais subtil, voici une vidéo issue de la série “les statistiques expliquées à mon chat” qui explique clairement le problème. Une troisième variable confondante peut en effet expliquer une corrélation, rendant alors la relation et/ou la causalité entre les deux variables fallacieuse… 1.1.3 Entraînement et confirmation En statistique, une règle universelle veut qu’une observation ne peut servir qu’une seule fois. Ainsi, toutes les données utilisées pour calculer le modèle ne peuvent pas servir simultanément à la confirmer. Il faut échantillonner d’autres valeurs pour effectuer cette confirmation. Il s’en suit une spécialisation des jeux de données en : jeu d’entraînement qui sert à établir le modèle jeu de confirmation ou de test qui sert à vérifier que le modèle est génaralisable car il est capable de prédire le comportement d’un autre jeu de données indépendant issu de la même population statistique. C’est une pratique cruciale de toujours confirmer son modèle, et donc, de prendre soin de séparer ses données en jeu d’entraînement et de test. Les bonnes façons de faire cela seront abordées au cours 3 dans la partie consacrée à l’apprentissage machine. Ici, nous nous focaliserons uniquement sur l’établissement du modèle dans la phase d’entraînement. Par conséquent, nous utiliserons toutes nos données pour cet entraînement, mais qu’il soit d’emblée bien clair qu’une confirmation du modèle est une seconde phase également indispensable. En biologie, le vivant peut être étudié essentiellement de deux manières complémentaires : par l’observation du monde qui nous entoure sans interférer, ou le moins possible, et par l’expérimentation où le biologiste fixe alors très précisément les conditions dans lesquelles il étudie ses organismes cibles. Les deux approches se prêtent à la modélisation mais seule l’expérimentation permet d’inférer avec certitude la causalité.↩ "],
["regression-lineaire-simple.html", "1.2 Régression linéaire simple", " 1.2 Régression linéaire simple Nous allons découvrir les bases de la régression linéaire de façon intuitive. Nous utilisons le jeu de données trees qui rassemble la mesure du diamètre, de la hauteur et du volume de bois de cerisiers noirs. # importation des données trees &lt;- read(&quot;trees&quot;, package = &quot;datasets&quot;, lang = &quot;fr&quot;) Rapellons-nous que dans le chapitre 12 du livre science des données 1, nous avons étudié l’association de deux variables quantitatives (ou numériques). Nous utilisons donc une matrice de corrélation afin de mettre en évidence la corrélation entre nos trois variables qui composent le jeu de donnée trees. La fonction correlation() nous renvoie un tableau de la matrice de correlation avec l’indice de Pearson (corrélation linéaire) par défaut. C’est précisement ce coefficient qui nous intéresse dans le cadre d’une régression linéaire comme description préalable des données autant que pour nous guider dans le choix de nos variables. (trees_corr &lt;- correlation(trees)) # Matrix of Pearson&#39;s product-moment correlation: # (calculation uses everything) # diameter height volume # diameter 1.000 0.519 0.967 # height 0.519 1.000 0.597 # volume 0.967 0.597 1.000 Nous pouvons également observer cette matrice sous la forme d’un graphique plus convivial. plot(trees_corr, type = &quot;lower&quot;) Cependant, n’oubliez pas qu’il est indispensable de visualiser les nuages de points pour ne pas tomber dans le piège mis en avant par le jeu de données artificiel appelé “quartet d’Anscombe” qui montre très bien comment des données très différentes peuvent avoir même moyenne, même variance et même coefficient de corrélation. Un graphique de type matrice de nuages de points est tout indiqué ici. GGally::ggscatmat(as.data.frame(trees), 1:3) Nous observons une plus forte corrélation linéaire entre le volume et le diamètre. Intéressons nous à cette association. chart(trees, volume ~ diameter) + geom_point() Si vous deviez ajouter une droite permettant de représenter au mieux les données, où est ce que vous la placeriez ? Pour rappel, une droite respecte l’équation mathématique suivante : \\[y = a \\ x + b\\] dont a est la pente (slope en anglais) et b est l’ordonnée à l’origine (intercept en anglais). # Sélection de pentes et d&#39;ordonnées à l&#39;origine models &lt;- tibble( model = paste(&quot;model&quot;, 1:4, sep = &quot;-&quot;), slope = c(5, 5.5, 6, 0), intercept = c(-0.5, -0.95, -1.5, 0.85) ) chart(trees, volume ~ diameter) + geom_point() + geom_abline(data = models, aes(slope = slope, intercept = intercept, color = model)) + labs( color = &quot;Modèle&quot;) Nous avons quatre droites candidates pour représenter au mieux les observations. Quel est la meilleure d’entre elles selon vous ? 1.2.1 Quantifier l’ajustement d’un modèle Nous voulons identifier la meilleure régression, c’est-à-dire la régression le plus proche de nos données. Nous avons besoin d’une règle qui va nous permettre de quantifier la distance de nos observations à notre modèle afin d’obtenir la régression avec la plus faible distance possible de l’ensemble de nos observations. Décomposons le problème étape par étape et intéressons nous au model-1 (droite en rouge sur le graphique précédent). Calculer les valeurs de \\(y_i\\) prédites par le modèle que nous noterons par convention \\(\\hat y_i\\) (prononcez “y chapeau” ou “y hat” en anglais) pour chaque observation \\(i\\). # Calculer la valeur de y pour chaque valeur de x suivant le model souhaité # Création de notre fonction model &lt;- function(slope, intercept, x) { prediction &lt;- intercept + slope * x attributes(prediction) &lt;- NULL prediction } # Application de notre fonction yhat &lt;- model(slope = 5, intercept = -0.5, x = trees$diameter) # Affichage des résultats yhat # [1] 0.555 0.590 0.620 0.835 0.860 0.870 0.895 0.895 0.910 0.920 0.935 # [12] 0.950 0.950 0.985 1.025 1.140 1.140 1.190 1.240 1.255 1.280 1.305 # [23] 1.340 1.530 1.570 1.695 1.720 1.775 1.785 1.785 2.115 Calculer la distance entre les observations \\(y_i\\) et les prédictions par notre modèle \\(\\hat y_i\\), soit \\(y_i - \\hat y_i\\) Les distances que nous souhaitons calculer, sont appelées les résidus du modèle et sont notés \\(\\epsilon_i\\) (epsilon). Nous pouvons premièrement visualiser ces résidus graphiquement (ici en rouge par rapport à model-1) : Nous pouvons ensuite facilement calculer leurs valeurs comme ci-dessous : # Calculer la distance entre y et y barre # Création de notre fonction de calcul des résidus distance &lt;- function(observations, predictions) { residus &lt;- observations - predictions attributes(residus) &lt;- NULL residus } # Utilisation de la fonction resid &lt;- distance(observations = trees$volume, predictions = yhat) # Impression des résultats resid # [1] -0.263 -0.298 -0.331 -0.371 -0.328 -0.312 -0.453 -0.380 -0.270 -0.357 # [11] -0.250 -0.355 -0.344 -0.382 -0.484 -0.511 -0.183 -0.414 -0.512 -0.550 # [21] -0.303 -0.407 -0.312 -0.445 -0.364 -0.126 -0.143 -0.124 -0.327 -0.341 # [31] 0.065 Définir une règle pour obtenir une valeur unique qui résume l’ensemble des distances de nos observations par rapport aux prédictions du modèle. Une première idée serait de sommer l’ensemble de nos résidus comme ci-dessous : sum(resid) # [1] -10.175 Appliquons ces calculs sur nos quatre modèles afin de les comparer… Le modèle pour lequel notree critère serait le plus proche de zéro serait alors considéré comme le meilleur. model-1 model-2 model-3 model-4 -10.175 -1.441 10.393 0.135 Selon notre méthode, il en ressort que le modèle 4 est le plus approprié pour représenter au mieux nos données. Qu’en pensez vous ? Intuitivement, nous nous aperçevons que le modèle 4 est loin d’être le meilleur. Nous pouvons en déduire que la somme des résidus n’est pas un bon critère pour ajuster un modèle linéaire. Le problème lorsque nous sommons des résidus, est la présence de résidus positifs et de résidus négatifs (ici par rapport à model-4). Ainsi avec notre première méthode naïve de somme des résidus, il suffit d’avoir autant de résidus positifs que négatifs pour avoir un résultat proche de zéro. Mais cela n’implique pas que les observations soient prochent de la droite pour autant. Avez-vous une autre idée que de sommer les résidus ? Sommer le carré des résidus aurait des propriétés intéressantes car d’une part les carrés de nombres positifs et négatifs sont tous positifs, et d’autre part, plus une observation est éloignée plus sa distance au carré pèse fortement dans la somme2. Nous obtenons les résultats suivants : model-1 model-2 model-3 model-4 3.842211 0.4931095 3.929195 6.498511 Sommer les valeurs absolues des résidus mène également à des contributions toutes positives, mais sans pénaliser outre mesure les observations les plus éloignées. Nous obtenons les résultats suivants : model-1 model-2 model-3 model-4 10.305 3.186 10.393 11.525 Nous avons trouvé deux solutions intéressantes pour quantifier la distance de notre droite par rapport à nos observations. En effet, dans les deux cas, la valeur minimale est obtenue pour le model-2 (en vert sur le graphique) qui est visuellement le meilleur des quatre. La méthode utilisant les carrés des résidus s’appelle une régression par les moindres carrés. Notre objectif est donc de trouver les meilleures valeurs des paramètres \\(a\\) et \\(b\\) de la droite pour minimiser ce critère. Il en résulte une fonction dite objective qui dépend de \\(x\\) et de nos paramètres \\(a\\) et \\(b\\) à minimiser. Cette approche s’appelle la régression par les moindres carrés et elle est la plus utilisée. L’approche utilisant la somme de la valeur absolue des résidus est également utilisable (et elle est d’ailleurs préférable en présence de valeurs extrêmes potentiellement suspectes). Elle s’apppelle régression par la médiane, un cas particulier de la régression quantile, une approche intéressante dans le cas de non normalité des résidus et/ou de présence de valeurs extrêmes suspectes. 1.2.2 Trouver la meilleure droite Essayez de trouver le meilleur modèle par vous-même dans une application interactive “shiny”. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir l’application : BioDataScience2::app(&quot;01a_lin_mod&quot;) # TODO Méthode alternative : shiny::runApp(system.file(&quot;shiny/01a_lin_mod&quot;, package = &quot;BioDataScience2&quot;)) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R lorsque vous aurez fini avec l’application shiny. Nous pouvons nous demander si notre modèle 2 qui est la meilleure droite de nos quatre modèles est le meilleur modèle possible dans l’absolu. Pour se faire nous allons devoir définir unez technique d’optimisation qui nous permet de déterminer quelle est la droite qui minimise notre fonction objective. Dans la suite, nous garderonsq le critère des moindres carrés (des résidus). Imaginons que nous n’avons pas quatre mais 5000 modèles linéaires avec des pentes et des ordonnées à l’origine différentes. Quelle est la meilleure droite ? set.seed(34643) models1 &lt;- tibble( intercept = runif(5000, -5, 4), slope = runif(5000, -5, 15)) chart(trees, volume ~ diameter) + geom_point() + geom_abline(aes(intercept = intercept, slope = slope), data = models1, alpha = 1/6) Nous voyons sur le graphique qu’un grand nombre de droites différentes sont testées, mais nous ne distinguons pas grand chose de plus. Cependant, sur ces 5000 modèles, nous pouvons maintenant calculer la somme des carrés des résidus et ensuite déterminer quel est le meilleur d’entre eux. # Fonction de calcul de la somme des carrés des résidus measure_distance &lt;- function(slope, intercept, x, y) { ybar &lt;- x * slope + intercept resid &lt;- y - ybar sum(resid^2) } # Test de la fonction #measure_distance(slope = 0, intercept = 0.85, x = trees$diameter, y = trees$volume) # Fonction adaptée pour être employé avec purrr:map() pour distribuer le calcul trees_dist &lt;- function(intercept, slope) { measure_distance(slope = slope, intercept = intercept, x = trees$diameter, y = trees$volume) } models1 &lt;- models1 %&gt;% mutate(dist = purrr::map2_dbl(intercept, slope, trees_dist)) Si nous réalisons un graphique de valeurs de pentes, d’ordonnées à l’origine et de la valeur de la fonction objective (distance) en couleur, nous obtenons le graphique ci-dessous. plot &lt;- chart(models1, slope ~ intercept %col=% dist) + geom_point() + geom_point(data = filter(models1, rank(dist) &lt;= 10), shape = 1, color = &#39;red&#39;, size = 3) + labs( y = &quot;Pente&quot;, x = &quot;Ordonnée à l&#39;origine&quot;, color = &quot;Distance&quot;) + scale_color_viridis_c(direction = -1) plotly::ggplotly(plot) Les 10 valeurs les plus faibles sont mises en évidence sur le graphique par des cercles rouges. Le modèle optimal que nous recherchons se trouve dans cette région. best_models &lt;- models1 %&gt;.% filter(., rank(dist) &lt;= 10) Nous pouvons afficher les 10 meilleurs modèles sur notre graphique : chart(trees, volume ~ diameter) + geom_abline(data = best_models, aes(slope = slope, intercept = intercept, color = dist), alpha = 3/4) + geom_point() + labs(color = &quot;Distance&quot;) + scale_color_viridis_c(direction = -1) En résumé, nous avons besoin d’une fonction qui calcule la distance d’un modèle par rapport à nos observations et d’un algorithme pour la minimiser. Il n’est cependant pas nécessaire de chercher pendant des heures la meilleure fonction et le meilleur algorithme. Il existe dans R, un outil spécifiquement conçu pour adapter des modèles linéaires sur des observations, la fonction lm(). Dans le cas particulier de la régression par les moindres carrés, la solution s’obtient très facilement par un simple calcul : \\[a = \\frac{cov_{x, y}}{var_x} \\ \\ \\ \\textrm{et} \\ \\ \\ b = \\bar y - a \\ \\bar x\\] où \\(\\bar x\\) et \\(\\bar y\\) sont les moyennes pour les deux variables, \\(cov\\) est la covariance et \\(var\\) est la variance. Vous avez à votre disposition des snippets dédiés aux modèles linéaires (tapez ..., ensuite choisissez models, ensuite models : linear et choisissez le snippet qui vous convient dans la liste. (lm. &lt;- lm(data = trees, volume ~ diameter)) # # Call: # lm(formula = volume ~ diameter, data = trees) # # Coefficients: # (Intercept) diameter # -1.047 5.652 Nous pouvons reporter ces valeurs sur notre graphique afin d’observer ce résultat par rapport à nos modèles aléatoires. nous avons en rouge la droite calculée par la fonction lm(). chart(trees, volume ~ diameter) + geom_abline(data = best_models, aes(slope = slope, intercept = intercept, color = dist), alpha = 3/4) + geom_point() + geom_abline( aes(intercept = lm.$coefficients[1], slope = lm.$coefficients[2]), color = &quot;red&quot;, size = 1.5) + labs( color = &quot;Distance&quot;) + scale_color_viridis_c(direction = -1) 1.2.3 La fonction lm() Suite à notre découverte de manière intuitive de la régression linéaire simple, nous pouvons récapituler quelques points clés : Une droite suit l’équation mathématique suivante : \\[y = a \\ x + b\\] dont \\(a\\) est la pente (slope en anglais) et \\(b\\) est l’ordonnée à l’origine (intercept en anglais), tous deux les paramètres du modèle, alors que \\(x\\) et \\(y\\) en sont les variables. La distance entre une valeur observée \\(y_i\\) et une valeur prédite \\(\\hat y_i\\) se nomme le résidu (\\(\\epsilon_i\\)) et se mesure toujours parallèlement à l’axe \\(y\\). Cela revient à considérer que toute l’erreur du modèle se situe sur \\(y\\) et non sur \\(x\\). Cela donne l’équation complète de notre modèle statistique : \\[y_i = a \\ x_i + b + \\epsilon_i\\] avec \\(y_i\\) est la valeur mesurée pour le point i sur l’axe y, \\(a\\) est la pente, \\(x_i\\) est la valeur mesurée pour le point i sur l’axe x, b est l’ordonnée à l’origine et \\(\\epsilon_i\\) les résidus. On peut montrer (nous ne le ferons pas ici pour limiter les développements mathématiques) que le choix des moindres carrés des résidus comme fonction objective revient à considérer que nos résidus suivent une distribution normale centrée autour de zéro et avec un écart type \\(\\sigma\\) constant/ : \\[\\epsilon_i \\approx N(0, \\sigma)\\] Dans le cas de la régression linéaire simple, la meilleure droite s’obtient très facilement par la minimisation de la somme des carrés des résidus. En effet, la pente \\(a = \\frac{cov_{x, y}}{var_x}\\) et l’ordonnée à l’origine \\(b = \\bar y - a \\ \\bar x\\). La fonction lm() permet de faire ce calcul très facilement dans R. # Régression linéaire lm. &lt;- lm(data = trees, volume ~ diameter) # Graphique de nos observations et de la droite obtenue avec la fonction lm() chart(trees, volume ~ diameter) + geom_point() + geom_abline( aes(intercept = lm.$coefficients[1], slope = lm.$coefficients[2]), color = &quot;red&quot;, size = 1.5) + labs( color = &quot;Modèle&quot;) + scale_color_viridis_c(direction = -1) La fonction lm() crée un objet spécifique qui contient de nombreuses informations pour pouvoir ensuite analyser notre modèle linéaire. La fonction class() permet de mettre en avant la classe de notre objet. class(lm.) # [1] &quot;lm&quot; 1.2.4 Résumé avec summary() Avec la fonction summary() nous obtenons un résumé condensé des informations les plus utiles pour interpréter notre régression linéaire. summary(lm.) # # Call: # lm(formula = volume ~ diameter, data = trees) # # Residuals: # Min 1Q Median 3Q Max # -0.231211 -0.087021 0.003533 0.100594 0.271725 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) -1.04748 0.09553 -10.96 7.85e-12 *** # diameter 5.65154 0.27649 20.44 &lt; 2e-16 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.1206 on 29 degrees of freedom # Multiple R-squared: 0.9351, Adjusted R-squared: 0.9329 # F-statistic: 417.8 on 1 and 29 DF, p-value: &lt; 2.2e-16 Call : Il s’agit de la formule employée dans la fonction lm(). C’est à dire le volume en fonction du diamètre du jeu de données trees. Residuals : La tableau nous fournit un résumé via les 5 nombres de l’ensemble des résidus (que vous pouvez récupérer à partir de lm.$residuals). fivenum(lm.$residuals) # 20 7 12 9 31 # -0.231210947 -0.087020695 0.003532709 0.100594122 0.271724973 Coefficients : Il s’agit des résultats associés à la pente et à l’ordonnée à l’origine dont les valeurs estimées des paramètres (Estimate). Les mêmes valeurs peuvent être obtenues à partir de lm.$coefficients : lm.$coefficients # (Intercept) diameter # -1.047478 5.651535 On retrouve également les écart-types calculés sur ces valeurs (Std.Error) qui donnent une indication de la précision de leur estimation, les valeurs des distibutions de Student sur les valeurs estimées (t value) et enfin les valeurs p (PR(&gt;|t|)) liées à un test de Student pour déterminer si le paramètre p correspondant est significativement différent de zéro (avec \\(H_0: p = 0\\) et \\(H_a: p \\neq 0\\)). Pour l’instant, nous nous contenterons d’interpréter et d’utiliser les informations issues de summary() dans sa partie supérieure. Le contenu des trois dernières lignes sera détaillé dans le module suivant. A partir des données de ce résumé, nous pouvons maintenant paramétrer l’équation de notre modèle : \\[y = ax + b\\] devient3 : \\[volume \\ de \\ bois = 5.65 \\ diamètre \\ à \\ 1.4 \\ m - 1.05 \\] Utiliser le carré des résidus a aussi d’autres propriétés statistiques intéressantes qui rapprochent ce calcul de la variance (qui vaut la somme de la distance au carré à la moyenne pour une seule variables numérique).↩ Lors de la paramétrisation du modèle, pensez à arrondir la valeur des paramètres à un nombre de chiffres significatifs raisonnables. Inutile de garder 5, ou même 3 chiffres derrière la virgule si vous n’avez que quelques dizaines d’obserrvations pour ajuster votre modèle.↩ "],
["outils-de-diagnostic.html", "1.3 Outils de diagnostic", " 1.3 Outils de diagnostic Une fois la meilleure droite de régression obtenue, le travail est loin d’être terminé. Il se peut que le nuage de point ne soit pas tout-à-fait linéaire, que sa dispersion ne soit pas homogène, que les résidus n’aient pas une distribution normale, qu’il existe des valeurs extrêmes aberrantes, ou qui tirent la droite vers elle de manière excessive. Nous allons maintenant devoir diagnostiquer ces possibles problèmes. L’analyse des résidus permet de le faire. Ensuite, si deux ou plusieurs modèles sont utilisable, il nous faut décider lequel conserver. Enfin, nous pouvons aussi calculer et visualiser l’enveloppe de confiance du modèle et extraire une série de données de ce modèle. 1.3.1 Analyse des résidus Le tableau numérique obtenu à l’aide de summary() peut faire penser que l’étude d’une régression linéaire se limite à quelques valeurs numériques et divers tests d’hypothèses associés. C’est un premier pas, mais c’est oublier que la technique est essentiellement visuelle. Le graphique du nuage de points avec la droite superposée est un premier outil diagnostic visuel indispensable, mais il n’est pas le seul ! Plusieurs graphiques spécifiques existent pour mettre en évidence diverses propriétés des résidus qui peuvent révéler des problèmes. Leur inspection est indispensable et s’appelle l’analyse des résidus. Les différents graphiques sont faciles à obtenir à partir des snippets. Le premier de ces graphique permet de vérifier la distribution homogène des résidus. Dans une bonne régression, nous aurons une distribution équilibrée de part et d’autre du zéro sur l’axe Y. Que pensez-vous de notre graphique d’anayse des résidus ? Nous avons une valeur plus éloignée du zéro qui est mise en avant par la courbe en bleu qui montre l’influence générale des résidus. #plot(lm., which = 1) lm. %&gt;.% chart(broom::augment(.), .resid ~ .fitted) + geom_point() + geom_hline(yintercept = 0) + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = &quot;Residuals&quot;) + ggtitle(&quot;Residuals vs Fitted&quot;) Le second graphique permet de vérifier la normalité des résidus (comparaison par graphique quantile-quantile à une distribution normale). #plot(lm., which = 2) lm. %&gt;.% chart(broom::augment(.), aes(sample = .std.resid)) + geom_qq() + geom_qq_line(colour = &quot;darkgray&quot;) + labs(x = &quot;Theoretical quantiles&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Normal Q-Q&quot;) Le troisième graphique va standardiser les résidus, et surtout, en prendre la racine carrée. Cela a pour effet de superposer les résidus négatifs sur les résidus positifs. Nous y diagnostiquons beaucoup plus facilement des problèmes de distribution de ces résidus. A nouveau, nous pouvons observer qu’une valeur influence fortement la régression. #plot(lm., which = 3) lm. %&gt;.% chart(broom::augment(.), sqrt(abs(.std.resid)) ~ .fitted) + geom_point() + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = expression(bold(sqrt(abs(&quot;Standardized residuals&quot;))))) + ggtitle(&quot;Scale-Location&quot;) Le quatrième graphique met en évidence l’influence des individus sur la régression linéaire. Effectivement, la régression linéaire est sensible aux valeurs extrêmes. Nous pouvons observer que nous avons une valeur qui influence fortement notre régression. On utilise pour ce faire la distance de Cook que nous ne détaillerons pas dans le cadre de ce cours. #plot(lm., which = 4) lm. %&gt;.% chart(broom::augment(.), .cooksd ~ seq_along(.cooksd)) + geom_bar(stat = &quot;identity&quot;) + geom_hline(yintercept = seq(0, 0.1, by = 0.05), colour = &quot;darkgray&quot;) + labs(x = &quot;Obs. number&quot;, y = &quot;Cook&#39;s distance&quot;) + ggtitle(&quot;Cook&#39;s distance&quot;) Le cinquième graphique utilise l’effet de levier (Leverage) qui met également en avant l’influence des individus sur notre régression. Il répond à la question suivante : “est-ce qu’un ou plusieurs points sont tellement influents qu’ils tirent la régression vers eux de manière abusive ?” Nous avons à nouveau une valeur qui influence fortement notre modèle. #plot(lm., which = 5) lm. %&gt;.% chart(broom::augment(.), .std.resid ~ .hat %size=% .cooksd) + geom_point() + geom_smooth(se = FALSE, size = 0.5, method = &quot;loess&quot;, formula = y ~ x) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + labs(x = &quot;Leverage&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Residuals vs Leverage&quot;) Le sixième graphique met en relation la distance de Cooks et l’effet de levier. Notre unique point d’une valeur supérieur à 0.5 m de diamètre influence fortement notre modèle. #plot(lm., which = 6) lm. %&gt;.% chart(broom::augment(.), .cooksd ~ .hat %size=% .cooksd) + geom_point() + geom_vline(xintercept = 0, colour = NA) + geom_abline(slope = seq(0, 3, by = 0.5), colour = &quot;darkgray&quot;) + geom_smooth(se = FALSE, size = 0.5, method = &quot;loess&quot;, formula = y ~ x) + labs(x = expression(&quot;Leverage h&quot;[ii]), y = &quot;Cook&#39;s distance&quot;) + ggtitle(expression(&quot;Cook&#39;s dist vs Leverage h&quot;[ii] / (1 - h[ii]))) A l’issue de l’analyse des résidus, nous abservons donc différents problèmes qui suggèrent que le modèle choisi n’est peut être pas le plus adapté. Nous comprendrons pourquoi plus loin. A vous de jouer Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant les bases de R : BioDataScience2::run(&quot;01b_reg_lin&quot;) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel dans la console R. Pièges et astuces : extrapolation Notre régression linéaire a été réalisée sur des cerisiers noirs dont le diamètre est compris entre 0.211 et 0.523 mètre. Pensez vous qu’il soit acceptable de prédire des volumes de bois pour des arbres dont le diamètre est inférieur ou supérieur à nos valeurs minimales et maximales mesurées (extrapolation) ? Utilisons notre régression linéaire afin de prédire 10 volumes de bois à partir d’arbre dont le diamètre varie entre 0.1 et 0.8m. new &lt;- data.frame(diameter = seq(0.1, 0.7, length.out = 8)) Ajoutons une variable pred qui contient les prédictions en volume de bois. Observez-vous un problème particulier sur base du tableau ci-dessous ? new %&gt;.% modelr::add_predictions(., lm.) -&gt; new new # diameter pred # 1 0.1000000 -0.482324424 # 2 0.1857143 0.002092891 # 3 0.2714286 0.486510206 # 4 0.3571429 0.970927522 # 5 0.4428571 1.455344837 # 6 0.5285714 1.939762152 # 7 0.6142857 2.424179468 # 8 0.7000000 2.908596783 Il est peut-être plus simple de voir le problème sur un nuage de points. Pour un diamètre de 0.1857143 m de diamètre, le volume de bois est de 0 mis en avant par l’intersection des lignes pointillées bleues. chart(trees, volume~diameter) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;, color = &quot;grey&quot;) + geom_vline(xintercept = new$diameter[2], linetype = &quot;twodash&quot;, color = &quot;blue&quot;) + geom_hline(yintercept = new$pred[2], linetype = &quot;twodash&quot;, color = &quot;blue&quot;) + geom_point() + geom_abline(aes(intercept = lm.$coefficients[1], slope = lm.$coefficients[2])) + geom_point(data = new, f_aes(pred~diameter), color = &quot;red&quot;) Le volume de bois prédit est négatif ! Notre modèle est-il alors complètement faux ? Rappelons-nous qu’un modèle est nécessairement une vision simplifiée de la réalité. En particulier, notre modèle a été entraîné avec des données comprises dans un intervalle. Il est alors valable pour effectuer des interpolations à l’intérieur de cet intervalle, mais ne peut pas être utilisé pour effectuer des extrapolations en dehors, comme nous venons de le faire. trees %&gt;.% modelr::add_predictions(., lm.) -&gt; trees chart(trees, volume~diameter) + geom_point() + geom_line(f_aes(pred ~ diameter)) Pièges et astuces : significativité fortuite Gardez toujours à l’esprit qu’il est possible que votre jeu de données donne une régression significative, mais purement fortuite. Les données supplémentaires de test devraient alors démasquer le problème. D’où l’importance de vérifier/valider votre modèle. Le principe de parcimonie veut que l’on ne teste pas toutes les combinaisons possibles deux à deux des variables d’un gros jeu de données, mais que l’on restreigne les explorations à des relations qui ont un sens biologique afin de minimiser le risque d’obtenir une telle régression de manière fortuite. 1.3.2 Enveloppe de confiance De même que l’on peut définir un intervalle de confiance dans lequel la moyenne d’un échantillon se situe avec une probabilité donnée, il est aussi possible de calculer et de tracer une enveloppe de confiance qui indique la région dans laquelle le “vrai” modèle se trouve avec une probabilité donnée (généralement, on choisi cette probabilité à 95%). Voici ce que cela donne : lm. %&gt;.% (function(lm, model = lm[[&quot;model&quot;]], vars = names(model)) chart(model, aes_string(x = vars[2], y = vars[1])) + geom_point() + stat_smooth(method = &quot;lm&quot;, formula = y ~ x))(.) Cette enveloppe de confiance est en réalité basée sur l’écart type conditionnel (écart type de \\(y\\) sachant quelle est la valeur de \\(x\\)) qui se calcule comme suit : \\[s_{y|x}\\ =\\ \\sqrt{ \\frac{\\sum_{i = 0}^n\\left(y_i - \\hat y_i\\right)^2}{n-2}}\\] A partir de là, il est possible de définir également un intervalle de confiance conditionnel à \\(x\\) : \\[CI_{1-\\alpha}\\ =\\ \\hat y_i\\ \\ \\pm \\ t_{\\frac{\\alpha}{2}}^{n-2} \\frac{s_{y|x}\\ }{\\sqrt{n}}\\] C’est cet intervalle de confiance conditionnel qui est matérialisé par l’enveloppe de confiance autour de la droite de régression représentée sur le graphique. 1.3.3 Extraire les données d’un modèle La fonction tidy() du package broom extrait facilement et rapidement sous la forme d’un tableau différentes valeurs associées à votre régression linéaire. (DF &lt;- broom::tidy(lm.)) # # A tibble: 2 x 5 # term estimate std.error statistic p.value # &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 (Intercept) -1.05 0.0955 -11.0 7.85e-12 # 2 diameter 5.65 0.276 20.4 9.09e-19 Pour extraire facilement et rapidement sous la forme d’un tableau de données les paramètres de votre modèle vouys pouvez aussi utiliser la fonction glance(). (DF &lt;- broom::glance(lm.)) # # A tibble: 1 x 11 # r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 0.935 0.933 0.121 418. 9.09e-19 2 22.6 -39.2 -34.9 # # … with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; Vous avez des snippets à votre disposition pour ces deux fonctions : ... -&gt; models ..m -&gt; .models tools .mt -&gt; .mtmoddf ou encore ... -&gt; models ..m -&gt; .models tools .mt -&gt; .mtpardf A vous de jouer Vous avez à votre disposition la première assignation GitHub Classroom : https://classroom.github.com/a/bvqsukEO "],
["lm2.html", "Module 2 Régression linéaire II", " Module 2 Régression linéaire II Objectifs Savoir utiliser les outils de diagnostic de la régression linéaire correctement, en particulier l’analyse des résidus. Appréhender les différentes formes de régressions linéaires par les moindres carrés. Choisir sa régression linéaire de manière judicieuse. Prérequis Le module précédent est une entrée en matière indispensable qui est complétée par le contenu du présent module. "],
["outils-de-diagnostic-suite.html", "2.1 Outils de diagnostic (suite)", " 2.1 Outils de diagnostic (suite) La régression linéaire est une matière complexe et de nombreux outils existent pour vous aider à déterminer si le modèle que vous ajustez tient la route ou non. Il est très important de le vérifier avant d’utiliser un modèle. Ajuster un modèle quelconque dans des données est à la portée de tout le monde, mais choisir un modèle pertinent et pouvoir expliquer pourquoi est nettement plus difficile ! 2.1.1 Résumé avec summary()(suite) Reprenons la sortie renvoyée par summary() appliqué à un objet lm. trees &lt;- read(&quot;trees&quot;, package = &quot;datasets&quot;, lang = &quot;fr&quot;) lm. &lt;- lm(data = trees, volume ~ diameter) summary(lm.) # # Call: # lm(formula = volume ~ diameter, data = trees) # # Residuals: # Min 1Q Median 3Q Max # -0.231211 -0.087021 0.003533 0.100594 0.271725 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) -1.04748 0.09553 -10.96 7.85e-12 *** # diameter 5.65154 0.27649 20.44 &lt; 2e-16 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.1206 on 29 degrees of freedom # Multiple R-squared: 0.9351, Adjusted R-squared: 0.9329 # F-statistic: 417.8 on 1 and 29 DF, p-value: &lt; 2.2e-16 Nous n’avons pas encore étudié la signification des trois dernières lignes de ce résumé. Voici de quoi il s’agit. Residual standard error : Il s’agit de l’écart-type résiduel, considérant que les degrés de liberté du modèle est le nombre d’observations \\(n\\) (ici 31) soustrait du nombre de paramètres à estimer (ici 2, la pente et l’ordonnée à l’origine de la droite). C’est donc une mesure globale de l’importance (c’est-à-dire de l’étendue) des résidus de manière générale. \\[\\sqrt{\\frac{\\sum(y_i - ŷ_i)^2}{n-2}}\\] Multiple R-squared : Il s’agit de la valeur du coefficient de détermination du modèle noté R^2 de manière générale ou r2 dans le cas d’une régression linéaire simple. Il exprime la fraction de variance exprimée par le modèle. Autrement dit, le R2 quantifie la capacité du modèle à prédire la valeur de \\(y\\) connaissant la valeur \\(x\\) pour le même individu. C’est dons une indication du pouvoir prédictif de notre modèle autant que de sa qualité d’ajustement (goodness-of-fit en anglais). Souvenons-nous que la variance totale respecte la propiété d’additivité. La variance est composée au numérateur d’une somme de carrés, et au dénominateur de degrés de liberté. La somme des carrés totaux (de la variance) peut elle-même être décomposée en une fraction expliquée par notre modèle, et la fraction qui ne l’est pas (les résidus) : \\[SC(total) = SC(rég) + SC(résidus)\\] avec : \\[SC(total) = \\sum_{i=0}^n(y_i - \\bar y_i)^2\\] \\[SC(rég) = \\sum_{i=0}^n(ŷ_i - \\bar y_i)^2\\] \\[SC(résidus) = \\sum_{i=0}^n(y_i - ŷ_i)^2\\] A partir de la décomposition de ces sommes de carrés, le coefficient R2 (ou r2) se définit comme : \\[R^2 = \\frac{SC(rég)}{SC(total)} = 1 - \\frac{SC(résidus)}{SC(total)}\\] La valeur du R2 est comprise entre 0 (lorsque le modèle est très mauvais et n’explique rien) et 1 (lorsque le modèle est parfait et “capture” toute la variance des données ; dans ce cas, tous les résidus valent zéro). Donc, plus le coefficient R2 se rapproche de un, plus le modèle explique bien les données et aura un bon pouvoir de prédiction. Dans R, le R2 multiple se réfère simplement au R2 (ou au r2 pour les régressions linéaires simples) calculé de cette façon. L’adjectif multiple indique simplement que le calcul est valable pour une régression multiple telle que nous verrons plus loin. Par contre, le terme au dénominateur considère en fait la somme des carrés totale par rapport à un modèle de référence lorsque la variable dépendante \\(y\\) ne dépend pas de la ou des variables indépendantes \\(x_i\\). Les équations indiquées plus haut sont valables lorsque l’ordonnée à l’origine n’est pas figée (\\(y = a \\ x + b\\)). Dans ce cas, la valeur de référence pour \\(y\\) est bien sa moyenne, \\(\\bar y\\). D’un autre côté, si l’ordonnée à l’origine est fixée à zéro dans le modèle simplifié \\(y = a \\ x\\) (avec \\(b = 0\\) obtenu en indiquant la formule y ~ x + 0 ou y ~ x - 1), alors le zéro sur l’axe \\(y\\) est considéré comme une valeur appartenant d’office au modèle et devient valeur de référence. Ainsi, dans les équations ci-dessus il faut remplacer \\(\\bar y\\) par 0 partout. Le R2 est alors calculé différemment, et sa valeur peut brusquement augmenter si le nuage de points est très éloigné du zéro sur l’axe y. Ne comparez donc jamais les R2 obtenus avec et sans forçage à zéro de l’ordonnée à l’origine ! Adjusted R-squared : La valeur du coefficient R2 ajustée, noté \\(\\bar{R^2}\\) n’est pas utile dans le cadre de la régression linéaire simple, mais est indispensable avec la régression multiple. En effet, à chaque fois que vous rendez votre modèle plus complexe en ajoutant une ou plusieurs variables indépendantes, le modèle s’ajustera de mieux en mieux dans les données, même par pur hasard. C’est un phénomène que l’on appelle l’inflation du R2. A la limite, si nous ajoutons une nouvelle variable fortement corrélée avec les précédentes4, l’apport en terme d’information nouvelle sera négligeable, mais le R2 augmentera malgré tout un tout petit peu. Alors dans quel cas l’ajout d’une nouvelle variable est-il pertinent ou non ? Le R2 ajusté apporte l’information désirée ici. Sa valeur n’augmentera pour l’ajout d’un nouveau prédicteur que si l’ajustement est meilleur que ce que l’on obtiendrait par le pur hasard. Le R2 ajusté se calcule comme suit (il n’est pas nécessaire de retenir cette formule, mais juste de constater que l’ajustement fait intervenir p, le nombre de paramètres du modèle et n, la taille de l’échantillon) : \\[ \\bar{R^2} = 1 - (1 - R^2) \\frac{n - 1}{n - p - 1} \\] F-statistic : Tout comme pour l’ANOVA, le test de la significativité de la régression car \\(MS(rég)/MS(résidus)\\) suit une distribution F à respectivement 1 et \\(n-2\\) degré de liberté, avec \\(MS\\) les carrés moyens, c’est-à-dire les sommes des carrés \\(SC\\) divisés par leurs degrés de liberté respectifs. p-value : Il s’agit de la valeur p associé à la statistique de F, donc à l’ANOVA associée à la régression linéaire. Pour cette ANOVA particulière, l’hypothèse nulle est que la droite n’apporte pas plus d’explication des valeurs de y à partir des valeurs de x que la valeur moyenne de y (ou zéro, dans le cas paerticulier d’un modèle dont l’ordonnée à l’origine est forcé à zéro). L’hypothèse alternative est donc que le modèle est significatif au seuil \\(\\alpha\\) considéré. Donc, notre objectif est de rejetter H0 pour cet test ANOVA pour que le modèle ait un sens (valeur p plus petite quez le seuil \\(\\alpha\\) choisi). Le tableau complet de l’ANOVA associée au modèle peut aussi être obtenu à l’aide de la fonction anova() : anova(lm.) # Analysis of Variance Table # # Response: volume # Df Sum Sq Mean Sq F value Pr(&gt;F) # diameter 1 6.0762 6.0762 417.8 &lt; 2.2e-16 *** # Residuals 29 0.4218 0.0145 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 On y retrouve les mêmes informations, fortement résumées en une ligne à la fin de la sortie de summary(), mais ici sous une forme plus classique de tableau de l’analyse de la variance. 2.1.2 Comparaison de régressions Vous pouvez à présent comparer ces résultats avec un tableau et les six graphiques d’analyse des résidus sans la valeur supérieure à 0.5m de diamètre. Attention, On ne peut supprimer une valeur sans raison valable. La suppression de points aberrants doit en principe être faite avant de débuter l’analyse. La raison de la suppression de ce point est liée au fait qu’il soit seul et unique point supérieur à 0.5m de diamètre. Nous le faisons ici à titre de comparaison. trees_red &lt;- filter(trees, diameter &lt; 0.5) lm1 &lt;- lm(data = trees_red, volume ~ diameter) chart(trees, volume ~ diameter) + geom_point() + geom_abline( aes(intercept = lm.$coefficients[1], slope = lm.$coefficients[2]), color = &quot;red&quot;, size = 1.5) + labs( color = &quot;Modèle&quot;) + scale_color_viridis_c(direction = -1) + geom_abline( aes(intercept = lm1$coefficients[1], slope = lm1$coefficients[2]), color = &quot;blue&quot;, size = 1.5) La droite en bleu correspond à la régression sans utiliser l’arbre de diamètre supérieur à 0,5m. Tentez d’analyser le tableau de notre régression en bleu (astuce : comparez avec ce que la régeression précédente donnait). summary(lm1) # # Call: # lm(formula = volume ~ diameter, data = trees_red) # # Residuals: # Min 1Q Median 3Q Max # -0.215129 -0.068502 -0.001149 0.070522 0.181398 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) -0.94445 0.09309 -10.15 6.98e-11 *** # diameter 5.31219 0.27540 19.29 &lt; 2e-16 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.1082 on 28 degrees of freedom # Multiple R-squared: 0.93, Adjusted R-squared: 0.9275 # F-statistic: 372.1 on 1 and 28 DF, p-value: &lt; 2.2e-16 Tentez d’analyser également les graphiques d’analyse des résidus ci-dessous. #plot(lm1, which = 1) lm1 %&gt;.% chart(broom::augment(.), .resid ~ .fitted) + geom_point() + geom_hline(yintercept = 0) + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = &quot;Residuals&quot;) + ggtitle(&quot;Residuals vs Fitted&quot;) #plot(lm1, which = 2) lm1 %&gt;.% chart(broom::augment(.), aes(sample = .std.resid)) + geom_qq() + geom_qq_line(colour = &quot;darkgray&quot;) + labs(x = &quot;Theoretical quantiles&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Normal Q-Q&quot;) #plot(lm1, which = 3) lm1 %&gt;.% chart(broom::augment(.), sqrt(abs(.std.resid)) ~ .fitted) + geom_point() + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = expression(bold(sqrt(abs(&quot;Standardized residuals&quot;))))) + ggtitle(&quot;Scale-Location&quot;) #plot(lm1, which = 4) lm1 %&gt;.% chart(broom::augment(.), .cooksd ~ seq_along(.cooksd)) + geom_bar(stat = &quot;identity&quot;) + geom_hline(yintercept = seq(0, 0.1, by = 0.05), colour = &quot;darkgray&quot;) + labs(x = &quot;Obs. number&quot;, y = &quot;Cook&#39;s distance&quot;) + ggtitle(&quot;Cook&#39;s distance&quot;) #plot(lm1, which = 5) lm1 %&gt;.% chart(broom::augment(.), .std.resid ~ .hat %size=% .cooksd) + geom_point() + geom_smooth(se = FALSE, size = 0.5, method = &quot;loess&quot;, formula = y ~ x) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + labs(x = &quot;Leverage&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Residuals vs Leverage&quot;) #plot(lm1, which = 6) lm1 %&gt;.% chart(broom::augment(.), .cooksd ~ .hat %size=% .cooksd) + geom_point() + geom_vline(xintercept = 0, colour = NA) + geom_abline(slope = seq(0, 3, by = 0.5), colour = &quot;darkgray&quot;) + geom_smooth(se = FALSE, size = 0.5, method = &quot;loess&quot;, formula = y ~ x) + labs(x = expression(&quot;Leverage h&quot;[ii]), y = &quot;Cook&#39;s distance&quot;) + ggtitle(expression(&quot;Cook&#39;s dist vs Leverage h&quot;[ii] / (1 - h[ii]))) Au travers de cet exemple, nous constatons que la comparaison de modèles, dans le but de choisir le meilleur est un travail utile. Cela apparaitra d’autant plus utile que la situation va passablement se complexifier (dans le bon sens) avec l’introduction de la régression multiple et polynomiale ci-dessous. Heureusement, nous terminerons ce module avec la découverte d’une métrique qui va nous permettre d’effectuer le choix du meilleur modèle de manière fiable : le critère d’Akaike. A vous de jouer ! Réalisez une nouvelle assignation individuelle : Vous avez à votre disposition une assignation GitHub Classroom : https://classroom.github.com/a/jkh3ruyX La corrélation entre les prédicteurs dans un modèle linéaire multiple est un gros problème et doit être évité le plus possible. Cela s’appelle la colinéarité ou encore multicollinéairité. Ainsi, il est toujours préférable de choisir un ensemble de variables indépendantes peu corrélées entre elles dans un même modèle, mais ce n’est pas toujours possible.↩ "],
["regression-lineaire-multiple.html", "2.2 Régression linéaire multiple", " 2.2 Régression linéaire multiple Dans le cas de la régression linéaire simple, nous considèrions le modèle stqatistique suivant (avec \\(\\epsilon\\) représentant les résidus, terme statistique dans l’équation) : \\[y = a \\ x + b + \\epsilon \\] Dans le cas de la régression, nous introduirons plusieurs variables indépendantes notés \\(x_1\\), \\(x_2\\), …, \\(x_n\\) : \\[y = a_1 \\ x_1 + a_2 \\ x_2 + ... + a_n \\ x_n + b + \\epsilon \\] La bonne nouvelle, c’est que tous les calculs, les métriques et les tests d’hypothèses relatifs à la régression linéaire simple se généraliser simplement et naturellement, tout comme nous sommes passés dans le cours SDD 1 de l’ANOVA à 1 facteur à un modèle plus complexe à 2 ou plusoieurs facteurs. Voyons tout de suite ce que cela donne si nous voulions utiliser à la fois le diamètree et la hauteur des cerisiers noirs pour prédire leur volume de bois : summary(lm2 &lt;- lm(data = trees, volume ~ diameter + height)) # # Call: # lm(formula = volume ~ diameter + height, data = trees) # # Residuals: # Min 1Q Median 3Q Max # -0.180423 -0.074919 -0.006874 0.062244 0.241801 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) -1.63563 0.24462 -6.686 2.95e-07 *** # diameter 5.25643 0.29594 17.762 &lt; 2e-16 *** # height 0.03112 0.01209 2.574 0.0156 * # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.1104 on 28 degrees of freedom # Multiple R-squared: 0.9475, Adjusted R-squared: 0.9438 # F-statistic: 252.7 on 2 and 28 DF, p-value: &lt; 2.2e-16 D’un point de vue pratique, nous voyons que la formule qui spécifie le modèle peut très bien comporter plusieurs variables séparées par des +. Nous avons ici trois paramètres dans notre modèle : l’ordonnée à l’origine qui vaut -1,63, la pente relative au diamètre de 5,25, et la pente relative à la hauteur de 0,031. Le modèle lm2 sera donc paramétré comme suit : volume de bois = 5,25 . diamètre + 0,031 . hauteur - 1,63. Notons que la pente relative à la hauteur (0,031) n’est pas significativement différente de zéro au seuil \\(\\alpha\\) de 5% (mais l’est seulement pour \\(\\alpha\\) = 1%). En effet, la valeur t du test de Student associé (H0 : le paramètre vaut zéro, H1 : le paramètre est différent de zéro) vaut 2,574. Cela correspond à une valeur p du test de 0,0156, une valeur moyennement significative donc, matérialisée par une seule astérisque à la droite du tableau. Cela dénote un plus faible pouvoir de prédiction du volume de bois via la hauteur que via le diamètre de l’arbre. Nous l’avions déjà observé sur le graphique matrice de nuages de points réalisé initialement, ainsi que via les coefficients de correlation respectifs. La représentation de cette régression nécessite un graphique à trois dimensions (diamètre, hauteur et volume) et le modèle représente en fait le meilleur plan dans cet espace à 3 dimensions. Pour un modèle comportant plus de deux variables indépendantes, il n’est plus possible de représenter graphiquement la régression. library(rgl) knitr::knit_hooks$set(webgl = hook_webgl) car::scatter3d(data = trees, volume ~ diameter + height, fit = &quot;linear&quot;, residuals = TRUE, bg = &quot;white&quot;, axis.scales = TRUE, grid = TRUE, ellipsoid = FALSE) # Loading required namespace: mgcv rgl::rglwidget(width = 800, height = 800) Utilisez la souris pour zoomer (molette) et pour retourner le graphique (cliquez et déplacer la souris en maintenant le bouton enfoncé) pour comprendre ce graphique 3D. La régression est matérialisée par un plan en bleu. Les observations sont les boules jaunes et les résidus sont des traits cyans lorsqu’ils sont positifs et magenta lorsqu’ils sont négatifs. Les graphes d’analyse des résidus sont toujours disponibles (nous ne représentons ici que les quatre premiers) : #plot(lm2, which = 1) lm2 %&gt;.% chart(broom::augment(.), .resid ~ .fitted) + geom_point() + geom_hline(yintercept = 0) + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = &quot;Residuals&quot;) + ggtitle(&quot;Residuals vs Fitted&quot;) #plot(lm2, which = 2) lm2 %&gt;.% chart(broom::augment(.), aes(sample = .std.resid)) + geom_qq() + geom_qq_line(colour = &quot;darkgray&quot;) + labs(x = &quot;Theoretical quantiles&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Normal Q-Q&quot;) #plot(lm2, which = 3) lm2 %&gt;.% chart(broom::augment(.), sqrt(abs(.std.resid)) ~ .fitted) + geom_point() + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = expression(bold(sqrt(abs(&quot;Standardized residuals&quot;))))) + ggtitle(&quot;Scale-Location&quot;) #plot(lm2, which = 4) lm2 %&gt;.% chart(broom::augment(.), .cooksd ~ seq_along(.cooksd)) + geom_bar(stat = &quot;identity&quot;) + geom_hline(yintercept = seq(0, 0.1, by = 0.05), colour = &quot;darkgray&quot;) + labs(x = &quot;Obs. number&quot;, y = &quot;Cook&#39;s distance&quot;) + ggtitle(&quot;Cook&#39;s distance&quot;) Est-ce que ce modèle est préférable à celui n’utilisant que le diamètre ? Le R2 ajusté est passé de 0,933 avec le modèle simple lm. utilisant uniquement le diamètre à 0,944 dans le présent modèle lm2 utilisant le diamètre et la hauteur. Cela semble une amélioration, mais le test de significativité de la pente pour la hauteur ne nous indique pas un résultat très significatif. De plus, cela a un coût en pratique de devoir mesurer deux variables au lieu d’une seule pour estimer le volume de bois. Cela en vaut-il la peine ? Nous sommes encore une fois confrontés à la question de comparer deux modèles, cette fois-ci ayant une complexité croissante. Dans le cas particulier de modèles imbriqués (un modèle contient l’autre, mais rajoute un ou plusieurs termes), une ANOVA est possible en décomposant la variance selon les composantes reprises respectivement par chacun des deux modèles. La fonction anova() est programmée pour faire ce calcul en lui indiquant chacun des deux objets contenant les modèles à comparer : anova(lm., lm2) # Analysis of Variance Table # # Model 1: volume ~ diameter # Model 2: volume ~ diameter + height # Res.Df RSS Df Sum of Sq F Pr(&gt;F) # 1 29 0.42176 # 2 28 0.34104 1 0.080721 6.6274 0.01562 * # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Notez que dans le cas de l’ajout d’un seul terme, la valeur p de cette ANOVA est identique à la valeur p de test de significativité du paramètre (ici, cette valeur p est de 0,0156 dans les deux cas). Donc, le choix peut se faire directement à partir de summary() pour ce terme unique. La conclusion est similaire : l’ANOVA donne un résultat seulement moyennent significatif entre les 2 modèles. Dans un cas plus complexe, la fonction anova() de comparaison pourra être utile. Enfin, tous les modèles ne sont pas nécessairement imbriqués. Dans ce cas, il nous faudra un autre moyen de les départager, … mais avant d’aborder cela, étudions une variante intéressante de la régression multiple : la régression polynomiale. A vous de jouer ! Réalisez une nouvelle assignation individuelle : Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant les bases de R : BioDataScience2::run(&quot;02a_reg_multi&quot;) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel dans la console R. "],
["regression-lineaire-polynomiale.html", "2.3 Régression linéaire polynomiale", " 2.3 Régression linéaire polynomiale Pour rappel, un polynome est une expression mathématique du type (notez la ressemblance avec l’équation de la régression multiple) : \\[ a_0 + a_1 . x + a_2 . x^2 + ... + a_n . x^n \\] Un polynome d’ordre 2 (terme jusqu’au \\(x^2\\)) correspond à une parabole dans le plan xy. Que se passe-t-il si nous calculons une variable diametre2 qui est le carré du diamètre et que nous prétendons faire une régression multiple en utilisant à la fois diamètre et diamètre2/ ? trees %&gt;.% mutate(., diameter2 = diameter^2) -&gt; trees summary(lm(data = trees, volume ~ diameter + diameter2)) # # Call: # lm(formula = volume ~ diameter + diameter2, data = trees) # # Residuals: # Min 1Q Median 3Q Max # -0.157938 -0.068324 -0.009027 0.060611 0.214988 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 0.3111 0.3180 0.978 0.336293 # diameter -2.3718 1.8381 -1.290 0.207489 # diameter2 11.2363 2.5563 4.396 0.000144 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.09441 on 28 degrees of freedom # Multiple R-squared: 0.9616, Adjusted R-squared: 0.9589 # F-statistic: 350.5 on 2 and 28 DF, p-value: &lt; 2.2e-16 Il semble que R ait pu réaliser cette analyse. Cette fois-ci, nous n’avons cependant pas une droite ou un plan ajusté, mais par ce subterfuge, nous avons pu ajuster une courbe dans les données ! Nous pourrions augmenter le degré du polynome (ajouter un terme en diameter^3, voire encore des puissance supérieures). Dans ce cas, nous obtiendrons une courbe de plus en plus flexible, toujours dans le plan xy. Ceci illustre parfaitement d’ailleurs l’ambiguité de la complexité du modèle qui s’ajuste de mieux en mieux dans les données, mais qui ce faisant, perd également progressivement son pouvoir explicatif. En effet, on sait qu’il existe toujours une droite qui passe entre deux points dans le plan. De même, il existe toujours une parabole qui passe par 3 points quelconques dans le plan. Et par extension, il existe une courbe correspondant à un polynome d’ordre n - 1 qui passe par n’importe quel ensemble de n points dans le plan. Un modèle construit à l’aide d’un tel polynome aura toujours un R2 égal à un, … mais en même temps ce modèle ne sera d’aucune utilité car il ne contient plus aucune information pertinente. C’est ce qu’on appelle le surajustement (overfitting en anglais). La figure ci-dessous (issue d’un article écrit par Anup Bhande ici) illuste bien ce phénomène. Devoir calculer les différentes puissance des variables au préalable devient rapidement fastidieux. Heureusement, R autorise de glisser ce cacul directement dans la formule, mais à condition de lui indiquer qu’il ne s’agit pas du nom d’une variable diameter^2, mais d’un calcul effectué sur diameter en utilisant la fonction, d’identité I(). Ainsi, sans rien calculer au préalable, nous pouvons utiliser la formule volume ~ diameter + I(diameter^2). Un snippet est d’ailleurs disponible pour ajuster un polynome d’ordre 2 ou d’ordre 3, et il est accompagné du code nécessaire pour représenter également graphiquement cette régression polynomiale. Le code ci-dessous qui construit le modèle lm3 l’utilise. summary(lm3 &lt;- lm(data = trees, volume ~ diameter + I(diameter^2))) # # Call: # lm(formula = volume ~ diameter + I(diameter^2), data = trees) # # Residuals: # Min 1Q Median 3Q Max # -0.157938 -0.068324 -0.009027 0.060611 0.214988 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 0.3111 0.3180 0.978 0.336293 # diameter -2.3718 1.8381 -1.290 0.207489 # I(diameter^2) 11.2363 2.5563 4.396 0.000144 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.09441 on 28 degrees of freedom # Multiple R-squared: 0.9616, Adjusted R-squared: 0.9589 # F-statistic: 350.5 on 2 and 28 DF, p-value: &lt; 2.2e-16 lm3 %&gt;.% (function(lm, model = lm[[&quot;model&quot;]], vars = names(model)) chart(model, aes_string(x = vars[2], y = vars[1])) + geom_point() + stat_smooth(method = &quot;lm&quot;, formula = y ~ x + I(x^2)))(.) Remarquez sur le graphique comment, à présent, la courbe s’ajuste bien mieux dans le nuage de point et comme l’arbre le plus grand avec un diamètre supérieur à 0,5m est à présent presque parfaitement ajusté par le modèle. Faites donc très attention que des points influents ou extrêmes peuvent apparaitre également comme tel à cause d’un mauvais choix de modèle ! L’analyse des résidus nous montre aussi un comportement plus sain. #plot(lm3, which = 1) lm3 %&gt;.% chart(broom::augment(.), .resid ~ .fitted) + geom_point() + geom_hline(yintercept = 0) + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = &quot;Residuals&quot;) + ggtitle(&quot;Residuals vs Fitted&quot;) #plot(lm3, which = 2) lm3 %&gt;.% chart(broom::augment(.), aes(sample = .std.resid)) + geom_qq() + geom_qq_line(colour = &quot;darkgray&quot;) + labs(x = &quot;Theoretical quantiles&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Normal Q-Q&quot;) #plot(lm3, which = 3) lm3 %&gt;.% chart(broom::augment(.), sqrt(abs(.std.resid)) ~ .fitted) + geom_point() + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = expression(bold(sqrt(abs(&quot;Standardized residuals&quot;))))) + ggtitle(&quot;Scale-Location&quot;) #plot(lm3, which = 4) lm3 %&gt;.% chart(broom::augment(.), .cooksd ~ seq_along(.cooksd)) + geom_bar(stat = &quot;identity&quot;) + geom_hline(yintercept = seq(0, 0.1, by = 0.05), colour = &quot;darkgray&quot;) + labs(x = &quot;Obs. number&quot;, y = &quot;Cook&#39;s distance&quot;) + ggtitle(&quot;Cook&#39;s distance&quot;) Revenons un instant sur le résumé de ce modèle. summary(lm3) # # Call: # lm(formula = volume ~ diameter + I(diameter^2), data = trees) # # Residuals: # Min 1Q Median 3Q Max # -0.157938 -0.068324 -0.009027 0.060611 0.214988 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 0.3111 0.3180 0.978 0.336293 # diameter -2.3718 1.8381 -1.290 0.207489 # I(diameter^2) 11.2363 2.5563 4.396 0.000144 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.09441 on 28 degrees of freedom # Multiple R-squared: 0.9616, Adjusted R-squared: 0.9589 # F-statistic: 350.5 on 2 and 28 DF, p-value: &lt; 2.2e-16 La pente relative au diameter nécessite quelques éléments d’explication. En effet, que signifie une pente pour une courbe dont la dérivée première (“pente locale”) change constamment ? En fait, il faut comprendre ce paramètre comme étant la pente de la courbe au point x = 0. Si le modèle est très nettement significatif (ANOVA, valeur p &lt;&lt;&lt; 0,001), et si le R2 ajusté grimpe maintenant à 0,959, seul le paramètre relatif au diamètre2 est significatif cette fois-ci. Ce résultat suggère que ce modèle pourrait êtrte simplifié en considérant que l’ordonnée à l’origine et la pente pour le terme diameter valent zéro. Cela peut être tenté, mais à condition de refaire l’analyse. On ne peut jamais laisser tomber un paramètre dans une analyse et considérer que les autres sont utilisable tels quels. tous les paramètres caculés sont interconnectés. Voyons ce que cela donne (la formule devient volume ~ I(diameter^2) - 1 ou volume ~ I(diameter^2) + 0, ce qui est identique) : summary(lm4 &lt;- lm(data = trees, volume ~ I(diameter^2) - 1)) # # Call: # lm(formula = volume ~ I(diameter^2) - 1, data = trees) # # Residuals: # Min 1Q Median 3Q Max # -0.19474 -0.07234 -0.04120 0.04522 0.18240 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # I(diameter^2) 7.3031 0.1397 52.26 &lt;2e-16 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.1027 on 30 degrees of freedom # Multiple R-squared: 0.9891, Adjusted R-squared: 0.9888 # F-statistic: 2731 on 1 and 30 DF, p-value: &lt; 2.2e-16 Notez bien que quand on réajuste un modèle simplifié, les paramètres restants doivent être recalculés. En effet, le paramètre relatif au diamètre2 vallait 11,2 dans le modèle lm3 plus haut. Un fois les autres termes éliminés, ce paramètre devient 7,30 dans ce modèle lm4 simplifié. Le modèle lm4 revient aussi (autre point de vue) à transformer d’abord le diamètre en diamètre2 et à effectuer ensuite une régression linéaire simple entre deux variables, volume et diametre2 : summary(lm4bis &lt;- lm(data = trees, volume ~ diameter2 - 1)) # # Call: # lm(formula = volume ~ diameter2 - 1, data = trees) # # Residuals: # Min 1Q Median 3Q Max # -0.19474 -0.07234 -0.04120 0.04522 0.18240 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # diameter2 7.3031 0.1397 52.26 &lt;2e-16 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.1027 on 30 degrees of freedom # Multiple R-squared: 0.9891, Adjusted R-squared: 0.9888 # F-statistic: 2731 on 1 and 30 DF, p-value: &lt; 2.2e-16 Notez qu’on obtient bien évidemment exactement les mêmes résultats si nous transformons d’abord les données ou si nous intégrons le calcul à l’intérieur de la formule qui décrit le modèle. Faites bien attention de ne pas comparer le R2 acvec ordonnée à l’origine fixée à zéro ici dans notre modèle lm4 avec les R2 des modèles lm. ou lm3 qui ont ce paramètre estimé. Rappelez-vous que le R2 est calculé différemment dans les deux cas ! Donc, nous voilà une fois de plus face à un nouveau modèle pour lequel il nous est difficile de décider s’il est meilleur que les précédents. Avant de comparer, élaborons un tout dernier modèle, le plus complexe, qui reprend à la fois notre régression polynomiale d’ordre 2 sur le diamètre et la hauteur. Autrement dit, une régression à la fois multiple et polynomiale. summary(lm5 &lt;- lm(data = trees, volume ~ diameter + I(diameter^2) + height)) # # Call: # lm(formula = volume ~ diameter + I(diameter^2) + height, data = trees) # # Residuals: # Min 1Q Median 3Q Max # -0.12169 -0.04806 -0.00237 0.05156 0.12559 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) -0.267097 0.284895 -0.938 0.356798 # diameter -3.281421 1.463401 -2.242 0.033354 * # I(diameter^2) 11.891724 2.019252 5.889 2.83e-06 *** # height 0.034788 0.008169 4.259 0.000223 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.07436 on 27 degrees of freedom # Multiple R-squared: 0.977, Adjusted R-squared: 0.9745 # F-statistic: 382.8 on 3 and 27 DF, p-value: &lt; 2.2e-16 Ah ha, ceci est bizarre ! Le R2 ajusté nous indique que le modèle serait très bon puisqu’il grimpe à 0,975. Le terme en diamètre2 reste très significatif, … mais la pente relative à la hauteur est maintenant elle aussi très significative alors que dans le modèle multiple lm2 ce n’était pas le cas. De plus, la pente à l’origine en face du diamètre semble devenir un peu plus significative. Bienvenue dans les instabilités liées aux intercorrelations entre paramètres dans les modèles linéaires complexes. "],
["rmse-critere-dakaike.html", "2.4 RMSE &amp; critère d’Akaike", " 2.4 RMSE &amp; critère d’Akaike Le R2 (ajusté) n’est pas la seule mesure d’ajustement d’un modèle. Il existe d’autres indicateurs. Par exemple, l’erreur quadratique moyenne, (root mean square error, ou RMSE en anglais) est la racine carrée de la moyenne des résidus au carré. Elle représente en quelque sorte la distance “typique” des résidus. Comme cette distance est exprimée dans les mêmes unités que l’axe y, cette mesure est particulièrement parlante. Nous pouvons l’obtenir par exemple comme ceci : modelr::rmse(lm., trees) # [1] 0.1166409 Cela signifie que l’on peut s’attendre à ce que, en moyenne, les valeurs prédites de volume de bois s’écartent (dans un sens ou dans l’autre) de 0,117 m3 de la valeur effectivement observée. Evidemment, plus un modèle est bon, plus le RMSE est faible, contrairement au R2 qui lui doit être élevé. Si le R2 comme le RMSE sont utiles pour quantifier la qualité d’ajustement d’une régression, ces mesures sont peu adaptées pour la comparaison de modèles entre eux. En effet, nous avons vu que plus le modèle est complexe, mieux il s’ajuste dans les données. Le R2 ajusté tente de remédier partiellement à ce problème, mais cette métrique reste peu fiable pour comparer des modèles très différents. Le critère d’Akaike, du nom du statisticien japonais qui l’a conçu, est une métrique plus adaptée à de telles comparaisons. Elle se base au départ sur encore une autre mesure de la qualité d’ajustement d’un modèle : la log-vraisemblance. Les explications relatives à cette mesure sont obligatoirement complexes d’un point de vue mathématique et nous vous proposons ici d’en retenir la définition sur un plan purement conceptuel. Un estimateur de maximum de vraisemblance est une mesure qui permet d’inférer le meilleur ajustement possible d’une loi de probabilité par rapport à des données. Dans le cas de la régression par les moindres carrés, la distribution de probabilité à ajuster est celle des résidus (pour rappel, il s’agit d’une distribution Normale de moyenne nulle et d’écart type constant \\(\\sigma\\)). La log-vraisemblance, pour des raisons purement techniques est souvent préféré au maximum de vraissemblance. Il s’agit simplement du logarithme de sa valeur. Donc, plus la log-vraisemblance est grande, mieux les données sont compatibles avec le modèle probabiliste considéré. Pour un même jeu de données, ces valeurs sont comparables entre elles… même pour des modèles très différents. Mais cela ne règle pas la question de la complexité du modèle. C’est ici qu’Akaike entre en piste. Il propose le critère suivant : \\[ \\textrm{AIC} = -2 . \\textrm{log-vraisemblance} + 2 . \\textrm{nbrpar} \\] où nbrpar est le nombre de paramètres à estimer dans le modèle. Donc ici, nous prenons comme point de départ moins deux fois la log-vraisemblance, une valeur a priori à minimiser, mais nous lui ajoutons le second terme de pénalisation en fonction de la complexité du modèle valant 2 fois le nombre de paramètres du modèle. Notons d’ailleurs que le terme multiplicateur 2 ici est modifiable. Si nous voulons un modèle le moins complexe possible, nous pourrions très bien multiplier par 3 ou 4 pour pénaliser encore plus. Et si nous voulons être moins restrictifs, nous pouvons aussi diminuer ce facteur multiplicatif. Dans la pratique, le facteur 2 est quand même très majoritairement adapté par les praticiens, mais la possibilité de changer l’impact de complexité du modèle est inclue dans le calcul de facto. Dès lors que ce critère peut être calculé (et R le fait pour pratiquement tous les modèles qu’il propose), une comparaison est possible avec pour objectif de sélectionner le, ou un des modèles qui a l’AIC la plus faible. N’oubliez toutefois pas de comparer visuellement les différents modèles ajustés et d’interpréter les graphiques d’analyse des résidus respectifs en plus des valeurs d’AIC. C’est l’ensemble de ces outils qui vous orientent vers le meilleur modèle, pas l’AIC seul ! Calculons maintenant les critères d’Akaike pour nos 6 modèles lm. à lm5… AIC(lm.) # Linéaire diamètre # [1] -39.24246 AIC(lm2) # Multiple diamètre et hauteur # [1] -43.82811 AIC(lm3) # Polynomial diamètre # [1] -53.50964 AIC(lm4) # Diamètre^2 # [1] -50.15027 AIC(lm5) # Multiple et polynomial # [1] -67.4391 D’après ce critère, le modèle linéaire est le moins bon, et le dernier modèle le plus complexe serait le meilleur. Notez toutefois que la différence est relativement minime (en regard du gain total) entre le modèle polynomial complet lm3 et la version simplifié au seul terme diamètre2 en lm4, ce qui permet de penser que cette simplification est justifiée. Dans l’hypothèse où nous déciderions de conserver le modèle lm5, en voici l’analyse des résidus qui est bonne dans l’ensemble : #plot(lm5, which = 1) lm5 %&gt;.% chart(broom::augment(.), .resid ~ .fitted) + geom_point() + geom_hline(yintercept = 0) + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = &quot;Residuals&quot;) + ggtitle(&quot;Residuals vs Fitted&quot;) #plot(lm5, which = 2) lm5 %&gt;.% chart(broom::augment(.), aes(sample = .std.resid)) + geom_qq() + geom_qq_line(colour = &quot;darkgray&quot;) + labs(x = &quot;Theoretical quantiles&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Normal Q-Q&quot;) #plot(lm5, which = 3) lm5 %&gt;.% chart(broom::augment(.), sqrt(abs(.std.resid)) ~ .fitted) + geom_point() + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = expression(bold(sqrt(abs(&quot;Standardized residuals&quot;))))) + ggtitle(&quot;Scale-Location&quot;) #plot(lm5, which = 4) lm5 %&gt;.% chart(broom::augment(.), .cooksd ~ seq_along(.cooksd)) + geom_bar(stat = &quot;identity&quot;) + geom_hline(yintercept = seq(0, 0.1, by = 0.05), colour = &quot;darkgray&quot;) + labs(x = &quot;Obs. number&quot;, y = &quot;Cook&#39;s distance&quot;) + ggtitle(&quot;Cook&#39;s distance&quot;) Naturellement, même si c’est le cas ici, ce n’est pas toujours le modèle le plus complexe qui “gagne” toujours. Même ici, nous pourrions nous demander si le modèle polynomial utilisant uniquement le diamètre ne serait pas plus intéressant en pratique car son ajustement est tout de même relativement bon (même si son critère d’Akaike est nettement moins en sa faveur), mais d’un point de vue pratique, il nous dispense de devoir mesurer la hauteur des arbres pour prédire le volume de bois. Ce n’est peut-être pas négligeable comme gain, pour une erreur de prédiction légèrement supérieure si on compare les valeurs de RMSE. modelr::rmse(lm5, trees) # Multiple et polynomial # [1] 0.06939391 modelr::rmse(lm3, trees) # Polynomial diamètre # [1] 0.08972287 L’erreur moyenne d’estimation du volume de bois passe de 0,07 m3 pour le modèle le plus complexe lm5 utilisant à la fois le diamètre et la hauteur à 0,09 m3. C’est à l’exploitant qu’il appartient de déterminer si le gain de précision vaut la peine de devoir effectuer deux mesures au lieu d’une seule. Mais au moins, nous sommes capables, en qualité de scientifiques des données, de lui proposer les alternatives possible et d’en quantifier les effets respectifs. Différentes méthodes d’ajustement par xkcd. A vous de jouer ! Après cette longue lecture avec énormément de nouvelles matières, nous vous proposons les exercices suivants : Répondez aux questions d’un learnr afin de vérifier vos acquis. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant les bases de R : BioDataScience2::run(&quot;02b_reg_poly&quot;) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel dans la console R. Réalisez un carnet de laboratoire sur la biométrie des oursins avec l’assignation ci-dessous. Vous avez à votre disposition une assignation GitHub Classroom : https://classroom.github.com/a/5hI-HSOv Réalisez un rapport scientifique sur la croissance des escargots géants d’Afrique. Vous avez à votre disposition une assignation GitHub Classroom : https://classroom.github.com/a/_wJZDbNp "],
["mod-lineaire.html", "Module 3 Modèle linéaire", " Module 3 Modèle linéaire Objectifs Comprendre le modèle linéaire (ANOVA et régression linéaire tout en un) Appréhender la logique des matrices de contraste Découvrir l’ANCOVA Comprendre le mécanisme du modèle linéaire généralisé Prérequis L’ANOVA (modules 10 &amp; 11 du cours SDD 1), ainsi que la régression linéaires (modules 1 et 2 du présent cours) doivent être maitrisés avant d’aborder cette matière. "],
["variables-numeriques-ou-facteurs.html", "3.1 Variables numériques ou facteurs", " 3.1 Variables numériques ou facteurs L’ANOVA analyse une variable dépendante numérique en fonction d’une ou plusieurs variables indépendantes qualitatives. Ces variables sont dites “facteurs” non ordonnés (objets de classe factor), ou “facteurs” ordonnés (objets de classe ordered) dans R. La régression linéaire analyse une variable dépendante numérique en fonction d’une ou plusieurs variables indépendantes numérique (quantitatives) également. Ce sont des objets de classe numeric (ou éventuellement integer, mais assimilé à numeric concrètement) dans R. Donc, la principale différence entre ANOVA et régression linéaire telles que nous les avnos abordés jusqu’ici réside dans la nature de la ou des variables indépendantes, c’est-à-dire, leur type. Pour rappel, il existe deux grandes catégories de variables : quantitatives et qualitatives, et deux sous-catégories pour chacune d’elle. Cela donne quatyre types principaux de variables, formant plus de 90% des cas rencontrés : variables quantitatives continues représentables par des nombres réels (numeric dans R), variables quantitatives discrètes pour des dénombrements d’événements finis par exemple, et représentables par des nombres entiers (integer dans R), variables qualitatives ordonnées pour des variables prenant un petit nombre de valeurs, mais pouvant être ordonnées de la plus petite à la plus grande (ordered dans R), variables qualitatives non ordonnées prenant également un petit nombre de valeurs possibles, mais sans ordre particulier (factor dans R). Par la suite, un encodage correct des variables sera indispensable afin de distinguer correctement ces différentes situations. En effet, R considèrera automatiquement comment mener l’analyse en fonction de la classe des variables fournies. Donc, si la classe est incorrecte, l’analyse le sera aussi ! Si vous avez des doutes concernant les types de variables, relisez la section type de variables avant de continuer ici. "],
["anova-et-regression-lineaire.html", "3.2 ANOVA et régression linéaire", " 3.2 ANOVA et régression linéaire Avez-vous remarqué une ressemblance particulière entre la régression linéaire que nous avons réalisé précédement et l’analyse de variance (ANOVA) ? Les plus observateurs auront mis en avant que la fonction de base dans R est la même dans les deux cas : lm(). Cette fonction est donc capable de traiter aussi bien des variables réponses qualitatives que quantitatives, et effectue alors une ANOVA dans un cas ou une régression linéaire dans l’autre. Par ailleurs, nous avons vu que l’ANOVA et la régression linéaire se représentent par des modèles semblables : \\(y = \\mu + \\tau_i + \\epsilon\\) pour l’ANOVA et \\(y = \\beta_1 + \\beta_2 x + \\epsilon\\) pour la régression linéaire, avec \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma)\\) dans les deux cas. Donc, nous retrouvons bien au niveau du modèle mathématique sous-jacent la différence principale entre les deux qui réside dans le type de variable indépendante (ou explicative) : Variable qualitative pour l’ANOVA, Variable quantitative pour la régression linéaire. Le calcul est, en réalité, identique en interne. Il est donc possible de généraliser ces deux approches en une seule appelée modèle linéaire, mais à condition d’utiliser une astuce pour modifier nos modèles afin qu’ils soient intercompatibles. 3.2.1 Modèle linéaire commun Le nœud du problème revient donc à transformer nos modèles mathématiques pour qu’ils puissent être fusionnés en un seul. Comment homogénéiser ces deux modèles ? \\(y = \\mu + \\tau_i + \\epsilon\\) pour l’ANOVA et \\(y = \\beta_1 + \\beta_2 x + \\epsilon\\) pour la régression linéaire. Avant de poursuivre, réfléchisser un peu par vous-même. Quelles sont les différences qu’il faut contourner ? Est-il possible d’effectuer une ou plusieurs transformations des variables pour qu’elles se comportent de manière similaire dans les deux cas ? 3.2.2 Réencodage des variables de l’ANOVA Considérons dans un premier temps, un cas très simple : une ANOVA à un facteur avec une variable indépendante qualitative (factor) à deux niveaux5. Nous pouvons écrire : \\[ y = \\mu + \\tau_1 I_1 + \\tau_2 I_2 + \\epsilon \\] avec \\(I_i\\), une variable dite indicatrice créée de toute pièce qui prend la valeur 1 lorsque le niveau correspond à i, et 0 dans tous les autres cas. Vous pouvez vérifier par vous-même que l’équation ci-dessus fonctionnera exactement de la même manière que le modèle utilisé jusqu’ici pour l’ANOVA. En effet, poiur un individu de la population 1, \\(I_1\\) vaut 1 et \\(\\tau_1\\) est utilisé, alors que comme \\(I_2\\) vaut 0, \\(\\tau_2\\) est annulé dans l’équation car \\(\\tau_2 I_2\\) vaut également 0. Et c’est exactement l’inverse qui se produit pour un individu de la population 2, de sorte que c’est \\(\\tau_2\\) qui est utilisé cette fois-ci. Notez que notre nouvelle formulation, à l’aide de variables indicatrices ressemble fortement à la régression linéaire. La seule différence par rapport à cette dernière est que nos variables \\(I_i\\) ne peuvent prendre que des valeurs 0 ou 1 (en tous cas, pour l’instant), alors que les \\(x_i\\) dans la régression linéaire multiple sont des variables quantitatives qui peuvent prendre une infinité de valeurs différentes (nombres réels). Nouys pouvons encore réécrire notre équation comme suit pour qu’elle se rapproche encore plus de celle de la régression linéaire simple. Passons par l’introduction de deux termes identiques \\(\\tau_1 I_2\\) additionné et soustrait, ce qui revient au même qu’en leur absence : \\[ y = \\mu + \\tau_1 I_1 + \\tau_1 I_2 - \\tau_1 I_2 + \\tau_2 I_2 + \\epsilon \\] En considérant \\(\\beta_2 = \\tau_2 - \\tau_1\\), cela donne : \\[ y = \\mu + \\tau_1 I_1 + \\tau_1 I_2 + \\beta_2 I_2 + \\epsilon \\] En considérant \\(\\beta_1 = \\mu + \\tau_1 = \\mu + \\tau_1 I_1 + \\tau_1 I_2\\) (car quelle que soit la population à laquelle notre individu appartient, il n’y a jamais qu’une seule des deux valeurs \\(I_1\\) ou \\(I_2\\) non nulle et dans tous les cas le résultat est donc égal à \\(\\tau_1\\)), on obtient : \\[ y = \\beta_1 + \\beta_2 I_2 + \\epsilon \\] Cette dernière formulation est strictement équivalente au modèle de la régression linéaire simple dans laquelle la variable \\(x\\) a simplement été remplacée par notre variable indicatrice \\(I_2\\). Ceci se généralise pour une variable indépendante à \\(k\\) niveaux, avec \\(k - 1\\) variables indicatrices au final. En prenant soin de réencoder le modèle de l’ANOVA relatif aux variables indépendantes qualitatives, nous pouvons à présent mélanger les termes des deux modèles en un seul : notre fameux modèle linéaire. Nous aurons donc, quelque chose du genre (avec les \\(x_i\\) correspondant aux variables quantitatives et les \\(I_j\\) des variables indicatrices pour les différents niveaux des variables qualitatives) : \\[ y = \\beta_1 + \\beta_2 x_1 + \\beta_3 x_2 + ... + \\beta_n I_1 + \\beta_{n+1} I_2 ... + \\epsilon \\] Concrètement, un cas aussi simple se traite habituellement à l’aide d’un test t de Student, mais pour notre démonstration, nous allons considérer ici utiliser une ANOVA à un facteur plutôt.↩ "],
["matrice-de-contraste.html", "3.3 Matrice de contraste", " 3.3 Matrice de contraste La version que nous avons étudié jusqu’ici pour nos variables indicatrices, à savoir, une seule prend la valeur 1 lorsque toutes les autres prend une valeur zéro, n’est qu’un cas particulier de ce qu’on appelle les contrastes appliqués à ces variables indicatrices. En réalité, nous pouvons leurs donner bien d’autres valeurs (on parle de poids), et cela permettra de considérer dses contrastes différents, eux-mêmes représentatifs de situations différentes. Afin de mieux comprendre les contrastes appliqués à nos modèles linéaires, les statisticiens ont inventé les matrices de contrastes. Ce sont des tableaux à deux entrées indiquant pour chaque niveau de la variable indépendante qualitative quelles sont les valeurs utilisées pour les différentes variables indicatrices présentées en colonne. Dans le cas de notre version simplifiée du modèle mathématique où nous avons fait disparaitre \\(I_1\\) en l’assimilant à la moyenne \\(\\mu\\) pour obteniur \\(\\beta_1\\). Dans le cas où notre variable qualitative a quatre niveaux, nous avons donc le modèle suivant : \\[ y = \\beta_1 + \\beta_2 I_2 + \\beta_3 I_3 + \\beta_4 I_4 + \\epsilon \\] Cela revient à considérer le premier niveau comme niveau de référence et à établir tous les contrastes par rapport à ce niveau de référence. C’est une situation que l’on rencontre fréquemment lorsque nos testons l’effet de différents médicaments ou de différents traitement par rapport à un contrôle (pas de traitement, ou placébo). La matrice de contrastes correspondante, dans un cas où on aurait trois traitements en plus du contrôle (donc, notre variable factor à quatre niveaux) s’obteint facilement dans R à l’aide de la fonction contr.treatment() : contr.treatment(4) # 2 3 4 # 1 0 0 0 # 2 1 0 0 # 3 0 1 0 # 4 0 0 1 Les lignes de cette matrice sobnt numérotées de 1 à 4. Elles correspondent aux quatres niveaux de notre variable factor, avec le niveau 1 qui doit nécessairement correspondre à la situation de référtence, donc au contrôle. Les colonnes de cette matrice correspondent aux trois variables indicatrices \\(I_1\\), \\(I_2\\) et \\(I_3\\) de l’équation au dessus. Nous voyons que pour une individu contrôle, de niveau 1, les trois \\(I_i\\) prennent la valeur 0. Nous sommes bien dans la situation de référence. En d’autres terme, le modèle de base est ajusté sur la moyenne des individus contrôle. Notre modèle se réduit à : \\(y = \\beta_1 + \\epsilon\\). Donc, seule la moyenne des individus contrôles, \\(\\beta_1\\) est considérée, en plus des résidus \\(\\epsilon\\) bien sûr. Pour le niveau deux, nous observons que \\(I_2\\) vaut 1 et les deux autres \\(I_i\\) valent 0. Donc, cela revient à considérer un décalage constant \\(\\beta_2\\) appliqué par rapport au modèle de référence matérialisé par \\(\\beta_1\\). En effezt, notre équation se réduit dans ce cas à : \\(y = \\beta_1 + \\beta_2 + \\epsilon\\). Le même raisonnement peut être fait pour les niveaux 3 et 4, avec des décalages constants par rapport à la situation cxontrôle de respectivement \\(\\beta_3\\) et \\(\\beta_4\\). En d’autres termes, les contrastes qui sont construits ici font tous référence au contrôle, et chaque médicament est explicitement comparté au contrôle (mais les médicaments ne sont pas comparés entre eux). Nous voyons donc que les variables indicatrices etr la matrice de contrastes permet de spécifier quelles sont les contrastes pertinents et éliminent ceux qui ne le sont pas (nous n’utilisons donc pas systématiquement toutes les comparaisons deux à deux des différents niveaux6). 3.3.1 Contraste orthogonaux Les contrastes doivent être de préférence orthogonaux par rapport à l’ordonnée à l’origine, ce qui signifie que la somme de leurs pondérations doit être nulle pour tous les contrastes définis (donc, en colonnes). Bien que n’étant pas obligatoire, cela confère des propriétés intéressantes au modèle (l’explication et la démonstration sortent du cadre de ce cours). Or, les contrastes de type traitement ne sont pas orthogonaux puisque toutes les sommes par colonnes vaut un. 3.3.2 Autres matrices de contrastes courantes Somme à zéro. Ces constraste, toujours pour une variable à quatre niveaux, se définissen t comme suit en utilisant la fonction contr.sum() dans R : contr.sum(4) # [,1] [,2] [,3] # 1 1 0 0 # 2 0 1 0 # 3 0 0 1 # 4 -1 -1 -1 Ici nous avons bien des contrastes orthogonaux puisque toutes les sommes par colonnes valeur zéro. Dans le cas présent, aucun niveau n’est considéré comme référence, mais les n - 1 niveaux sont systématiquement contrastés avec le dernier et nîème^ niveau. Ainsi, un contraste entre deux niveaux particuliers peut s’élaborer en indiquant une pondération de 1 pour le premier niveau à comparer, une pondération de -1 pour le second à comparer et une pondération de 0 pour tous les autres. Matrice de contrastes de Helmert : chaque niveau est comparé à la moyenne des niveaux précédents. La matrice de constrastes correspondant pour une variable à quatre niveaux s’obtient à l’aide de la fonction R contr.helmert() : contr.helmert(4) # [,1] [,2] [,3] # 1 -1 -1 -1 # 2 1 -1 -1 # 3 0 2 -1 # 4 0 0 3 Cette matrice est également orthogonale avec toutes les sommes par colonnes qui valent zéro. Ici, nous découvrons qu’il est possible de créer un contrastye entre un niveau et la moyenne de plusieurs autres niveaux en mettant le poids du premier à m (le nombre de populations à comparer de l’autre côté du contraste), et les poids des autres populations tous à -1. Ainsi, la colonne 4 compare le niveau quatre avec pondération 3 aux trois autres niveaux qui reçoivent tous une pondération -1. Matrice de contrastes polynomiaux : adapté aux facteurs ordonnés (ordered dans R) pourvlesquels on s’attend à une certaine évolution du modèle du niveau le plus petit au plus grand. Donc ici aussi une comparaison deux à deux de tous les niveaux n’est pas souhaitable, mais une progression d’un effet qui se propage de manière graduelle du plus petit niveau au plus grand. A priori cela parait difficile à métérialiser dans une matrice de contraste… et pourtant, c’est parfaitement possible ! Il s’agit de constrastes polynomiaux où nous ajustons de polynomes de degré croissant comme pondération des différents contrastes étudiés. La fonction contr.poly() permet d’obtenir ce type de contraste dans R. Pour une variable ordonnée à quatre niveaux, cela donne : contr.poly(4) # .L .Q .C # [1,] -0.6708204 0.5 -0.2236068 # [2,] -0.2236068 -0.5 0.6708204 # [3,] 0.2236068 -0.5 -0.6708204 # [4,] 0.6708204 0.5 0.2236068 Ici, les pondérations sont plus difficiles à expliquer rien qu’en observant la matrice de contrastes. De plus, les colonnes portent ici des noms particuliers .L pour un contraste linéaire (polynome d’ordre 1), .Q pour un contraste quadratique (polynome d’ordre 2), et .C pour un contraste conique (ou polynome d’ordre 3). Les pondérations appliquées se comprennent mieux lorsqu’on augmente le nombre de niveaux etr que l’on représente graphiquement la valeur des pondérations choisées. Par exemple, pour une variable facteur ordonnée à dix niveaux, nous représentrons graphiquement les 3 premeirs contrastes (linéaire, quadratique et conique) comme suit : plot(contr.poly(10)[, 1], type = &quot;b&quot;) plot(contr.poly(10)[, 2], type = &quot;b&quot;) plot(contr.poly(10)[, 3], type = &quot;b&quot;) Sur le graphique, l’axe X nommé index correspiond en réalité à la succession des 10 niveaux de la variable présentés dans l’ordre du plus petit au plus grand. Nous voyons maintenant clairement comment les contrastes sont construits ici. Pour le conbtraste linéaire, on contraste les petits niveaux avec les grands, et ce, de manière proportionnelle par rapport à la progression d’un niveau à l’autre (polynome d’ordre un = droite). Pour le contraste quadratique, on place “dans le même sac” les petits et greand niveaux qui sont contrastés avec les niveaux moyens (nous avons une parabole ou polynome d’ordre 2). Pour le troisième graphique, la situation se complexifie en encore un peu plus avec un polynome d’ordre 3, et ainsi de suite pour des polynomes d’ordres croissants jusqu’à remplir complètement la matrice de contrastes. R utilise par défaut des contrastes de traitement pour les facteurs non ordonnés et des contrastes polynomiaux pour des facteurs ordonnés. Ces valeurs par défaut sont stockées dans l’option contrasts que l’on peut lire à l’aide de getOption(). Bien sûr, il est possible de changer ces contrastes, tant au niveau global qu’au niveau de la construction d’un modèle en particulier. getOption(&quot;contrasts&quot;) # unordered ordered # &quot;contr.treatment&quot; &quot;contr.poly&quot; Attention : le fait d’utiliser une matrtice de contraste qui restreint ceux utilisés dans le modèle est indépendant des tests post hoc de comparaisons multiples, qui restent utilisables par après. Les comparaisons deux à deux des médicaments restent donc accessibles, mais ils ne sont tout simplement pas mis en évidence dans le modèle de base.↩ "],
["ancova.html", "3.4 ANCOVA", " 3.4 ANCOVA Avant l’apparition du modèle linéaire, une version particulière d’un mélange de régression linéaire et d’une ANOVA avec une variable indépendante quantitative et une autre variable indépendante qualitative s’appelait une ANCOVA (ANalyse de la COVariance). Un tel modèle d’ANCOVA peut naturellement également se résoudre à l’aide de la fonction lm() qui, en outre, peut faire bien plus. Nous allons maintenant ajuster un tel modèle à titre de première application concrète de tout ce que nous venons de voir sur le modèle linéaire et sur les matrices de contrastes associées. 3.4.1 Bébés à la naissance Nous étudions la masse de nouveaux nés en fonction du poids de la mère et du fait qu’elle fume ou non. Cette analyse s’inspire de Verzani (2005). Nous avons donc ici une variable dépendante wt, la masse des bébés qui est quantitative, et deux variables indépendantes ou prédictives wt1, la masse de la mère, et smoke le fait que la mère fume ou non. Or la première de ces variables explicatives est quantitative (wt1) et l’autre (smoke) est une variable facteur à quatre niveaux (0 = la mère n’a jamais fumé, 1 = elle fume y compris pendant la grossesse, 2 = elle fumait mais a arrêté à la grossesses, et 3 = la mère a fumé, mais a arrêté, et ce, bien avant la grossesse. Un dernier niveau 9 = inconnu encode de manière non orthodoxe les valeurs manquantes dans notre tableau de données (valeurs que nous éliminerons). De même les masses des nouveaux nés et des mères sont des des unités impériales (américaines) respectivement en “onces” et en “livres”. Enfin, nous devons prendre soin de bien encoder la variable smoke comme une variable factor (ici nous ne considèrerons pas qu’il s’agit d’un facteur ordonné et nous voulons faire un contraste de type traitement avec comparaison à des mères qui n’ont jamais fumé). Un reminement soigneux des données est donc nécessaire avant de pouvoir appliquer notre modèle ! SciViews::R babies &lt;- read(&quot;babies&quot;, package = &quot;UsingR&quot;) knitr::kable(head(babies)) id pluralty outcome date gestation sex wt parity race age ed ht wt1 drace dage ded dht dwt marital inc smoke time number 15 5 1 1411 284 1 120 1 8 27 5 62 100 8 31 5 65 110 1 1 0 0 0 20 5 1 1499 282 1 113 2 0 33 5 64 135 0 38 5 70 148 1 4 0 0 0 58 5 1 1576 279 1 128 1 0 28 2 64 115 5 32 1 99 999 1 2 1 1 1 61 5 1 1504 999 1 123 2 0 36 5 69 190 3 43 4 68 197 1 8 3 5 5 72 5 1 1425 282 1 108 1 0 23 5 67 125 0 24 5 99 999 1 1 1 1 5 100 5 1 1673 286 1 136 4 0 25 2 62 93 3 28 2 64 130 1 4 2 2 2 Ce tableau est “brut de décoffrage”. Voyez help(&quot;babies&quot;, package = &quot;UsingR&quot;) pour de plus amples informations. Nous allons maintenant remanier tout cela correctement. # wt = masse du bébé à la naissance en onces et 999 = valeur manquante # wt1 = masse de la mère à la naissance en livres et 999 = valeur manquante # smoke = 0 (non), = 1 (oui), = 2 (jusqu&#39;à grossesse), # = 3 (plus depuis un certain temps) and = 9 (inconnu) babies %&gt;.% select(., wt, wt1, smoke) %&gt;.% # Garder seulement wt, wt1 &amp; smoke filter(., wt1 &lt; 999, wt &lt; 999, smoke &lt; 9) %&gt;.% # Eliminer les valeurs manquantes mutate(., wt = wt * 0.02835) %&gt;.% # Transformer le poids en kg mutate(., wt1 = wt1 * 0.4536) %&gt;.% # Idem mutate(., smoke = as.factor(smoke)) -&gt; # S&#39;assurer d&#39;avoir une variable factor Babies # Enregistrer le résultat dans Babies knitr::kable(head(Babies)) wt wt1 smoke 3.40200 45.3600 0 3.20355 61.2360 0 3.62880 52.1640 1 3.48705 86.1840 3 3.06180 56.7000 1 3.85560 42.1848 2 Description des données : skimr::skim(Babies) # Skim summary statistics # n obs: 1190 # n variables: 3 # # ── Variable type:factor ──────────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n n_unique top_counts # smoke 0 1190 1190 4 0: 531, 1: 465, 3: 102, 2: 92 # ordered # FALSE # # ── Variable type:numeric ─────────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n mean sd p0 p25 p50 p75 p100 # wt 0 1190 1190 3.39 0.52 1.56 3.06 3.4 3.71 4.99 # wt1 0 1190 1190 58.3 9.49 39.46 51.82 56.7 62.6 113.4 # hist # ▁▁▂▆▇▅▁▁ # ▂▇▆▂▁▁▁▁ chart(data = Babies, wt ~ wt1 %col=% smoke) + geom_point() + xlab(&quot;Masse de la mère [kg]&quot;) + ylab(&quot;Masse du bébé [kg]&quot;) chart(data = Babies, wt ~ smoke) + geom_boxplot() + ylab(&quot;Masse du bébé [kg]&quot;) chart(data = Babies, wt1 ~ smoke) + geom_boxplot() + ylab(&quot;Masse de la mère [kg]&quot;) Visuellement, nous ne voyons pas d’effet marquant. Peut-être la condition 1 de smoke (mère qui fume pendant la grossesse) mène-t-il à des bébés moins gros, mais est-ce significatif ? Pour cela, ajustons notre modèle ANCOVA avec matrice traitement (choix par défaut pour une la variable factor smoke). Comme nous savons déjà utiliser lm(), c’est très simple. Cela fonctionne exactement comme avant7. # ANCOVA Babies_lm &lt;- lm(data = Babies, wt ~ smoke * wt1) summary(Babies_lm) # # Call: # lm(formula = wt ~ smoke * wt1, data = Babies) # # Residuals: # Min 1Q Median 3Q Max # -1.9568 -0.3105 0.0133 0.3136 1.4989 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 3.000663 0.128333 23.382 &lt; 2e-16 *** # smoke1 -0.303614 0.196930 -1.542 0.123405 # smoke2 0.901888 0.371393 2.428 0.015314 * # smoke3 -0.035502 0.371379 -0.096 0.923858 # wt1 0.008117 0.002149 3.777 0.000167 *** # smoke1:wt1 0.001153 0.003346 0.345 0.730444 # smoke2:wt1 -0.015340 0.006390 -2.401 0.016523 * # smoke3:wt1 0.001177 0.006147 0.191 0.848258 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.4992 on 1182 degrees of freedom # Multiple R-squared: 0.08248, Adjusted R-squared: 0.07705 # F-statistic: 15.18 on 7 and 1182 DF, p-value: &lt; 2.2e-16 anova(Babies_lm) # Analysis of Variance Table # # Response: wt # Df Sum Sq Mean Sq F value Pr(&gt;F) # smoke 3 18.659 6.2197 24.9636 1.158e-15 *** # wt1 1 6.162 6.1621 24.7325 7.559e-07 *** # smoke:wt1 3 1.653 0.5511 2.2117 0.08507 . # Residuals 1182 294.497 0.2492 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 L’analyse de variance montre que la masse de la mère a un effet significatif au seuil alpha de 5%, de même si la mère fume. Par contre, il n’y a pas d’interactions entre les deux. Le fait de pouvoir meurer des interactions entre variables qualitatives et quantitatives est ici bien évidemment un plus du modèle linéaire par rapport à ce qu’on pouvait faire avant ! Le résumé de l’analyse nous montre que la régression de la masse des bébés en fonction de la masse de la mère (ligne wt1 dans le tableau des coefficients), bien qu’étant significative, n’explique que 8% de la variance totale (le \\(R^2\\)). Les termes smoke1, smoke2 et smoke3 sont les contrastes appliqués par rapport au contrôle (smoke == 0). On voit ici qu’aucun de ces contrastes n’est significatif au seuil alpha de 5%. Cela signifie que le seul effet significatif est celui lié à une ordonnée à l’origine non nulle (Intercept) matérialisant la condition smoke == 0. Cela signifie que des mères de masse nulle n’ayant jamais fumé engendreraient des bébés pesant environ 3kg. Dans le contexte présent, cette constatation n’a bien sûr aucun sens, et l’interprétation de l’ordonnée à l’origine ne doit pas être faite. Donc, le modèle linéaire, en offrant plus de contrôle dans notre ajustement et une définition de contrastes “utiles” matérialisés par les lignes smoke1, smoke2 et smoke3 du tableau nous permet de faire des tests plus utiles dans le contexte de notre analyse. N’oublions pas non plus la possibilité de déterminer si des interactions entre smoke et wt1 existent pour ces différents contrastes, interactions testées respectivements aux lignes smoke1:wt1, smoke2:wt1, et smoke3:wt1du tableau des coefficients. Dans le cas présent, aucune de ces interactions n’est siginificative au seuil alpha de 5%. Pour comprendre à quoi tout cela fait référence, il faut considérer le modèle de base comme une droite de régression ajustée entre wt et wt1 pour la population de référence smoke == 0. Ainsi, si nous faisons : summary(lm(data = Babies, wt ~ wt1, subset = smoke == 0)) # # Call: # lm(formula = wt ~ wt1, data = Babies, subset = smoke == 0) # # Residuals: # Min 1Q Median 3Q Max # -1.95685 -0.25825 0.01476 0.25464 1.49890 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 3.000663 0.123572 24.283 &lt; 2e-16 *** # wt1 0.008117 0.002069 3.922 9.92e-05 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.4806 on 529 degrees of freedom # Multiple R-squared: 0.02826, Adjusted R-squared: 0.02642 # F-statistic: 15.38 on 1 and 529 DF, p-value: 9.924e-05 Nous voyons en effet que les pentes et ordonnées à l’origine sont ici parfaitement identiques au modèle ANCOVA complet (mais pas les tests associés). Maintenant plus difficile : à quoi correspond une régression entre wt et wt1 pour smoke == 1 ? summary(lm(data = Babies, wt ~ wt1, subset = smoke == 1)) # # Call: # lm(formula = wt ~ wt1, data = Babies, subset = smoke == 1) # # Residuals: # Min 1Q Median 3Q Max # -1.70870 -0.35089 0.01034 0.33576 1.39420 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 2.697048 0.153270 17.597 &lt; 2e-16 *** # wt1 0.009270 0.002632 3.522 0.000471 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.5122 on 463 degrees of freedom # Multiple R-squared: 0.02609, Adjusted R-squared: 0.02399 # F-statistic: 12.4 on 1 and 463 DF, p-value: 0.0004711 Nous avons une ordonnées à l’origine qui vaut 2,70 ici. Notons que cela correspond aussi à (Intercept) + smoke1 = 3,00 - 0,30 = 2,70. Donc, l’ordonnées à l’origine pour smoke == 1 est bien la valeur de référence additionnée de la valeur fournie à la ligne smoke1 dans l’ANCOVA. Cela se vérifie aussi pour les deux autres droites pour smoke2 et smoke3. Maintenant, la pente pour notre droite ajustée sur la population smoke == 1 uniquement vaut 0,00927. Dans l’ANCOVA, nous avions une pente wt1 de 0,00812 et une interaction smoke1:wt1 claculée comme 0,00115. Notez alors que la pente de la droite seule 0,00927 = 0,00812 + 0,00115. Donc, tout comme smoke1 correspond au décalage de l’ordonnée à l’origine du modèle de référence, les interactions smoke1:wt1 correspondent au décalage de la pente par rapport au modèle de référence. Cela se vérifie également pour smoke2:wt1 et smoke3:wt1. Donc, notre modèle complet ne fait rien d’autre que d’ajuster les quatre droites correspondant aux relations linéaires entre wt et wt1, mais en décompose les effets, niveau par niveau de la variable qualitative smoke en fonction de la matrice de contraste que l’on a choisie. En bonnus, nous avons la possibilité de tester si chacune des composantes (tableau coefficient de summary()) ou si globalement chacune des variables (tableau obtenu avec anova()) a un effet significatif ou non dans le modèle. Le graphique correspondant est le même que si nous avions ajusté les 4 régressions linéaires indépendamment l’une de l’autre (mais les tests et les enveloppes de confiance diffèrent). chart(data = Babies, wt ~ wt1 %col=% smoke) + geom_point() + stat_smooth(method = &quot;lm&quot;, formula = y ~ x) + xlab(&quot;Masse de la mère [kg]&quot;) + ylab(&quot;Masse du bébé [kg]&quot;) chart(data = Babies, wt ~ wt1 | smoke) + geom_point() + stat_smooth(method = &quot;lm&quot;, formula = y ~ x) + xlab(&quot;Masse de la mère [kg]&quot;) + ylab(&quot;Masse du bébé [kg]&quot;) Comme toujours, lorsqu’un effet n’est pas siugnificatif, nous pouvons décider de simplifier le modèle. Mais attention ! Toujours considérer que les composantes sont interdépendantes. Donc, éliminer une composante du modèle peut avoir des effets parfois surprenants sur les autres. Voyons ce que cela donne si nous éliminons les interactions. Dans ce cas, nous ajustons des droites toutes parallèles avec uniquement un décalage de leur ordonnée à l’origine matérialisé par smoke1, smoke2 et smoke3 par rapport au modèle de référence ajusté pour la population smoke == 0 (notez l’utilisation, du signe + dans la formuile, là où nous utilisions le signe * dans la modèle précédent). # ANCOVA Babies_lm2 &lt;- lm(data = Babies, wt ~ smoke + wt1) summary(Babies_lm2) # # Call: # lm(formula = wt ~ smoke + wt1, data = Babies) # # Residuals: # Min 1Q Median 3Q Max # -1.95453 -0.30780 0.01289 0.31108 1.49443 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 3.030052 0.092861 32.630 &lt; 2e-16 *** # smoke1 -0.237938 0.031816 -7.478 1.46e-13 *** # smoke2 0.022666 0.056508 0.401 0.688 # smoke3 0.035486 0.054068 0.656 0.512 # wt1 0.007617 0.001534 4.966 7.85e-07 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.4999 on 1185 degrees of freedom # Multiple R-squared: 0.07733, Adjusted R-squared: 0.07422 # F-statistic: 24.83 on 4 and 1185 DF, p-value: &lt; 2.2e-16 anova(Babies_lm2) # Analysis of Variance Table # # Response: wt # Df Sum Sq Mean Sq F value Pr(&gt;F) # smoke 3 18.659 6.2197 24.887 1.285e-15 *** # wt1 1 6.162 6.1621 24.657 7.853e-07 *** # Residuals 1185 296.150 0.2499 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Hé, ça c’est intéressant ! Maintenant que nous avons éliminé les interactions qui apparaissent non pertinentes ici, nous avons toujours une régression significative entre wt et wt1 (mais avec un \\(R^2\\) très faible de 7,7%, attention), mais maintenant, nous faisons apparaitre un effet signicfication du contraste avec smoke1 au seuil alpha de 5%. Et du coup, les effets des deux variables deviennent plus clairs dans notre tableau de l’ANOVA. Le graphique correspondant est l’ajustement de droites parallèles les unes aux autres pour les 4 sous-populations en fonction de smoke. Ce graphique est difficile à réaliser. Il faut ruser, et les détails du code vont au delà de ce cours (il n’est pas nécessaire de les comprendre à ce stade). cols &lt;- iterators::iter(scales::hue_pal()(4)) # Get colors for lines chart(data = Babies, wt ~ wt1) + geom_point(aes(col = smoke)) + lapply(c(0, -0.238, 0.0227, 0.0355), function(offset) geom_smooth(method = lm, formula = y + offset ~ x, col = iterators::nextElem(cols))) + xlab(&quot;Masse de la mère [kg]&quot;) + ylab(&quot;Masse du bébé [kg]&quot;) Voyons ce que donne l’analyse post hoc des comparaisons multiples (nous utilisons ici simplement le snippet disponible à partir de ... -&gt; hypothesis tests -&gt; hypothesis tests: means -&gt; hmanovamult : anova - multiple comparaisons [multcomp]) que nous avons déjà employé et qui reste valable ici. summary(anovaComp. &lt;- confint(multcomp::glht(Babies_lm2, linfct = multcomp::mcp(smoke = &quot;Tukey&quot;)))) # # Simultaneous Tests for General Linear Hypotheses # # Multiple Comparisons of Means: Tukey Contrasts # # # Fit: lm(formula = wt ~ smoke + wt1, data = Babies) # # Linear Hypotheses: # Estimate Std. Error t value Pr(&gt;|t|) # 1 - 0 == 0 -0.23794 0.03182 -7.478 &lt;1e-04 *** # 2 - 0 == 0 0.02267 0.05651 0.401 0.977 # 3 - 0 == 0 0.03549 0.05407 0.656 0.908 # 2 - 1 == 0 0.26060 0.05704 4.568 &lt;1e-04 *** # 3 - 1 == 0 0.27342 0.05478 4.991 &lt;1e-04 *** # 3 - 2 == 0 0.01282 0.07199 0.178 0.998 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # (Adjusted p values reported -- single-step method) .oma &lt;- par(oma = c(0, 5.1, 0, 0)); plot(anovaComp.); par(.oma); rm(.oma) Ici, comme nous testons tous les contrastes, nous pouvons dire que la population des mères qui ont fumé pendant la grossesse smoke == 1 donne des bébés significativement moins gros au seuil alpha de 5%, et ce, en comparaison de tous les autres niveaux (mère n’ayant jamais fumé, ou ayant fumé mais arrêté avant la grossesse, que ce soit longtemps avant ou juste avant). Il semble évident maintenant qu’il n’est pas utile de préciser si la mère a fumé ou non avant sa grossesse. L’élément déterminant est uniquement le fait de fumer pendant la grossesse ou non. Nous pouvons le montrer également en utilisant des contrastes de Helmert, à condition de recoder smoke avec des niveaux de “gravité” croissants (“0” = n’a jamais fumé, “1” = a arrêté il y a longtemps, “2” = a arrêté juste avant la grossesse et finalement, “1” = a continué à fumé à la grossesse). Il faut donc intervertir les cas “1” et “3”. Nous pouvons utiliser recode() pour cela, mais attention, nous avons ici une variable factor, donc, ce ne sont pas des nombres mais des chaines de caractères (à placer entre guillements). Une fois le recodage réalisé, il faut aussi retrier les niveaux en appelant factor(..., levels = c(&quot;0&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;)) sinon l’ancien ordre est conservé. Babies %&gt;.% mutate(., smoke = recode(smoke, &quot;0&quot; = &quot;0&quot;, &quot;1&quot; = &quot;3&quot;, &quot;2&quot; = &quot;2&quot;, &quot;3&quot; = &quot;1&quot;)) %&gt;.% mutate(., smoke = factor(smoke, levels = c(&quot;0&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;))) -&gt; Babies2 Si cela semble trop compliqué, vous pouvez aussi utiliser l’addins de réencodage dans R (QUESTIONR -&gt; Levels Recoding or Levels Ordering). A présent que l’encodage de smoke est corrigé dans Babies2, nous pouvons modéliser à nouveau, mais cette fois-ci avec des contrastes de Helmert (notez la façon particulière de spécifier des contrastes différents de la valeur pas défaut pour une variable factor) : Babies_lm3 &lt;- lm(data = Babies2, wt ~ smoke + wt1, contrasts = list(smoke = &quot;contr.helmert&quot;)) summary(Babies_lm3) # # Call: # lm(formula = wt ~ smoke + wt1, data = Babies2, contrasts = list(smoke = &quot;contr.helmert&quot;)) # # Residuals: # Min 1Q Median 3Q Max # -1.95453 -0.30780 0.01289 0.31108 1.49443 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 2.985106 0.091695 32.555 &lt; 2e-16 *** # smoke1 0.017743 0.027034 0.656 0.512 # smoke2 0.001641 0.019599 0.084 0.933 # smoke3 -0.064330 0.008540 -7.533 9.82e-14 *** # wt1 0.007617 0.001534 4.966 7.85e-07 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.4999 on 1185 degrees of freedom # Multiple R-squared: 0.07733, Adjusted R-squared: 0.07422 # F-statistic: 24.83 on 4 and 1185 DF, p-value: &lt; 2.2e-16 anova(Babies_lm3) # Analysis of Variance Table # # Response: wt # Df Sum Sq Mean Sq F value Pr(&gt;F) # smoke 3 18.659 6.2197 24.887 1.285e-15 *** # wt1 1 6.162 6.1621 24.657 7.853e-07 *** # Residuals 1185 296.150 0.2499 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Ici les valeurs estimées pour smoke1-3 sont à interpréter en fonction des contrastes utilisés, soit : contr.helmert(4) # [,1] [,2] [,3] # 1 -1 -1 -1 # 2 1 -1 -1 # 3 0 2 -1 # 4 0 0 3 smoke1 est le décalage de l’ordonnée à l’origine entre la modèle moyen établi avec les données smoke == 0 et smoke == 1 et celui pour smoke == 1 (non significatif au seuil alpha de 5%), smoke2 est le décalage de l’ordonnée à l’origine pour smoke == 2 par rapport au modèle ajusté sur smoke == 0, smoke == 1 et smoke == 2 avec des pondérations respectives de -1, -1, et 2 (non significatif au seuil alpha de 5%), smoke3 est le décalage de l’ordonnée à l’origine par rapport au modèle ajusté sur l’ensemble des autres observations, donc, avec smoke valant 0, 1, ou 2, et des pondérations respectives comme dans la dernière colonne de la matrice de contraste.. Donc, ce dernier contraste est celui qui nous intéresse car il compare les cas où la mère n’a pas fumé pendant la grossesse avec le cas smoke == 3 où la mère a fumé pendant la grossesse, et il est significatif au seuil alpha de 5%. L’interprétation des vlauers estimées est plus complexe ici. Comparer ce résultat avec le modèle ajusté avec les contrastes traitement par défaut avec smoke réencodé : summary(lm(data = Babies2, wt ~ smoke + wt1)) # # Call: # lm(formula = wt ~ smoke + wt1, data = Babies2) # # Residuals: # Min 1Q Median 3Q Max # -1.95453 -0.30780 0.01289 0.31108 1.49443 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 3.030052 0.092861 32.630 &lt; 2e-16 *** # smoke1 0.035486 0.054068 0.656 0.512 # smoke2 0.022666 0.056508 0.401 0.688 # smoke3 -0.237938 0.031816 -7.478 1.46e-13 *** # wt1 0.007617 0.001534 4.966 7.85e-07 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.4999 on 1185 degrees of freedom # Multiple R-squared: 0.07733, Adjusted R-squared: 0.07422 # F-statistic: 24.83 on 4 and 1185 DF, p-value: &lt; 2.2e-16 Les conclusions sont les mêmes, mais la valeurs estimées pour smoke1, smoke2 et smoke3 diffèrent. Par exemple, dans ce dernier cas, smoke1 est double de la valeur avec les contrastes Helmert, ce qui est logique puisque la référence est ici la droite ajustée pour la sous-population smoke == 0 là où dans le modèle avec les contrastes de Helmert, le décalage est mesuré par rapport au modèle moyen (donc à “mi-chemin” entre les deux droites pour smoke == 0 et smoke == 1). Naturellement, nous pouvons aussi considérer la variable smoke réencodée dans Babies2 comme une variable facteur ordonné (ordered). Dans ce cas, c’est les contrastes polynomiaux qui sont utilisés : Babies2 %&gt;.% mutate(., smoke = as.ordered(smoke)) -&gt; Babies3 summary(lm(data = Babies3, wt ~ smoke + wt1)) # # Call: # lm(formula = wt ~ smoke + wt1, data = Babies3) # # Residuals: # Min 1Q Median 3Q Max # -1.95453 -0.30780 0.01289 0.31108 1.49443 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 2.985106 0.091695 32.555 &lt; 2e-16 *** # smoke.L -0.162480 0.026780 -6.067 1.75e-09 *** # smoke.Q -0.148045 0.039294 -3.768 0.000173 *** # smoke.C -0.044605 0.048790 -0.914 0.360787 # wt1 0.007617 0.001534 4.966 7.85e-07 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.4999 on 1185 degrees of freedom # Multiple R-squared: 0.07733, Adjusted R-squared: 0.07422 # F-statistic: 24.83 on 4 and 1185 DF, p-value: &lt; 2.2e-16 Notez comment R est capable d’utiliser automatiquement les contrasts adéquats (polynomiaux) lorsque la variable facteur smoke est encodée en ordered. Nous voyons ici que des contrastes tenant compte d’une variation le long des successions croissante de niveaux de “gravité” de la variable smoke sont maintenant calculés. La ligne smoke.L du tableau Coefficients indique une variation linéaire (significative au seuil alpha de 5%), smoke.Q est une variation quadratique (également significative) et enfin smoke.C est une variation cubique. Voyez la présentation des matrices de contrastes plus haut pour bien comprendre ce qui est calculé ici. Au final, l’élément important relatif à la variable smoke est en définitive le fait de fumer pendant la grossesse ou non, pas l’histoire de la mère avant sa grossesse en matière de tabocologie ! En modélisation, nous avons toujours intérêt à choisir le modèle le plus simple. Donc ici, cela vaut le coup de simplifier smoke à une variable à deux niveaux smoke_preg qui indique uniquement si la mère fume ou non pendant la grossesse. Ensuite, nous ajustons à nouveau un modèle plus simple avec cette nouvelle variable. Babies %&gt;.% mutate(., smoke_preg = recode(smoke, &quot;0&quot; = &quot;0&quot;, &quot;1&quot; = &quot;1&quot;, &quot;2&quot; = &quot;0&quot;, &quot;3&quot; = &quot;0&quot;)) %&gt;.% mutate(., smoke_preg = factor(smoke_preg, levels = c(&quot;0&quot;, &quot;1&quot;))) -&gt; Babies Babies_lm4 &lt;- lm(data = Babies, wt ~ smoke_preg + wt1) summary(Babies_lm4) # # Call: # lm(formula = wt ~ smoke_preg + wt1, data = Babies) # # Residuals: # Min 1Q Median 3Q Max # -1.96243 -0.30708 0.01208 0.31051 1.48662 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 3.037508 0.091896 33.054 &lt; 2e-16 *** # smoke_preg1 -0.245797 0.029747 -8.263 3.76e-16 *** # wt1 0.007624 0.001531 4.981 7.25e-07 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.4996 on 1187 degrees of freedom # Multiple R-squared: 0.07692, Adjusted R-squared: 0.07537 # F-statistic: 49.46 on 2 and 1187 DF, p-value: &lt; 2.2e-16 anova(Babies_lm4) # Analysis of Variance Table # # Response: wt # Df Sum Sq Mean Sq F value Pr(&gt;F) # smoke_preg 1 18.497 18.4971 74.106 &lt; 2.2e-16 *** # wt1 1 6.193 6.1934 24.813 7.253e-07 *** # Residuals 1187 296.281 0.2496 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 A présent, tous les termes de notre modèle sont significatifs au seuil alpha de 5%. La ligne smoke_preg1 est le décalage de l’ordonnée à l’origine du poids des bébés issus de mères fumant pendant la grossesse. Il donne donc directement la perte moyenne de poids du à la tabacologie. La représentation graphique de ce dernier modèle est la suivante : cols &lt;- iterators::iter(scales::hue_pal()(2)) # Get colors for lines chart(data = Babies, wt ~ wt1) + geom_point(aes(col = smoke_preg)) + lapply(c(0, -0.246), function(offset) geom_smooth(method = lm, formula = y + offset ~ x, col = iterators::nextElem(cols))) + xlab(&quot;Masse de la mère [kg]&quot;) + ylab(&quot;Masse du bébé [kg]&quot;) Enfin, n’oublions pas que notre modèle n’est valide que si les conditions d’application sont rencontrées, en particulier, une distribution normale des résidus et une homoscédasticité (même variance pour les résidus). Nous vérifions cela visuellement toujours avec les graphiques d’analyse des résidus. En voici les plus importants (pensez à utiliser les snippets pour récupérer le template du code) : #plot(Babies_lm4, which = 1) Babies_lm4 %&gt;.% chart(broom::augment(.), .resid ~ .fitted) + geom_point() + geom_hline(yintercept = 0) + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = &quot;Residuals&quot;) + ggtitle(&quot;Residuals vs Fitted&quot;) #plot(Babies_lm4, which = 2) Babies_lm4 %&gt;.% chart(broom::augment(.), aes(sample = .std.resid)) + geom_qq() + geom_qq_line(colour = &quot;darkgray&quot;) + labs(x = &quot;Theoretical quantiles&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Normal Q-Q&quot;) #plot(Babies_lm4, which = 3) Babies_lm4 %&gt;.% chart(broom::augment(.), sqrt(abs(.std.resid)) ~ .fitted) + geom_point() + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = expression(bold(sqrt(abs(&quot;Standardized residuals&quot;))))) + ggtitle(&quot;Scale-Location&quot;) #plot(Babies_lm4, which = 4) Babies_lm4 %&gt;.% chart(broom::augment(.), .cooksd ~ seq_along(.cooksd)) + geom_bar(stat = &quot;identity&quot;) + geom_hline(yintercept = seq(0, 0.1, by = 0.05), colour = &quot;darkgray&quot;) + labs(x = &quot;Obs. number&quot;, y = &quot;Cook&#39;s distance&quot;) + ggtitle(&quot;Cook&#39;s distance&quot;) Ici le comportement des résidus est sain. Des petits écarts de la normalité sur le graphique quantile-quantile s’observent peut-être, mais ce n’est pas dramatique et le modèle linéaire est rabuste à ce genre de petis changements d’autant plus qu’ils apparaissent relativement symétriques en haut et et en bas de la distribution. En conclusion de cette analyse, nous pouvons dire que la masse du bébé dépend de la masse de la mère, mais assez faiblement (seulement 7,7% de la variance totale expliquée). Par contre, nous pouvons aussi dire que le fait de fumer pendant la grossesse a un effet significatif sur la réduction de la masse du bébé à la naissance (en moyenne cette réduction est de 0,246kg pour une masse moyenne à la naissance de 3,038kg, soit une réduction de 0,246 / 3,034 * 100 = 8%). Voilà, nous venons d’analyser et d’interpréter notre premier modèle linéaire sous forme d’une ANCOVA. A vous de jouer ! Après cette longue lecture avec énormément de nouvelles matières, nous vous proposons les exercices suivants : Répondez aux questions d’un learnr afin de vérifier vos acquis. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant les bases de R : BioDataScience2::run(&quot;03a_mod_lin&quot;) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel dans la console R. Poursuivez l’analyse des données sur la biométrie des oursins en y intégrant vos nouvelles notions sur le modèle linéaire Reprenez votre travail sur la biométrie des oursins et appliquer les nouvelles notions vues Références "],
["modele-lineaire-generalise.html", "3.5 Modèle linéaire généralisé", " 3.5 Modèle linéaire généralisé Le modèle linéaire nous a permis de combiner différent types de variables indépendantes ou explicatives dans un même modèle. Cependant la variable dépendante ou réponse à la gauche de l’équation doit absolument être numérique et une distribution normale est exigée pour la composante statistique du modèle exprimée dans les résidus \\(\\epsilon\\). Donc, si nous voulons modéliser une variable dépendante qui ne répond pas à ces caractéristiques, nous sommes dans l’impasse avec la fonction lm(). Dans certains cas, une transformation des données peut résoudre le problème. Par exemple, prendre le logarithme d’une variable qui a une distribution log-normale. Dans d’autres cas, il semble qu’il n’y ait pas de solution… C’est ici que la modèle linéaire généralisé vient nous sauver la mise. Le modèle linéaire généralisé se représente comme suit : \\[ f(y) = \\beta_1 + \\beta_2 I_2 + \\beta_3 I_3 + ... + \\beta_k I_k + \\beta_l x_1 + \\beta_m x_2 + ... + \\epsilon \\] La différence par rapport au modèle linéaire, c’est que notre variable dépendante \\(y\\) est transformée à l’aide d’une fonction \\(f(y)\\) que l’on appelle fonction de lien. Cette fonction de lien est choisie soigneusement pour transformer une variable qui a une distribution non-normale vers une distribution normale ou quasi-normale. Du coup, il ne faut rien changer à la droite du signe égal par rapport au modèle linéaire, et les outils existants peuvent être réemployés. Toute la difficulté ici tient donc à la définition des fonctions de liens pertinentes par rapport à la distribution de \\(y\\). Le tableau suivant reprend les principales situations prises en compte par la fonction glm() dans R qui calcule le modèle linéaire généralisé. Distribution de Y Fonction de lien Code R Gaussienne (Normale) identité (pas de transfo.) glm(..., family = gaussian(link = &quot;identity&quot;)) Log-Normale log glm(..., family = gaussian(link = &quot;log&quot;)) Binomiale logit glm(..., family = binomial(link = logit)) Binomiale probit (alternative) glm(..., family = binomial(link = probit)) Poisson log glm(..., family = poisson(link = log)) Il en existe bien d’autres. Voyez l’aide de ?family pour plus de détails. Par exemple, pour une variable réponse binaire acceptant seulement deux valeurs possibles et ayant une distribution binomiale, avec une réponse de type logistique (une variation croissante d’une ou plusieurs variables indépendantes fait passer la proportion des individus appartenant au second état selon une courbe logistique en S), une fonction de type logit est à utiliser. \\[ y = 1/(1 + e^{- \\beta x}) \\] La transformation logit calcule alors : \\(\\ln(y / (1 - y)) = \\beta x\\). Les situations correspondant à ce cas de figure concernent par exemple des variables de type (vivant versus mort) par rapport à une situation potentiellement léthale, ou alors, le développement d’une maladie lors d’une épidémie (sain versus malade). 3.5.1 Exemple Continuons à analyser nos données concernant les bébés à la naissance. Un bébé prématuré est un bébé qui nait avant 37 semaines de grossesse. Dans notre jeu de données Babies, nous pouvons déterminer si un enfant est prématuré ou non (variable binaire) à partir de la variable gestation(en jours). Transformons nos données pour obtenir les variables d’intérêt. SciViews::R babies &lt;- read(&quot;babies&quot;, package = &quot;UsingR&quot;) babies %&gt;.% select(., gestation, smoke, wt1, ht, race, age) %&gt;.% # Eliminer les valeurs manquantes filter(., gestation &lt; 999, smoke &lt; 9, wt1 &lt; 999, ht &lt; 999, race &lt; 99, age &lt; 99) %&gt;.% # Transformer wt1 en kg et ht en cm mutate(., wt1 = wt1 * 0.4536) %&gt;.% mutate(., ht = ht / 39.37) %&gt;.% # Transformer smoke en variable facteur mutate(., smoke = as.factor(smoke)) %&gt;.% # Idem pour race mutate(., race = as.factor(race)) %&gt;.% # Déterminer si un bébé est prématuré ou non (en variable facteur) mutate(., premat = as.factor(as.numeric(gestation &lt; 7*37))) %&gt;.% # Calculer le BMI comme meilleur index d&#39;embonpoint des mères que leur masse mutate(., bmi = wt1 / ht^2) -&gt; Babies_prem Comment se répartissent les enfants entre prématurés et nés à terme ? table(Babies_prem$premat) # # 0 1 # 1080 96 Nous avons un nombre relativement faible de prématurés dans l’ensemble. C’était à prévoir. Attention à un plan très mal balancé ici : c’est défavorable à une bonne analyse, mais pas rédhibitoire. Décrivons ces données. Babies_table &lt;- table(Babies_prem$premat, Babies_prem$smoke) knitr::kable(addmargins(Babies_table)) 0 1 2 3 Sum 0 486 420 83 91 1080 1 39 40 9 8 96 Sum 525 460 92 99 1176 Ce tableau de contingence ne nous donne pas encore l’idée de la répartition de prématurés en fonction de statut de fumeuse de la mère, mais le graphique suivant nous le montre. chart(data = Babies_prem, ~smoke %fill=% premat) + geom_bar(position = &quot;fill&quot;) Il ne semble pas y avoir un effet flagrant, même si le niveau smoke == 2 semble contenir une plus forte proportion de prématurés. Qu’en est-il en fonction de l’éthnicité (voir help(babies, packahge = &quot;UsingR&quot;) pour le détail sur les variétés éthniques considérées) de la mère (variable race) ? Babies_table &lt;- table(Babies_prem$premat, Babies_prem$race) knitr::kable(addmargins(Babies_table)) 0 1 2 3 4 5 6 7 8 9 10 Sum 0 491 42 22 58 50 124 31 192 34 24 12 1080 1 24 2 5 1 6 8 3 41 5 1 0 96 Sum 515 44 27 59 56 132 34 233 39 25 12 1176 chart(data = Babies_prem, ~race %fill=% premat) + geom_bar(position = &quot;fill&quot;) Ici, nous voyons déjà un effet semble-t-il plus marqué. Qu’en est-il du BMI ? chart(data = Babies_prem, bmi ~ premat) + geom_boxplot() + ylab(&quot;BMI de la mère&quot;) Sur ce graphique, il ne semble pas y avoir d’influence du BMI sur le fait d’avoir un enfant prématuré ou non. Enfin, l’âge de la mère influence-t-il également ? chart(data = Babies_prem, age ~ premat) + geom_boxplot() + ylab(&quot;Age de la mère (an)&quot;) Il ne sembnle pas y avoir un effet flagrant. Voyons ce que donne le modèle (nous ne considérons pas les interactions possibles ici, mais cela doit être fait dans le cas de plusieurs effets significatifs au moins). Avec 4 variables explicatives, le modèle est déjà très complexe sans interactions. Nous serions trop ambitieux de vouloir ici ajuster un modèle complet ! # Modèle linéaire généralisé avec fonction de lien de type logit Babies_glm &lt;- glm(data = Babies_prem, premat ~ smoke + race + bmi + age, family = binomial(link = logit)) summary(Babies_glm) # # Call: # glm(formula = premat ~ smoke + race + bmi + age, family = binomial(link = logit), # data = Babies_prem) # # Deviance Residuals: # Min 1Q Median 3Q Max # -0.6875 -0.4559 -0.3228 -0.2857 2.8199 # # Coefficients: # Estimate Std. Error z value Pr(&gt;|z|) # (Intercept) -3.349082 0.860945 -3.890 0.00010 *** # smoke1 0.247161 0.244442 1.011 0.31196 # smoke2 0.247474 0.401007 0.617 0.53715 # smoke3 0.245883 0.417970 0.588 0.55634 # race1 -0.037501 0.754621 -0.050 0.96037 # race2 1.510254 0.539895 2.797 0.00515 ** # race3 -1.017780 1.030541 -0.988 0.32334 # race4 0.891191 0.481411 1.851 0.06414 . # race5 0.303990 0.421363 0.721 0.47064 # race6 0.750868 0.644376 1.165 0.24391 # race7 1.495166 0.280313 5.334 9.61e-08 *** # race8 1.155898 0.531592 2.174 0.02967 * # race9 -0.123997 1.043118 -0.119 0.90538 # race10 -13.513140 691.812473 -0.020 0.98442 # bmi -0.001689 0.032621 -0.052 0.95871 # age 0.007806 0.019119 0.408 0.68307 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # (Dispersion parameter for binomial family taken to be 1) # # Null deviance: 665.00 on 1175 degrees of freedom # Residual deviance: 618.84 on 1160 degrees of freedom # AIC: 650.84 # # Number of Fisher Scoring iterations: 15 Nous voyons que le résumé de l’objet glm est très similaire à celui d’un objet lm, notamment avec un tableau des Coefficients identique et qui s’interprète de la même manière. Ici, nous pouvons confirmer que ni le fait de fumer, ni l’âge, ni le BMI de la mère n’a d’incidence sur les bébés prématurés au seuil alpha de 5%. En revanche, certaines éthnies sont significativement plus susceptibles d’accoucher avant terme. Cela suggère soit un facteur génétique, soit un facteur environnmental/culturel lié à ces éthnies. Naturellement, il faudrait ici simplifier le modèle qui se ramène en fin de compte à l’équivalent d’une ANOVA à un facteur, mais en version glm() une fois que les variables non significatives sont éliminées. De même, on pourrait légitimement se demander si la variable premat ne pourrait pas aussi être modélisée avec une autre fonction de lien en considérant une distribution de Poisson par exemple. A vous de voir… A vous de jouer ! Réalisez un rapport scientifique sur la maturation d’ovocytes, en définissant un modèle linéaire généralisé le plus pertinent pour ces données. Vous avez à votre disposition une assignation GitHub Classroom : https://classroom.github.com/a/mXAIu4Ir Lisez le README afin de prendre connaissance de l’exercice Réalisez un rapport scientifique sur la biométrie humaine Vous avez à votre disposition une assignation GitHub Classroom : https://classroom.github.com/a/hS069etL Lisez le README afin de prendre connaissance de l’exercice "],
["reg-non-lin.html", "Module 4 Régression non linéaire", " Module 4 Régression non linéaire Objectifs Comprendre comment ajuster une courbe dans un nuage de points à l’aide de la régression non linéaire par les moindres carrés Apprendre à réaliser une régression non linéaire dans R, éventuellement en utilisant des modèles “self-start” Comparer les modèles à l’aide du coefficient d’Akaike Connaitre quelques unes des courbes mathématiques les plus utilisées en biologie Prérequis Les modules 1 &amp; 2 du présent cours concernant la régression linéaire sont une entrée en matière indispensable puisque la régression va être abordée ici comme une extension de ce qui a déjà été vu. "],
["rendement-photosynthetique.html", "4.1 Rendement photosynthétique", " 4.1 Rendement photosynthétique Afin d’avoir un premier aperçu de ce qu’est une régression non linéaire par les moindres carrés et comment on la calcule dans R, nous allons résoudre un exemple concret. La posidonie (Posidonia oceanica (L.) Delile (1813)) est une plante à fleur marine qui forme des herbiers denses en mer Méditerranée. Ses feuilles sont particulièrement adaptées à l’utilisation de la lumière qui règne à quelques mètres en dessous de la surface où elle prospère en herbiers denses. (un mémoire portant sur l’étude du diméthylsulfoniopropionate et du diméthylsulfoxyde chez Posidonia oceanica (L.) Delile (1813) a été réalisé au sein du service d’Écologie numérique des Milieux aquatiques). Herbier de posidonies Pour étudier le rendement de sa photosynthèse, c’est-à-dire la part du rayonnement lumineux reçu qui est effectivement utilisée pour initier la chaîne de transport d’électrons au sein de son photosystème II, nous pouvons utiliser un appareil spécialisé : le diving PAM. Diving PAM Cet appareil est capable de déterminer le taux de transfert des électrons (ETR en µmol électrons/m2/s) par l’analyse de la fluorescence réémise par la plante lorsque ses photosites sont excités par une lumière monochromatique pulsée (Walz 2018). Une façon de déterminer la réponse de la plante en rendement photosynthétique en fonction de l’intensité de la lumière reçue est de mesurer successivement l’ETR pour différentes intensités de lumière. En anglais cela s’appelle la “Rapid Light Curve” ou RLC en abbrégé. Comme toutes les longueurs d’ondes lumineuses ne sont pas utilisables par la chlorophylle, l’intensité lumineuse est exprimé dans une unité particulière, le “PAR” ou “Photosynthetically Active Radiation” en µmol photons/m2/s. Une RLC represente donc la variation de l’ERT en fonction des PAR8. Une RLC typique commence par une relation quasi-linéaire aux faibles intensités, pour s’infléchir et atteindre un plateau de rendement maximum. Au delà, si l’intensité lumineuse augmente encore, des phénomènes de photoinhibition apparaissent et le rendement diminue dans une troisième phase. Voici une RLC mesurée à l’aide du diving PAM sur une feuille de P. oceanica. rlc &lt;- tribble( ~etr, ~par, 0.0, 0, 0.5, 2, 3.2, 11, 5.7, 27, 7.4, 50, 8.4, 84, 8.9, 170, 8.4, 265, 7.8, 399 ) rlc &lt;- labelise(rlc, self = FALSE, label = list(etr = &quot;ETR&quot;, par = &quot;PAR&quot;), units = list(etr = &quot;µmol électrons/m^2/s&quot;, par = &quot;µmol photons/m^2/s&quot;) ) chart(data = rlc, etr ~ par) + geom_point() Les trois phases successives sont bien visibles ici (linéaire pour des PAR de 0 à 25, plateau à des PAR de 150-200 et photoinhibition à partir de 200 PAR). Naturellement, une régression linéaire dans ces données n’a pas de sens. Une régression polynomiale d’ordre trois donnerait ceci (code issu du snippet correspondant) : rlc_lm &lt;- lm(data = rlc, etr ~ par + I(par^2) + I(par^3)) rlc_lm %&gt;.% (function(lm, model = lm[[&quot;model&quot;]], vars = names(model)) chart(model, aes_string(x = vars[2], y = vars[1])) + geom_point() + stat_smooth(method = &quot;lm&quot;, formula = y ~ x + I(x^2) + I(x^3)))(.) La régression polynomiale tente maladroitement de s’ajuster dans les données mais est incapable de retranscrire les trois phases correctement. En particulier, la troisième est incorrecte puisque le modèle semble indiquer une reprise du rendement aux intensités les plus élevées. Une représentation incorrecte est à craindre lorsque le modèle mathématique utilisé ne représente pas les différentes caractéristiques du phénomène étudié. L’utilisation d’un modèle adéquat est possible ici seulement par régression non linéaire. En effet, aucune transformation monotone croissante ou décroissante ne peut linéariser ce type de données puisque la courbe monte d’abord pour s’infléchir ensuite. Quand passer à la régression non linéaire ? Nous pouvons être amenés à utiliser une régression non linéaire pour l’une de ces deux raisons, voire les deux en même temps : Lorsque le nuage de points est curvilinéaire, évidemment, mais après avoir tenté de le linéariser (et de résoudre un problème éventuel d’hétéroscédasticité ou de non-normalité des résidus) par transformation sans succès, En fonction de nos connaissances a priori du phénomène. Tout phénomène issu d’un mécanisme dont nous connaissons le mode de fonctionnement menant à une équation mathématique non linéaire. Cela se rencontre fréquemment en physique, en chimie, et même en biologie (courbes de croissance, effet de modifications environmentales, etc.) Les spécialistes de la photosynthèse ont mis au point différents modèles pour représenter les RLC. (Platt, Gallegos, and Harrison 1980) ont proposé une formulation mathématique des phénomènes mis en jeu ici. Leur équation est la suivante : \\[ETR = ETR_{max} \\cdot (1 - e^{-PAR \\cdot \\alpha/ETR_{max}}) \\cdot e^{-PAR \\cdot \\beta/ETR_{max}}\\] avec \\(PAR\\) la variable indépendante, \\(ETR\\), la variable dépendante, et \\(ETR_{max}\\), \\(\\alpha\\) et \\(\\beta\\), les trois paramètres du modèle. \\(ETR_{max}\\) est le rendement maximum possible, \\(\\alpha\\) est la pente de la partie initiale linéaire avant infléchissement vers le maximum et \\(\\beta\\) est le coefficient de photoinhibition. En matière de régression non linéaire, il est tout aussi important de bien comprendre les propriétés mathématiques de la fonction utilisée que de faire un choix judicieux du modèle. En particulier, il faut s’attacher à bien comprendre la signification (biologique) des paramètres du modèle. Non seulement, cela aide à en définir des valeurs intiales plausibles, mais c’est aussi indispensable pour pouvoir ensuite bien interpréter les résultats obtenus. Nous pouvons facilement créer une fonction dans R qui représente ce modèle : pgh_model &lt;- function(x, etr_max, alpha, beta) etr_max * (1 - exp(-x * alpha/etr_max)) * (exp(-x * beta/etr_max)) Le premier argument de la fonction doit être la variable indépendante (notée de manière générique x, mais n’importe quel nom fait l’affaire ici) et les autres arguments correspondent aux paramètres du modèle, donc etr_max, alpha et beta. Ce modèle peut être ajusté dans R à l’aide de la fonction nls() pour “Nonlinear Least Squares” (regression). Par contre, nous devons fournir une information supplémentaire (l’explication sera détaillée plus loin) : des valeurs approximatives de départ pour les paramètres. Nous voyons sur le graphique que etr_max = 9 est une estimation plausible, mais il est difficile de déterminer alpha et beta rien qu’en regardant le graphique. Nous allons fixer alpha = 1 (rendement max), et partir d’un modèle sans photoinhibition en utilisant beta = 0. Voici comment la régression non linéaire par les moindre carrés avec notre fonction pgh_model peut être calculée : rlc_nls &lt;- nls(data = rlc, etr ~ pgh_model(par, etr_max, alpha, beta), start = list(etr_max = 9, alpha = 1, beta = 0)) summary(rlc_nls) # # Formula: etr ~ pgh_model(par, etr_max, alpha, beta) # # Parameters: # Estimate Std. Error t value Pr(&gt;|t|) # etr_max 9.351064 0.261969 35.695 3.22e-08 *** # alpha 0.327385 0.013416 24.402 3.11e-07 *** # beta 0.003981 0.001084 3.673 0.0104 * # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.1729 on 6 degrees of freedom # # Number of iterations to convergence: 7 # Achieved convergence tolerance: 3.619e-06 La dernière ligne, “achieved convergence” indique que le modèle a pu être calculé. Nous avons un tableau des paramètres qui ressemble très fort à celui de la régression linéaire, y compris les tests t de Student sur chacun des paramètres. Nous obtenons etr_max = 9.4, alpha = 0.33 et beta = 0.0040, mais d’après le test de Student ce dernier paramètre n’est pas significativement différent de zéro9. Pour l’instant, nous allons conserver ce modèle tel quel. Notre modèle paramétré donne donc : \\[ETR = 9.4 \\cdot (1 - e^{-PAR \\cdot 0.33/9.3}) \\cdot e^{-PAR \\cdot 0.0040/9.3}\\] Voyons ce que cela donne sur le graphique. Nous utiliserons pour ce faire une petite astuce qui consiste à transformer l’objet nls obtenu en une fonction utilisable par stat_function() pour le graphique ggplot2 réalisé à l’aide de chart()10. as.function.nls &lt;- function(x, ...) { nls_model &lt;- x name_x &lt;- names(nls_model$dataClasses) stopifnot(length(name_x) == 1) function(x) predict(nls_model, newdata = structure(list(x), names = name_x)) } Maintenant, nous allons pouvoir ajouter la courbe correspondant à notre modèle comme ceci : chart(data = rlc, etr ~ par) + geom_point() + stat_function(fun = as.function(rlc_nls)) Ce modèle représente bien mieux le phénomène étudié, et il s’ajuste d’ailleurs beaucoup mieux également dans les données que notre polynome. Une comparaison sur base du critère d’Akaïke est également en faveur de ce dernier modèle non linéaire (pour rappel, plus la valeur est faible, mieux c’est) : AIC(rlc_lm, rlc_nls) # df AIC # rlc_lm 5 31.834047 # rlc_nls 4 -1.699078 Super ! Nous venons de réaliser ensemble notre première régression non linéaire par les moindres carrés. Étudions un petit peu plus dans le détail cette technique dans la section suivante. En effet, il est utile de connaitre et comprendre les différents pièges qui peuvent se présenter à nous pour les éviter. Références "],
["principe.html", "4.2 Principe", " 4.2 Principe La régression non linéaire consiste à modéliser la variation d’une variable (dite variable réponse ou dépendante) par rapport à la variation d’une ou plusieurs autres variables (dites explicatives ou indépendantes). Le modèle utilisé pour représenter cette relation est une fonction mathématique de forme quelconque. Ceci constitue une généralisation de la régression linéaire où la fonction mathématique était nécessairement une droite (\\(y = a x + b\\) dans le cas de la régression linéaire simple). La fonction est, dans la technique la plus courante, ajustée en minimisant la somme des carrés des résidus (écart entre les observations \\(y_i\\) et les valeurs prédites par la droite, notées \\(\\hat{y_i}\\)). Lorsque le nuage de point ne s’étire pas le long d’une droite, nous pouvons tenter de transformer les données afin de les linéariser. Malheureusement, il existe de nombreux cas où la relation n’est pas linéarisable et la régression non linéaire est alors notre meilleur choix. Il existe, en réalité, une autre raison pour laquelle nous pourrions être amenés à ne pas transformer les données pour les linéariser. Il s’agit du cas où les résidus ont une distribution correcte avec les données non transformées (distribution normale, et variance homogène -homoscédasticité-) lorsqu’on utilise un modèle non linéaire. Dans ce cas précis, une transformation pour linéariser les données devrait permettre d’utiliser une régression linéaire. Mais ce faisant, on rend alors les résidus non normaux et/ou on perd la propriété d’homoscédasticité, ce qui constitue une violation des conditions d’application de la régression par les moindres carrés que nous utilisons ici. Ainsi, dans ce cas-là, il vaut alors mieux ne pas transformer et utiliser plutôt une régression non linéaire à la place d’une régression linéaire pourtant plus simple d’emploi. 4.2.1 Fonction objective Nous appelons “fonction objective” la fonction qui quantifie la qualité de l’ajustement de sorte que plus le nombre renvoyé par cette fonction est petit, meilleur est l’ajustement. Cette fonction objective peut être définie librement, mais dans de nombreux cas, il s’agit du même critère que pour la régression linéaire par les moindres carrés, à savoir (considérant que la fonction \\(f\\) que nous souhaitons ajuster a \\(k\\) paramètres notés \\(p_1\\), \\(p_2\\), …, \\(p_k\\)) : \\[f_{obj}(p_{1},p_{2},...,p_{k})=\\sum_{i=1}^{n}(y_{i}-f(x_{i,}p_{1},p_{2},...,p_{k}))^{2}=\\sum_{i}(y_{i}-\\hat{{y_{i}}})^{2}\\] 4.2.2 Calcul itératif L’ajustement de notre courbe selon le modèle \\(y = f(x, p_1, p_2, ... p_k) + \\epsilon\\) avec les résidus \\(\\epsilon \\approx N(0, \\sigma)\\) peut se faire de manière itérative, en testant différentes valeurs des paramètres de la fonction, et en retenant au final la combinaison qui minimise le plus la fonction objective. Par exemple, si notre courbe à ajuster est : \\(y = a x^b\\), nous devrons estimer conjointement la valeur des deux paramètres \\(a\\) et \\(b\\) de la courbe. Bien qu’il s’agisse effectivement de la technique employée pour ajuster une courbe dans un nuage de point, nls() utilise ici des algorithmes d’optimisation efficaces pour trouver la solution plus rapidement. Une recherche en aveugle serait très peu efficace évidemment. La description détaillée et les développements mathématiques de ces algorithmes d’optimisation sortent du cadre de ce cours. Nous renvoyons le lecteur intéressé à l’annexe C de (Sen and Srivastava 1990). Les algorithmes utilisés dans la fonction nls() sont : Gauss-Newton, (par défaut), un algorithme utilisant la différentiation de la courbe et une expansion en série de Taylor pour approximer cette courbe par une série de termes additifs dont la solution peut être trouvée par régression linéaire multiple. plinear de Golub-Pereyra, bien que peu répandu, est également implémenté. Il est utile en ce sens qu’il sépare les paramètres en deux sous-groupes : ceux qui sont linéaires dans la fonction (coefficients multiplicateurs de termes additifs) et ceux qui ne le sont pas. La recherche itérative ne se fait que sur les seconds. Les premiers étant estimés par régression linéaire. Ainsi, lorsque la fonction ne comporte que peu de paramètres non linéaires, l’algorithme converge beaucoup plus rapidement. Port est également disponible. Il a la particularité, contrairement aux deux précédents, de permettre de définir des limites supérieures et inférieures acceptables pour chaque paramètres. Cela limite la recherche dans un domaine de validité, lorsque celui-ci peut être défini. Reprenons le calcul de notre RLC pour P. oceanica. L’argument trace = TRUE peut être ajouté à l’appel de nls() pour visionner les différentes étapes du calcul itératif. Voici ce que cela donne : rlc_nls &lt;- nls(data = rlc, etr ~ pgh_model(par, etr_max, alpha, beta), start = list(etr_max = 9, alpha = 1, beta = 0), trace = TRUE) # 24.34024 : 9 1 0 # 4.236559 : 8.4785637209 0.5324097242 -0.0004037391 # 1.141967 : 8.772552037 0.289806296 0.001855493 # 0.1807006 : 9.361592818 0.325563316 0.003954183 # 0.1793698 : 9.353620948 0.327230129 0.003990887 # 0.1793659 : 9.351296953 0.327369993 0.003982181 # 0.1793658 : 9.35108479 0.32738331 0.00398142 # 0.1793658 : 9.351064462 0.327384565 0.003981347 A gauche, nous avons la valeur de la fonction objective \\(f_{obj}\\), et à droite des deux points, la valeur actuelle des trois paramètres dans l’ordre etr_max, alpha et beta. La première ligne indique l’étape 0. \\(f_{obj}\\) vaut 24,3 avec les valeurs par défaut. Dès l’étape suivante (itération #1), cette valeur est divisée par 6 et vaut 4,2. A ce stade, etr_max = 8,5, alpha = 0,29 et beta = 0,0019. Les étapes #2 et #3 font encore significativement diminuer \\(f_{obj}\\), et puis nous grapillons des décimales. Voici une animation de ce processus : La convergence est considérée comme obtenue lorsque la différence de valeur de \\(f_{obj}\\) d’une étape à l’autre tombe en dessous d’un seuil de tolérance que nous nous sommes fixés. C’est l’argument control = de nls() qui en reprend les détails, voir ?nls.control. Les options sont : maxiter = le nombre maximum d’itérations permises. Pour éviter de calculer à l’infini, nous arrêtons à cette valeur et décrétons que le calcul n’a pas convergé. La valeur pas défaut est de 50 itérations. tol = le niveau de tolérance pour décider que l’on a convergé. C’est lui notre seuil de tolérance ci-dessus. Par défaut, il vaut 1e-05, soit 0,00001. C’est une valeur relative. Cela siginifie que la variation de la fonction objective ne doit pas être plus grande que cela en valeur relative, soit \\(|\\frac{fobj_i - fobj_{i + 1}}{fobj_i}| &lt; tol\\). … d’autres arguments moins importants ici. Comme nous pouvons le constater, le seuil de tolérance par défaut est très bas, de sorte que nous n’ayons besoin de le changer que très rarement. 4.2.3 Pièges et difficultés L’ajustement d’un modèle de manière itérative en utilisant un algorithme d’optimisation est une tâche délicate. Dans certains cas, la convergence est lente (il faut beaucoup d’étapes pour arriver au résultat). Dans d’autre cas, le processus s’interrompt à cause d’une singularité de la fonction à ajuster, d’une discontinuité, ou d’une fonction qui n’est pas définie sur une partie du domaine, … Nous pouvons aussi rencontrer des divisions par zéro ou des paramètres qui tendent vers l’infini lors des calculs. Dans ce cas, il faut tenter d’autres valeurs de départ, voire changer le paramétrage de la fonction. Valeurs initiales La recherche de la solution optimale nécessite de partir de valeurs de départ plausibles pour les paramètres du modèle (c’est-à-dire, un ensemble de valeurs pour les paramètres telle que la courbe initiale est proche de la solution recherchée). Définir les valeurs initiales n’est pas toujours chose aisée, et de plus, le résultat final peut dépendre fortement de ces valeurs initiales, à savoir que, si l’on choisi des valeurs initiales trop éloignées de la solution recherchée, on peut très bien se trouver enfermé dans une fausse solution (un minimum local de la fonction objective qui est moins bas que le minimum absolu que l’on recherche). Considérons un cas fictif simple où la fonction à ajuster n’aurait qu’un seul paramètre \\(p\\). Dans ce cas, nous pouvons visualiser la valeur de la fonction objective en fonction de la valeur choisie pour le paramètre \\(p\\). Admettons que nous obtenons le graphique suivant pour cette représentation : Nous observons que la fonction objective a un minimum global pour \\(p\\) valant 0,72. Par contre, un minimum local est également présent pour \\(p\\) valant -0,26. Dans ce cas, si nous prenons une valeur de départ pour \\(p\\) supérieure à 0,25, nous atterrirons dans le minimum global. Mais si nous prenons une valeur plus faible, par exemple 0 ou -0.5 comme valeur de départ, nous nous ferons piéger dans le minimum local. Dans ce cas, nls() déclarera qu’elle a convergé et que la valeur de \\(p\\) vaut -0,26. Si ce n’est toujours pas clair, imaginez que la courbe en rouge représente la forme d’un terrain sur lequel vous placer une balle à la position de départ. En fonction de l’endroit où vous la placer, la balle va glisser le long de la pente et finir par s’immobiliser dans une des deux cuvettes… mais celle de droite (minimum global) est plus profonde que celle de gauche (minimum local). Il vaut toujours mieux tester différentes combinaisons de valeurs de départ pour vérifier si la convergence se fait correctement et pour démasquer les cas de minima locaux. Complexité du modèle Lorsque la convergence a du mal à se faire, ceci est éventuellement lié à la complexité de la fonction que l’on cherche à ajuster. Les propriétés mathématique de la courbe choisie peuvent très bien faire que cette courbe ne soit pas définie pour certaines valeurs des paramètres, ou qu’elle ait un comportement spécial dans un certain domaine (par exemple, courbe tendant vers l’infini). Ces cas sont difficilement traités par la plupart des algorithmes de minimisation de la fonction objective, et des erreurs de calcul de type “division par zéro”, ou “résultat non défini” peuvent apparaître. Dans les meilleures implémentations (nls() en est une), des gardes-fous ont été ajoutés dans le code de la fonction pour éviter les erreurs dans pareils cas. Toutefois, il n’est pas possible de traiter tous les cas possibles. Ainsi, il arrive parfois que le modèle choisi ne soit pas ajustable sur les données. Au final, l’ajustement d’un modèle non linéaire par les moindres carrés est une opération beaucoup plus délicate que l’ajustement par les moindres carrés d’un modèle linéaire. Dans le cas linéaire, la solution est trouvée de manière immédiate grâce à un calcul matriciel simple. Dans le cas non linéaire, il n’existe souvent pas de solution miracle et le meilleur ajustement doit être recherché de manière itérative. Nous verrons ci-dessous que R propose, pour certains modèles appelés ‘SelfStart’ une solution élégante pour calculer automatiquement les valeurs initiales et pour converger très rapidement vers la solution recherchée, mais la plupart du temps il faut tâtonner pour ajuster sa courbe. A vous de jouer ! Réalisez un rapport scientifique sur la vitesse d’une réaction chimique, en définissant un modèle non linéaire pertinent pour ces données. Vous avez à votre disposition une assignation GitHub Classroom : https://classroom.github.com/a/pa48JVSl Lisez le README afin de prendre connaissance de l’exercice 4.2.4 Modèles ‘selfStart’ dans R Dans certains cas, il existe des petites astuces de calcul pour converger directement vers la solution, ou du moins, pour calculer des valeurs de départ très proches de la solution recherchée de sorte que l’algorithme de recherche pourra converger très rapidement. Par exemple, lorsqu’il est possible de linéariser le modèle en transformant les données. Dans ce cas, une bonne estimation des valeurs de départ des paramètres peut être obtenue en linéarisant la relation et en calculant les paramètres par régression linéaire. Ensuite, les paramètres obtenus sont retransformés dans leur forme initiale, et ils sont utilisés comme valeurs de départ. Par exemple, si nous décidons d’ajuster un modèle allométrique de Huxley, de type \\(y = a x^b\\)11, nous pouvons linéariser la relation en transformant les deux variables en logarithmes. En effet, \\(\\log(y) = \\log(a x^b)\\), ce qui est équivalent à une droite en développant: \\(\\log(y) = b \\log(x) + \\log(a)\\). Le paramètre \\(b\\) devient donc la pente de la droite, et \\(\\log(a)\\) devient l’ordonnée à l’origine dans le modèle linéaire transformé. Une fois la droite ajustée, on a directement la valeur de \\(b\\), et il suffit de calculer l’exponentielle de l’ordonnée à l’origine pour obtenir \\(a\\). Toutefois, comme les résidus sont différents dans la relation non transformée initiale, il ne s’agit pas de la solution recherchée mais d’une bon point de départ très proche de cette solution. Ainsi, on prendra les valeurs de \\(a\\) et de \\(b\\) ainsi calculées par la droite en double log comme point de départ, et on laissera l’algorithme de recherche du minimum terminer le travail. En fait, c’est exactement de cette façon que les modèles dits ‘selfStart’ fonctionnent dans R. Plus qu’une fonction, il s’agit en réalité d’un programme complet qui contient : la fonction elle-même, la résolution analytique de la dérivée première de la fonction en chaque point (utilisée par l’algorithme de convergence pour déterminer l’écart à apporter aux paramètres à l’itération suivante), du code optimisé pour choisir les valeurs initiales idéales (proches du minimum global pour la \\(f_{obj}\\)) automatiquement, du code pour optimiser la convergence, éventuellement. Toutes les fonctions ‘SelfStart’ ont un nom commençant par SS. nous pouvons donc les lister à l’aide de l’instruction suivante : apropos(&quot;^SS&quot;) # [1] &quot;SSasymp&quot; &quot;SSasympOff&quot; &quot;SSasympOrig&quot; &quot;SSbiexp&quot; &quot;SSD&quot; # [6] &quot;SSfol&quot; &quot;SSfpl&quot; &quot;SSgompertz&quot; &quot;SSlogis&quot; &quot;SSmicmen&quot; # [11] &quot;SSweibull&quot; Ensuite, nous recherchons l’aide relative à ces fonctions, par exemple via ?SSasymp. Références "],
["modeles-courants-en-biologie.html", "4.3 Modèles courants en biologie", " 4.3 Modèles courants en biologie Les domaines les plus courants où des modèles non linéaires sont utilisés en biologie concernent les cinétiques de réactions (chimiques, biochimiques), les courbes de type dose-réponse, les courbes de survie et les modèles de croissance. Nous verrons principalement divers modèles de croissance dans la section suivante. Certains de ces modèles, comme le modèle exponentiel, celui de Gompertz ou de Weibull sont aussi utilisés comme courbes de survie. De plus, la courbe logistique est un modèle classique de type dose-réponse. Ainsi, les différentes courbes de croissances recouvrent également une majorité des modèles utilisés dans d’autres domaines. 4.3.1 Modèle de Michaelis-Menten La corube de Michaelis-Menten est bien connue pour modéliser des cinétiques chimiques simples, enzymatiques en particulier. Son équation est : \\[V = \\frac{V_{max} \\cdot conc}{K + conc}\\] où \\(conc\\) est la concentration des réactifs au début de la réaction, c’est-à-dire, en absence de produits de cette réaction en mol/L, \\(V\\) est la vitesse de réaction en mol/min. Le modèle a deux paramètres \\(V_{max}\\) la vitesse maximale asymptotique en mol/min et \\(K\\) en mol/L correspondant à la concentration telle que la vitesse est la moitié de \\(V_{max}\\). Dans R, il existe un modèle ‘selfStart’ facile à utiliser pour ajuster une courbe de type Michaelis-Menten. Il s’agit de la fonction SSmicmen(). Voici le graphique d’un modèle Michaelis-Menten avec \\(V_{max} = 1\\) et \\(K = 0,4\\). Le trait horizontal en Vm = 1 représente la vitesse maximale possible (asymptote horizontale du modèle). Nous voyons que cette vitesse maximale n’est atteinte que très lentement ici, et il faudrait que l’axe des X s’étende beaucoup plus sur la droite pour l’observer. micmen_data &lt;- tibble( conc = seq(0, 10, by = 0.1), v = SSmicmen(conc, Vm = 1, K = 0.5) ) chart(data = micmen_data, v ~ conc) + geom_line() + xlab(&quot;Concentration [mol/L]&quot;) + ylab(&quot;Vitesse [mol/min]&quot;) + geom_vline(xintercept = 0, col = &quot;darkgray&quot;) + geom_hline(yintercept = c(0.5, 1), col = &quot;gray&quot;, linetype = &quot;dashed&quot;) + geom_vline(xintercept = 0.4, col = &quot;gray&quot;, linetype = &quot;dashed&quot;) + annotate(&quot;text&quot;, label = &quot;Vm&quot;, x = -0.4, y = 1) + annotate(&quot;text&quot;, label = &quot;Vm/2&quot;, x = -0.5, y = 0.5) + annotate(&quot;text&quot;, label = &quot;K&quot;, x = 0.5, y = 0.03) A vous de jouer ! Ajuste les pramètres du modèles afin de trouver le meilleur modèle de Michaelis-Menten dans une application interactive “shiny”. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir l’application : BioDataScience2::app(&quot;04a_michaelis_menten&quot;) # TODO Méthode alternative : shiny::runApp(system.file(&quot;shiny/04a_michaelis_menten&quot;, package = &quot;BioDataScience2&quot;)) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R lorsque vous aurez fini avec l’application shiny. 04a_michaelis_menten Concernant les modèles utiles en chimie et biochimie, le modèle à compartiment de premier ordre permet de décrire la cinétique de transformation d’une substance au cours du temps. Voyez le modèle ?Ssfol. Il peut servir par exemple pour déterminer la cinétique d’élimination d’une substance du sang après injection. 4.3.2 Modèles de croissance Parmi les phénomènes biologiques courants qui sont essentiellement non linéaires, les modèles de croissance occupent une part importante. Il s’agit de phénomènes complexes, résultat d’un ensemble de processus, eux-même très complexes : l’anabolisme, ou élaboration de matière organique et le catabolisme qui la détruit pour la transformer en énergie, en ce qui concerne la croissance somatique individuelle. Un modèle typique de croissance individuelle est le modèle de von Bertalanffy (von Bertalanffy, 1938, 1957). Il existe un autre type de modèle de croissance : la croissance des populations. Ce type de modèle décrit l’évolution du nombre d’individus dans une population au cours du temps. Dans ce cas particulier, il s’agit également du résultat de plusieurs processus : la natalité, la mortalité et, éventuellement, les migrations. Une courbe type de modèle de croissance de population est la courbe logistique. Un autre modèle couramment utilisé est celui de Weibull pour décrire un nombre décroissant d’individus suite à une mortalité prédominante dans celle-ci. De nombreux modèles de croissance différents sont disponibles. Certains sont exclusivement des modèles de croissance individuelle, d’autres sont exclusivement des modèles de croissance de populations, mais beaucoup peuvent être utilisés indifféremment dans les deux cas. Tous ont comme particularité d’être l’une ou l’autre forme de modèle exponentiel, exprimant ainsi le fait que la croissance est fondamentalement un processus exponentiel (comprenez que l’augmentation de masse ou du nombre d’individus est proportionnelle à la masse ou au nombre préexistant à chaque incrément temporel). Nous allons décrire ci-dessous quelques un des modèles de croissance principaux. Ensuite, nous les utiliserons pour ajuster une courbe de croissance dans un jeu de donnée réel. 4.3.3 Courbe exponentielle En 1798, Thomas Malthus a décrit un modèle de croissance applicable pour décrire la croissance de la population humaine. Cependant, d’après Murray (1993), ce modèle a été suggéré en premier lieu par Euler. Quoi qu’il en soit, ce modèle n’est plus guère utilisé actuellement, mais son importance historique ne doit pas être négligée. Il s’agit, en effet, de la première modélisation mathématique d’une des caractéristiques les plus fondamentales de la croissance : son caractère exponentiel (positive ou négative). Malthus a observé que la population des Etat-Unis double tous les 25 ans. Il suggère alors que les populations humaines augmentent d’une proportion fixe \\(r\\) sur un intervalle de temps donné, lorsqu’elles ne sont pas affectées de contraintes environnementales ou sociales. Cette proportion \\(r\\) est par ailleurs indépendante de la taille initiale de la population : \\[y_{t+1} = (1+r) \\ y_t = k \\ y_t\\] Une forme continue du modèle précédent (intervalle de temps infinitésimal) donne une équation différentielle : \\[\\frac{d y(t)}{dt} = y&#39;(t) = k \\ y(t)\\] Cette équation différentielle admet la solution suivante : \\[y(t) = y_0 \\ e^{k \\ t}\\] avec \\(y_0\\), la taille initiale de la population au temps \\(t = 0\\). Ce modèle à deux paramètres est également intéressant parce qu’il montre une bonne manière de construire un modèle de croissance. Il suffit de décrire la croissance pour un accroissement infinitésimal de temps par le biais d’une équation différentielle, et ensuite de la résoudre (modélisation dynamique). Presque tous les modèles de croissance existants ont été élaborés de cette manière. Ainsi, la quasi-totalité des modèles de croissance correspondent en fait à une équation différentielle relativement simple. Dans R, nous pourrons utiliser la fonction suivante : exponent &lt;- function(x, y0, k) y0 * exp(k * x) Voici un exemple d’un modèle de croissance exponentiel avec \\(y_0 = 1,5\\) et \\(k = 0,9\\). Le paramètre \\(y_0\\) est indiqué sur le graphique. # Graphique avec y0 = 1.5 et k = 0.9 exponent_data &lt;- tibble( t = seq(0, 3, by = 0.1), y = exponent(t, y0 = 1.5, k = 0.9) ) chart(data = exponent_data, y ~ t) + geom_line() + geom_vline(xintercept = 0, col = &quot;darkgray&quot;) + annotate(&quot;text&quot;, label = &quot;y0&quot;, x = -0.05, y = 1.5) Il existe aussi un modèle bi-exponentiel qui combine deux signaux exponentiels différents. Pour plus de détails, voyez ?SSbiexp. A vous de jouer ! Ajuste les pramètres du modèles afin de trouver la meilleure courbe exponentielle dans une application interactive “shiny”. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir l’application : BioDataScience2::app(&quot;04b_exponent&quot;) # TODO Méthode alternative : shiny::runApp(system.file(&quot;shiny/04b_exponent&quot;, package = &quot;BioDataScience2&quot;)) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R lorsque vous aurez fini avec l’application shiny. 04b_exponent 4.3.4 Courbe logistique Le modèle exponentiel décrit une croissance infinie sans aucunes contraintes. Ce n’est pas une hypothèse réaliste. En pratique, la croissance est limitée par les ressources disponibles. Verhulst (1838), en travaillant aussi sur la croissance de populations, propose un modèle qui contient un terme d’auto-limitation \\([y_\\infty – y (t)] / y_\\infty\\) qui représente une quelconque limite théorique des ressources disponibles : \\[\\frac{dy(t)}{dt} = k \\ \\frac{y_\\infty - y(t)}{y_\\infty} \\ y(t) = - \\frac{k}{y_\\infty} \\ y(t)^2 + k \\ y(t)\\] Lorsque l’on résout et simplifie cette équation différentielle, on obtient : \\[y(t) = \\frac{y_\\infty}{1 + e^{-k \\ (t - t_0)}}\\] Ceci est une des formes de la courbe logistique. Cette fonction a deux asymptotes horizontales en \\(y(t) = 0\\) et \\(y(t) = y_\\infty\\) (voir schéma ci-dessous) et c’est une sigmoïde symétrique autour du point d’inflexion (les deux courbes du S sont identiques). Le modèle ‘selfStart’ correspondant dans R s’appelle SSlogis(). Ses paramètres sont Asym (= \\(y_\\infty\\)), xmid (= \\(t_0\\)), et scal (= \\(k\\)). Voici le graphique d’une courbe logistique avec \\(y_\\infty = 0,95\\), \\(t_0 = 5\\) et \\(k = 1\\). logis_data &lt;- tibble( t = seq(0, 10, by = 0.1), y = SSlogis(t, Asym = 0.95, xmid = 5, scal = 1) ) chart(data = logis_data, y ~ t) + geom_line() + geom_vline(xintercept = 0, col = &quot;darkgray&quot;) + geom_hline(yintercept = c(0, 0.95/2, 0.95), col = &quot;gray&quot;, linetype = &quot;dashed&quot;) + geom_vline(xintercept = 5, col = &quot;gray&quot;, linetype = &quot;dashed&quot;) + annotate(&quot;text&quot;, label = &quot;Asym&quot;, x = -0.4, y = 0.95) + annotate(&quot;text&quot;, label = &quot;Asym/2&quot;, x = -0.5, y = 0.95/2) + annotate(&quot;text&quot;, label = &quot;xmid&quot;, x = 5.4, y = 0.03) + annotate(&quot;text&quot;, label = &quot;point d&#39;inflexion&quot;, x = 6, y = 0.45) Cette courbe sigmoïdale est asymptotique en 0 et \\(y_\\infty\\), et elle est également symétrique autour de son point d’inflexion situé à \\({t_0, y_\\infty / 2}\\). A vous de jouer ! Ajuste les pramètres du modèles afin de trouver la meilleure courbe exponentielle dans une application interactive “shiny”. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir l’application : BioDataScience2::app(&quot;04c_logistique&quot;) # TODO Méthode alternative : shiny::runApp(system.file(&quot;shiny/04c_logistique&quot;, package = &quot;BioDataScience2&quot;)) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R lorsque vous aurez fini avec l’application shiny. 04c_logistique Il est possible de généraliser ce modèle en définissant une courbe logistique dont l’asymptote basse peut se situer n’importe où ailleurs qu’en 0. Si cette asymptote se situe en \\(y_0\\), nous obtenons l’équation : \\[y(t) = y_0 + \\frac{y_\\infty - y_0}{1 + e^{-k \\ (t - t_0)}}\\] Ceci est le modèle logistique généralisé à quatre paramètres (modèle ‘selfSart’ SSfpl() dans R, pour four-parameters logistic). Les arguments sont A, la première asymptote horizontale (= \\(y_0\\)), B, la seconde asymptote horizontale (= \\(y_\\infty\\)), xmid (= \\(t_0\\)) et scal (= \\(k\\)). Le graphique ressemble très fort à celui de la fonction logistique, mais la première asymptote n’est plus nécessairement à 0 (ici, \\(y_0\\) = 0,15). fpl_data &lt;- tibble( t = seq(0, 10, by = 0.1), y = SSfpl(t, A = 0.15, B = 0.95, xmid = 5, scal = 1) ) chart(data = fpl_data, y ~ t) + geom_line() + geom_vline(xintercept = 0, col = &quot;darkgray&quot;) + geom_hline(yintercept = c(0.15, 0.8/2 + 0.15, 0.95), col = &quot;gray&quot;, linetype = &quot;dashed&quot;) + geom_vline(xintercept = 5, col = &quot;gray&quot;, linetype = &quot;dashed&quot;) + annotate(&quot;text&quot;, label = &quot;A&quot;, x = -0.4, y = 0.15) + annotate(&quot;text&quot;, label = &quot;B&quot;, x = -0.4, y = 0.95) + annotate(&quot;text&quot;, label = &quot;xmid&quot;, x = 5.4, y = 0.13) + annotate(&quot;text&quot;, label = &quot;point d&#39;inflexion&quot;, x = 6.1, y = 0.53) A vous de jouer ! Ajuste les pramètres du modèles afin de trouver la meilleure courbe exponentielle dans une application interactive “shiny”. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir l’application : BioDataScience2::app(&quot;04d_logistique&quot;) # TODO Méthode alternative : shiny::runApp(system.file(&quot;shiny/04d_logistique_gen&quot;, package = &quot;BioDataScience2&quot;)) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R lorsque vous aurez fini avec l’application shiny. 04d_logistique_gen 4.3.5 Modèle de Gompertz Gompertz (1825) a observé de manière empirique que le taux de survie décroît souvent de manière proportionnelle au logarithme du nombre d’animaux qui survivent. Bien que ce modèle reste utilisé pour décrire des courbes de survie, elle trouve de nombreuses applications pour décrire également des données de croissance. L’équation différentielle du modèle de Gompertz est : \\[\\frac{dy(t)}{dt} = k \\ [ \\ln y_\\infty - \\ln y(t)] \\ y(t)\\] qui se résout et se simplifie en : \\[y(t) = y_\\infty \\ e^{-k \\ (t - t_0)} = y_\\infty \\ e^{-a \\cdot b^t} = y_\\infty \\ a^{e^{-k \\ t}} = y_\\infty \\ a^{b^t}\\] La dernière forme apparaît plus simple et est le plus souvent utilisée. La première forme est directement dérivée de l’équation différentielle et donne une meilleure comparaison avec la courbe logistique, puisque \\(t_0\\) correspond aussi à l’abscisse du point d’inflexion, qui n’est plus en position symétrique ici (voir figure ci-dessous). Le modèle ‘selfStart’ correspondant dans R s’appelle SSgompertz(). Sa paramétriqation correspond à la seconde forme, mais avec \\(a\\) appelé b2 et \\(b\\) appelé b3 (\\(y(t) = Asym \\ e^{-b2 \\cdot b3^t}\\)). Voici le graphique d’une courbe de Gompertz avec \\(y_\\infty = 0,95\\), \\(a = 5\\) et \\(b = 0,5\\). gomp_data &lt;- tibble( t = seq(0, 10, by = 0.1), y = SSgompertz(t, Asym = 0.95, b2 = 5, b3 = 0.5) ) chart(data = gomp_data, y ~ t) + geom_line() + geom_vline(xintercept = 0, col = &quot;darkgray&quot;) + geom_hline(yintercept = c(0, 0.95/exp(1), 0.95), col = &quot;gray&quot;, linetype = &quot;dashed&quot;) + geom_vline(xintercept = 2.3, col = &quot;gray&quot;, linetype = &quot;dashed&quot;) + annotate(&quot;text&quot;, label = &quot;Asym&quot;, x = -0.4, y = 0.95) + annotate(&quot;text&quot;, label = &quot;Asym/e&quot;, x = -0.5, y = 0.95/exp(1)) + annotate(&quot;text&quot;, label = &quot;point d&#39;inflexion&quot;, x = 3.5, y = 0.32) A vous de jouer ! Ajuste les pramètres du modèles afin de trouver la meilleure courbe exponentielle dans une application interactive “shiny”. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir l’application : BioDataScience2::app(&quot;04e_gompertz&quot;) # TODO Méthode alternative : shiny::runApp(system.file(&quot;shiny/04e_gompertz&quot;, package = &quot;BioDataScience2&quot;)) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R lorsque vous aurez fini avec l’application shiny. 04e_gompertz 4.3.6 Modèles de von Bertalanffy Le modèle de von Bertalanffy, parfois appelé Brody-Bertalanffy (d’après les travaux de von Bertalanffy et Brody) ou modèle de Pütter (dans Ricker, 1979 notamment), est la première courbe de croissance qui a été élaborée spécifiquement pour décrire la croissance somatique individuelle. Il est basé sur une analyse bioénergétique simple. Un individu est vu comme un simple réacteur biochimique dynamique où les entrées (anabolisme) sont en compétition avec les sorties (catabolisme). Le résultat de ces deux flux étant la croissance. L’anabolisme est plus ou moins proportionnel à la respiration et la respiration est proportionnelle à une surface pour beaucoup d’animaux (la surface développée des poumons ou des branchies), soit encore, les 2/3 de la masse. Le catabolisme est toujours proportionnel à la masse. Ces relations mécanistiques sont rassemblées dans l’équation différentielle suivante où \\(y(t)\\) mesure l’évolution d’un volume ou d’un poids au cours du temps : \\[\\frac{dy(t)}{dt} = a \\ y(t)^{2/3} - b \\ y(t)\\] En résolvant cette équation, nous obtenons le modèle de croissance pondérale de von Bertalanffy : \\[y(t) = y_\\infty \\ (1 - e^{-k \\ (t - t_0)})^3\\] La forme la plus simple de ce modèle est obtenue lorsque nous mesurons des dimensions linéaires pour quantifier la taille de l’organisme, car une dimension linéaire est, en première approximation, la racine cubique d’une masse, proportionnelle au volume à densité constante (sans prendre en compte une possible allométrie). Le modèle de von Bertalanffy pour des mesures linéaires est alors simplement  : \\[y(t) = y_\\infty \\ (1 - e^{-k \\ (t - t_0)})\\] Un graphique des deux modèles est présenté ci-dessous. Le modèle de von Bertalanffy pour mesures linéaire n’a pas de point d’inflexion. La croissance est la plus rapide à la naissance et ne fait que diminuer avec le temps pour finalement atteindre zéro lorsque la taille maximale asymptotique est atteinte. Avec ce modèle, la croissance est donc déterminée et elle ne peut dépasser cette asymptote horizontale située en \\(y(t)= y_\\infty\\). A cause de la puissance cubique de la forme pondérale du modèle von Bertalanffy, cette dernière est une sigmoïde asymétrique, comme l’est le modèle de Gompertz également. Trois modèles ‘selfStart’ existent dans R : SSasympOff(), SSasymp() et SSasympOrig(). SSAsympOff() est définie comme \\(y(t) = y_\\infty \\ (1 - e^{-e^{lrc} \\ (t - t_0)})\\). Ici \\(k\\) est remplacé par \\(e^{lrc}\\). Cette astuce permet d’avoir un paramètre \\(k\\) qui ne prend pas de valeur négatives, puisque l’exponentielle d’une valeur négative est un nombre compris entre 0 et 1. Donc, un \\(lrc\\) négatif donne un \\(k\\) compris entre 0 et 1. Cela permet de contraindre un paramètre du modèle sans nécessité de passer obligatoirement par l’algorithme “Port”. A noter finalement que \\(y_\\infty\\) s’appelle Asym dans tous les modèles ‘selfStart’ dans R, y compris ici, et que \\(t_0\\) s’appelle ici c0. SSAsymp() est définie comme \\(y(t) = y_\\infty + (R_0 - y_\\infty) \\ e^{-e^{lrc} \\cdot t}\\). Par rapport à la forme habituelle, outre l’astuce de \\(e^{lrc}\\) à la place de \\(k\\), \\(t_0\\) est également reparamétré en \\(R_0\\) qui représente la taille initiale au temps \\(t = 0\\). Cela mets l’accent sur cette “taille à la naissance”. Reparamétriser un modèle consiste à exprimer la fonction mathématique qui le représente d’une façon différente. Etant donné que l’interprétation biologique des paramètres fait partie des objectifs de la méthode. On parle d’approche mécanistique, qui vise à décrypter le mécanisme sous-jacent versus une approche purement empirique basée sur les données uniquement avec un modèle polynomial tout venant, par exemple. Ainsi, L’équation du modèle de von Bertalanffy présentée au début montre une paramétrisation “classique”, implémentée dans SSasympOff() alors que SSasymp() mets la taille initiale en évidence via le paramètre \\(R_0\\). SSasympOrig() force le modèle à passer par l’origine des axes {0, 0}. Il n’y a donc plus que deux paramètres Asym = \\(y_\\infty\\) et lrc tel que \\(k = e^{lrc}\\). Ce modèle simplifié est souvent utile en pratique. Il a l’avantage d’être simple, avec seulement deux paramètres. Il est l’équivalent d’une droite forcée à zéro pour le modèle von Bertalanffy. Si R0 dans SSasymp() ou c0 dans SSasympOff() ne sont pas significativement différents de zéro (test t de Student dans le tableau des paramètres), vous pouvez envisager de simplifier le modèle vers SSasympOrig(). Le modèle von Bertalanffy en poids n’est pas implémenté, nous devons utiliser une fonction personnalisée dans ce cas (avec une paramétrisation similaire à SSasympOff() : asympOff3 &lt;- function(x, Asym, lrc, c0, m) Asym*(1 - exp(-exp(lrc) * (x - c0)))^3 Voici les deux modèles de von Bertalanffy présentés sur le même graphique avec \\(y_\\infty\\) (alias Asym) = 0,95, lrc = 0,1 et \\(t_0\\) (alias c0) = 0. vb_data &lt;- tibble( t = seq(0, 10, by = 0.1), y = SSasympOff(t, Asym = 0.95, lrc = 0.1, c0 = 0), y3 = asympOff3(t, Asym = 0.95, lrc = 0.1, c0 = 0), ) chart(data = vb_data, y ~ t %col=% &quot;VB en taille&quot;) + geom_line() + geom_line(f_aes(y3 ~ t %col=% &quot;VB en poids&quot;)) + geom_vline(xintercept = 0, col = &quot;darkgray&quot;) + geom_hline(yintercept = 0.95, col = &quot;gray&quot;, linetype = &quot;dashed&quot;) + annotate(&quot;text&quot;, label = &quot;Asym&quot;, x = -0.4, y = 0.95) + labs(color = &quot;Modèle&quot;) Dans les deux cas l’asymptote représentant la taille maximale possible vaut 0,95. Pour le modèle pondéral, c’est le cube de la valeur obtenue avec SSasympOff() appliquée sur la racine cubique des masses. Dans les deux cas, lrc = 0,1 donc \\(k = e^{0,1} = 1,1\\) et c0 = 0, la droite passe par l’origine (donc, nous aurions également pu utiliser SSasympOrig() pour générer ces données. 4.3.7 Modèle de Richards La forme généralisée du modèle de von Bertalanffy est : \\[y(t) = y_\\infty \\ (1 - e^{-k \\ (t - t_0)})^m\\] Von Bertalanffy (1938, 1957) a fixé \\(m\\) à 1 ou à 3. Richards (1959) permet à \\(m\\) de varier librement, et donc son modèle a un paramètre de plus. Cette dernière courbe est très flexible (voir schéma ci-dessous) et il est possible de démontrer que plusieurs autres modèles de croissance ne sont que des cas particuliers de ce modèle généraliste avec différentes valeurs de \\(m\\). Nous avons déjà observé que le modèle de Richards se réduit aux deux modèles de von Bertalanffy quand \\(m = 1\\) et \\(m = 3\\). Il se réduit aussi à une courbe logistique lorsque \\(m = -1\\) et il est possible de montrer qu’il converge vers le modèle de Gompertz lorsque \\(|m| \\rightarrow \\infty\\). Il n’existe aucun modèle ‘selfStart’ pour la courbe de Richards dans R. Par ailleurs, il s’agit d’un modèle particulièrement délicat à ajuster, comme nous le verrons dans un exemple concret plus loin dans la section “choix du modèle”. Avec une paramétrisation proche de celle de SSasympOff(), la fonction de Rcichards peut s’écrire comme ceci : richards &lt;- function(x, Asym, lrc, c0, m) Asym*(1 - exp(-exp(lrc) * (x - c0)))^m Voici l’allure de différentes courbes de Richards en fonction de la valeur de \\(m\\) (0,5, 1, 3, 6 et 9) avec \\(lrc = 0,1\\), \\(y_\\infty = 0,95\\) et \\(t_0 = 0\\) pour toutes les courbes. rich_data &lt;- tibble( t = seq(0, 10, by = 0.1), y = richards(t, Asym = 0.95, lrc = 0.1, c0 = 0, m = 1), y05 = richards(t, Asym = 0.95, lrc = 0.1, c0 = 0, m = 0.5), y3 = richards(t, Asym = 0.95, lrc = 0.1, c0 = 0, m = 3), y6 = richards(t, Asym = 0.95, lrc = 0.1, c0 = 0, m = 6), y9 = richards(t, Asym = 0.95, lrc = 0.1, c0 = 0, m = 9) ) chart(data = rich_data, y ~ t %col=% &quot;m = 1&quot;) + geom_line() + geom_line(f_aes(y05 ~ t %col=% &quot;m = 0.5&quot;)) + geom_line(f_aes(y3 ~ t %col=% &quot;m = 3&quot;)) + geom_line(f_aes(y6 ~ t %col=% &quot;m = 6&quot;)) + geom_line(f_aes(y9 ~ t %col=% &quot;m = 9&quot;)) + geom_vline(xintercept = 0, col = &quot;darkgray&quot;) + geom_hline(yintercept = 0.95, col = &quot;gray&quot;, linetype = &quot;dashed&quot;) + annotate(&quot;text&quot;, label = &quot;Asym&quot;, x = -0.4, y = 0.95) + labs(color = &quot;Richards avec :&quot;) 4.3.8 Modèle de Weibull Depuis son introduction en 1951 par Weibull, ce modèle est présenté comme polyvalent. Il a été décrit à l’origine comme une distribution statistique. Il trouve de nombreuses applications en croissance de population (éventuellement négative), et il est utilisé également pour décrire la courbe de survie en cas de maladie ou dans des études de dynamique de populations. Il a parfois été utilisé comme un modèle de croissance. La forme la plus générale de ce modèle est  : \\[y(t) = y_\\infty - d \\ e^{-k \\ t^m}\\] avec \\(d = y_\\infty - y_0\\). Un modèle à trois paramètres est également utilisé où \\(y_0 = 0\\). La fonction est sigmoïdale lorsque \\(m &gt; 1\\), sinon elle ne possède pas de point d’inflexion (voir graphique ci-dessous). Dans R, le modèle ‘selfStart’ s’appelle SSweibull(). Comme pour le modèle von Bertalanffy, le paramètre \\(k\\) est contraint à une valeur positive via l’astuce \\(k = e^{lrc}\\). Comme d’habitude, \\(y_\\infty\\) se nomme Asym. \\(d\\) est ici appelé Drop et \\(m\\) est appelé pwr. Voici l’allure de différentes courbes de Weibull pour respectivement \\(m\\) (alias pwr) = 5, 2, 1 et 0,5, avec \\(lrc = -0,5\\) (donc, \\(k = e^{-0,5}\\) = 0,61), \\(y_\\infty\\) (alias Asym) = 0,95 et \\(y_0= 0,05\\), ce qui donne \\(d\\) (alias Drop) = 0,95 - 0,05 = 0,9. La courbe avec \\(m = 1\\) est équivalente à un modèle de von Bertalanffy linéaire. Toutes les courbes démarrent en \\(y_0\\) et passent par \\(y_\\infty - d \\ e^{-k}\\) qui est également le point d’inflexion pour les sigmoïdes lorsque \\(m &gt; 1\\). weib_data &lt;- tibble( t = seq(0, 10, by = 0.1), y = SSweibull(t, Asym = 0.95, Drop = 0.9, lrc = -0.5, pwr = 1), y05 = SSweibull(t, Asym = 0.95, Drop = 0.9, lrc = -0.5, pwr = 0.5), y2 = SSweibull(t, Asym = 0.95, Drop = 0.9, lrc = -0.5, pwr = 2), y5 = SSweibull(t, Asym = 0.95, Drop = 0.9, lrc = -0.5, pwr = 5) ) chart(data = weib_data, y ~ t %col=% &quot;m = 1&quot;) + geom_line() + geom_line(f_aes(y05 ~ t %col=% &quot;m = 0.5&quot;)) + geom_line(f_aes(y2 ~ t %col=% &quot;m = 2&quot;)) + geom_line(f_aes(y5 ~ t %col=% &quot;m = 5&quot;)) + geom_vline(xintercept = 0, col = &quot;darkgray&quot;) + geom_hline(yintercept = c(0.05, 0.95, 0.95 - 0.9 * exp(-exp(-0.5))), col = &quot;gray&quot;, linetype = &quot;dashed&quot;) + annotate(&quot;text&quot;, label = &quot;Asym&quot;, x = -0.4, y = 0.95) + annotate(&quot;text&quot;, label = &quot;y0&quot;, x = -0.4, y = 0.05) + annotate(&quot;text&quot;, label = &quot;Asym-d*e^-k&quot;, x = 0, y = 0.95 - 0.9 * exp(-exp(-0.5))) + annotate(&quot;text&quot;, label = &quot;point d&#39;inflexion si m &gt; 1&quot;, x = 3, y = 0.43) + labs(color = &quot;Weibull avec :&quot;) 4.3.9 Modèle Preece-Baines 1 Preece et Baines (1978) ont décrit plusieurs modèles spécifiques à la croissance humaine. Ces modèles combinent deux phases de croissance exponentielle pour représenter la croissance graduelle d’enfants suivie par une courte phase de croissance accélérée à l’adolescence, mais qui atteint rapidement un plateau correspondant à la taille adulte définitive (voir graphique ci-dessous). Ce type de modèle est naturellement très utile pour tous les mammifères, mais certains, comme le modèle 1 présenté ici, ont aussi été utilisés dans d’autres circonstances, profitant de sa grande flexibilité. Son équation est : \\[y(t) = y_\\infty - \\frac{2 (y_\\infty - d)}{e^{k1 \\ (t - t_0)} + e^{k2 \\ (t - t_0)}}\\] Dans R, la fonction à utiliser (avec une paramétrisation similaire à celle des autres modèles ‘selfStart’) est : preece_baines1 &lt;- function(x, Asym, Drop, lrc1, lrc2, c0) Asym - (2 * (Asym - Drop)) / (exp(exp(lrc1) * (x - c0)) + exp(exp(lrc2) * (x - c0))) \\(k1\\) et \\(k2\\) sont forcés à des valeurs positives ou nulles grace à l’astuce de passer par lrc1 et lrc2. \\(d\\) est Drop et \\(t_0\\) est c0. Ci-dessous un exemple de courbe Preece-Baines 1 avec \\(y_\\infty\\) (alias Asym) = 0,95, \\(d\\) (alias Drop) = 0,8, \\(k1 = 0,19\\) (alias lrc1 = log(0,19) = -1,7), \\(k2 = 2,5\\) (alias lrc2= log(2,5) = 0,92) et \\(t_0\\) (alias c0) = 6. pb1_data &lt;- tibble( t = seq(0, 10, by = 0.1), y = preece_baines1(t, Asym = 0.95, Drop = 0.8, lrc1 = -1.7, lrc2 = 0.92, c0 = 6) ) chart(data = pb1_data, y ~ t) + geom_line() + geom_vline(xintercept = 0, col = &quot;darkgray&quot;) + geom_hline(yintercept = 0.95, col = &quot;gray&quot;, linetype = &quot;dashed&quot;) + annotate(&quot;text&quot;, label = &quot;Asym&quot;, x = -0.4, y = 0.95) 4.3.10 Modèle de Tanaka Tous les modèles précédents sont asymptotiques, à l’exception de la courbe exponentielle (mais cette dernière ne modélise valablement que la phase de croissance initiale). Tous ces modèles décrivent donc une croissance déterminée qui n’excédera jamais une taille maximale représentée par une asymptote horizontale en \\(y(t) = y_\\infty\\). Knight (1968) s’est demandé s’il s’agit d’une réalité biologique ou simplement d’un artefact mathématique. Dans le second cas, la croissance n’apparaîtrait déterminée que parce que les modèles choisis pour la représenter sont asymptotiques. Pour s’affranchir d’une telle contrainte, Tanaka (1982, 1988) a élaboré un nouveau modèle de croissance qui décrit une croissance non déterminée : \\[y(t) = \\frac{1}{\\sqrt{b}} \\ \\ln |2 \\ b \\ (t - t_0) + 2 \\ \\sqrt{b^2 \\ (t - t_0)^2 + a \\ b}| + d\\] Ce modèle complexe à quatre paramètres a une période initiale de croissance lente, suivie d’une période de croissance exponentielle qui se poursuit par une croissance continue mais plus faible tout au long de la vie de l’animal (voir graphique ci-dessous). Dans R, nous pouvons utiliser la fonction suivante pour ajuster un modèle de Tanaka : tanaka &lt;- function(x, a, b, c0, d) 1 / sqrt(b) * log(abs(2 * b * (x - c0) + 2 * sqrt(b^2 * (x - c0)^2 + a * b))) + d Ci-dessous, un exemple de courbe de Tanaka avec \\(a\\) = 3, \\(b\\) = 2,5, \\(d\\) = -0,2 et \\(t_0\\) (alias c0) = 2. tanaka_data &lt;- tibble( t = seq(0, 10, by = 0.1), y = tanaka(t, a = 3, b = 2.5, c0 = 2, d = -0.2) ) chart(data = tanaka_data, y ~ t) + geom_line() + geom_vline(xintercept = 0, col = &quot;darkgray&quot;) "],
["choix-du-modele.html", "4.4 Choix du modèle", " 4.4 Choix du modèle Le choix d’un modèle non linéaire fait intervenir des critères identiques à ceux d’un modèle linéaire (qualité d’ajustement évaluée par l’AIC, inspection visuelle de l’ajustement dans le nuage de points), mais il fait aussi intervenir une dimension supplémentaire : le choix de la fonction marthématique à ajuster. Comme nous venons de le voir au travers de quelques modèles courants en biologie, le nombre de fonctions mathématiques résultant en des formes similaire, par exemple de type sigmoïde, est grand. Ainsi, le choix de la meilleure fonction à utiliser dans un cas particulier est rendu plus difficile. Nous allons illustrer l’ajustement d’une courbe non linéaire par le choix et l’ajustement d’un modèle de croissance dans un jeu de données en modélisant la croissance somatique de l’oursin Paracentrotus lividus (Grosjean 2001). Ce jeu de données est disponible dans le package data.io, sous le nom urchin_growth. SciViews::R urchins &lt;- read(&quot;urchin_growth&quot;, package = &quot;data.io&quot;, lang = &quot;fr&quot;) chart(data = urchins, diameter ~ age) + geom_point() Comme vous pouvez le voir, différents oursins ont été mesurés via le diamètre à l’ambitus du test (zone la plus large) en mm à différents âges (en années). Les mesures ont été effectuées tous les 3 à 6 mois pendant plus de 10 ans, ce qui donne un bon aperçu de la croissance de cet animal y compris la taille maximale asymptotique qui est atteinte vers les 4 à 5 ans (pour ce genre de modèle, il est très important de continuer à mesurer les animaux afin de bien quantifier cette taille maximale asymptotique). Ainsi, l’examen du graphique nous permet d’emblée de choisir un modèle à croissance finie (pas le modèle de Tanaka, donc), et de forme sigmoïdale. Les modèles logistique, Weibull ou Gompertz pourraient convenir par exemple. Nous pouvons à ce stade, essayer différents modèles et choisir celui qui nous semble le plus adapté. Le choix du meilleur modèle se fait grâce à deux critères : Les connaissances théoriques et a priori du modèle que l’on ajuste. En effet, il n’existe qu’un seul modèle linéaire, mais une infinité de modèles curvilinéaires qui peuvent s’ajuster dans les données. Le choix du meilleur modèle se fait en fonction de considérations sur le phénomène sous-jacent qui doivent se refléter dans les propriétés mathématiques de la courbe choisie. Par exemple, si on sait que la croissance est asymptotique vers une taille maximale, nous devrons choisir une courbe mathématique qui présente une asymptote horizontale à son maximum pour représente au mieux le phénomène étudié. Le coefficient de détermination \\(R^2\\) n’est pas calculé par R pour une régression non linéaire car sa validité est sujette à discussion entre les statisticiens (d’autres logiciels statistiques le calculent). Nous n’avons donc pas d’estimation de la qualité de l’ajustement par ce biais, comme dans le cas de la régression linéaire. Par contre, il est possible de calculer un autre critère plus fiable que nous avons déjà utilisé : le critère d’Akaïke (fonction AIC() dans R). Ce critère tient compte à la fois de la qualité d’ajustement et de la complexité du modèle, exprimée par le nombre de paramètres qu’il faut estimer. Plus le modèle est complexe, plus on peut s’attendre à ce qu’il s’ajuste bien aux données car il est plus flexible. Cependant, en ce domaine, la complexité n’est pas forcément un gage de qualité. On recherche plutôt un compromis entre meilleur ajustement et simplicité. Le critère d’information d’Akaiké quantifie précisément ce compromis, c’est-à-dire que le modèle qui a un AIC le plus faible est considéré comme le meilleur. Appliquons donc ce concept pour sélectionner le meilleur modèle de croissance pour décrire la croissance somatique de nos oursins après avoir sélectionné les modèles candidats les plus judicieux (modèle sigmoïdal avec asymptote horizontale au maximum). Notons toutefois que, comme les animaux sont mesurés aux mêmes âges, tous les 3 à 6 mois, certains points se superposent. Afin d’afficher tous les points, il est utile d’utiliser la fonction geom_jitter() à la place de geom_point() qui décale les points d’une valeur aléatoire pour éviter ces superpositions (l’argument width = indique le décalage maximum à appliquer). Voici ce que cela donne (en ajoutant également un titre avec formattage correct du nom en latin)  : urchins_plot &lt;- chart(data = urchins, diameter ~ age) + geom_jitter(width = 0.1, alpha = 0.2, color = &quot;darkgrey&quot;) + ggtitle(expression(paste(&quot;Croissance de l&#39;oursin &quot;, italic(&quot;Paracentrotus lividus&quot;)))) urchins_plot Nous avons ici également représenté les points de manière semi-transparente avec alpha = 0.2(transparence de 20%) pour encore mieux mettre en évidence les points de mesures qui se superposent. Ajustons maintenant un modèle de Gompertz (modèle ‘SelfStart’) : urchins_gomp &lt;- nls(data = urchins, diameter ~ SSgompertz(age, Asym, b2, b3)) summary(urchins_gomp) # # Formula: diameter ~ SSgompertz(age, Asym, b2, b3) # # Parameters: # Estimate Std. Error t value Pr(&gt;|t|) # Asym 57.403687 0.257588 222.85 &lt;2e-16 *** # b2 3.901916 0.046354 84.18 &lt;2e-16 *** # b3 0.434744 0.003852 112.86 &lt;2e-16 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 5.49 on 7021 degrees of freedom # # Number of iterations to convergence: 3 # Achieved convergence tolerance: 2.655e-06 Utilisons maintenant notre fonction as.function.nls() pour ajouter la courbe sur le graphique. as.function.nls &lt;- function(x, ...) { nls_model &lt;- x name_x &lt;- names(nls_model$dataClasses) stopifnot(length(name_x) == 1) function(x) predict(nls_model, newdata = structure(list(x), names = name_x)) } A présent, nous pouvons faire ceci : urchins_plot + stat_function(fun = as.function(urchins_gomp), color = &quot;red&quot;, size = 1) L’ajustement de cette fonction semble très bon, à l’oeil. Voyons ce qu’il en est d’autres modèles. Par exemple, une courbe logistique : urchins_logis &lt;- nls(data = urchins, diameter ~ SSlogis(age, Asym, xmid, scal)) summary(urchins_logis) # # Formula: diameter ~ SSlogis(age, Asym, xmid, scal) # # Parameters: # Estimate Std. Error t value Pr(&gt;|t|) # Asym 54.628069 0.202985 269.1 &lt;2e-16 *** # xmid 2.055285 0.009568 214.8 &lt;2e-16 *** # scal 0.764830 0.007355 104.0 &lt;2e-16 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 5.6 on 7021 degrees of freedom # # Number of iterations to convergence: 4 # Achieved convergence tolerance: 1.079e-06 Et voici le graphique avec les deux modèles superposés : urchins_plot + stat_function(fun = as.function(urchins_gomp), aes(color = &quot;Gompertz&quot;), size = 1) + stat_function(fun = as.function(urchins_logis), aes(color = &quot;logistique&quot;), size = 1) + labs(color = &quot;Modèle&quot;) Notez que ici, la couleur a été incluse dans le “mapping” (argument mapping =) de stat_function() en l’incluant dans aes(). Cela change fondamentalement la façon dont la couleur est perçue par ggplot2. Dans ce cas-ci, la valeur est interprétée non comme une couleur à proprement parler, mais comme un niveau (une couche) à inclure dans le graphique et à reporter via une légende. Ensuite, à l’aide de labs() on change le titre de la légende relatif à la couleur par un nom plus explicite : “Modèle”. Nous pouvons comparer ces modèles à l’aide du critère d’Akaïke. AIC(urchins_gomp, urchins_logis) # df AIC # urchins_gomp 4 43861.73 # urchins_logis 4 44139.13 Comme on peut le voir clairement sur le graphe, la courbe logistique donne une autre solution, cette dernière est à peine moins bonne que le modèle de Gompertz, selon le critère d’Akaiké. Pourtant, la courbe est assez bien démarquée de celle de Gompertz. Essayons maintenant un modèle de Weibull. Ce modèle est plus complexe car il a quatre paramètres au lieu de trois pour les deux modèles précédents : urchins_weib &lt;- nls(data = urchins, diameter ~ SSweibull(age, Asym, Drop, lrc, pwr)) summary(urchins_weib) # # Formula: diameter ~ SSweibull(age, Asym, Drop, lrc, pwr) # # Parameters: # Estimate Std. Error t value Pr(&gt;|t|) # Asym 56.80491 0.32704 173.69 &lt;2e-16 *** # Drop 56.81320 0.59393 95.66 &lt;2e-16 *** # lrc -1.57392 0.02865 -54.94 &lt;2e-16 *** # pwr 1.67880 0.02960 56.72 &lt;2e-16 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 5.47 on 7020 degrees of freedom # # Number of iterations to convergence: 3 # Achieved convergence tolerance: 2.532e-06 Ajoutons ce nouveau modèle sur le graphique : urchins_plot + stat_function(fun = as.function(urchins_gomp), aes(color = &quot;Gompertz&quot;), size = 1) + stat_function(fun = as.function(urchins_logis), aes(color = &quot;logistique&quot;), size = 1) + stat_function(fun = as.function(urchins_weib), aes(color = &quot;Weibull&quot;), size = 1) + labs(color = &quot;Modèle&quot;) … et comparons à l’aide du critère d’Akaïke : AIC(urchins_gomp, urchins_logis, urchins_weib) # df AIC # urchins_gomp 4 43861.73 # urchins_logis 4 44139.13 # urchins_weib 5 43810.72 Ce modèle fait presque jeu égal avec le modèle de Gompertz en terme de critère d’Akaiké ; juste un tout petit peu mieux. En fait, les deux courbes sont pratiquement superposées l’une à l’autre, mais le modèle de Weibull à un démarrage de croissance plus lent au début, ce qui se reflète dans les données. Par contre, il est pénalisé par le fait que c’est un modèle plus complexe qui possède un paramètre de plus. L’un dans l’autre, le critère d’information d’Akaiké considère donc les deux modèles sur pratiquement sur le même plan du point de vue de la qualité de leurs ajustements respectifs. A ce stade, nous voudrions également essayer un autre modèle flexible à quatre paramètres : le modèle de Richards. Malheureusement, il n’existe pas de fonction ‘SelfStart’ dans R pour ce modèle. Nous sommes donc réduit “à mettre les mains dans le cambouis”, à définir la fonction nous même, à trouver de bonnes valeurs de départ, etc. Voici comment définir la fonction : richards &lt;- function(x, Asym, lrc, c0, m) Asym*(1 - exp(-exp(lrc) * (x - c0)))^m Pour les valeurs de départ, là ce n’est pas facile. Asym est l’ asymptote horizontale à la taille maximum. On voit qu’elle se situe aux environ de 55 mm sur le graphique. Pour les autres paramètres, c’est plus difficile à évaluer. Prenons par exemple 1 comme valeur de départ pour les trois autres paramètres, ce qui donne (les valeurs de départ sont obligatoires ici puisque ce n’est pas un modèle ‘SelfStart’) : urchins_rich &lt;- nls(data = urchins, diameter ~ richards(age, Asym, lrc, c0, m), start = c(Asym = 55, lrc = 0.1, c0 = 1, m = 1)) # Error in numericDeriv(form[[3L]], names(ind), env): Missing value or an infinity produced when evaluating the model … et voilà ! Un excellent exemple de plantage de l’algorithme de minimisation de la fonction objective suite à un comportement inadéquat de la fonction avec les valeurs testées. Ici, la fonction renvoie l’infini et l’algorithme ne peut donc effectuer la minimisation. La fonction de Richards est effectivement connue pour être difficile à ajuster pour cette raison. Il nous faut donc soit tester d’autres valeurs de départ, soit utiliser un autre algorithme de minimisation, soit les deux. Après différents essais il apparaît que le changement des valeurs de départ suffit dans le cas présent : urchins_rich &lt;- nls(data = urchins, diameter ~ richards(age, Asym, lrc, c0, m), start = c(Asym = 55, lrc = -0.7, c0 = 0, m = 1)) summary(urchins_rich) # # Formula: diameter ~ richards(age, Asym, lrc, c0, m) # # Parameters: # Estimate Std. Error t value Pr(&gt;|t|) # Asym 58.14348 0.37772 153.934 &lt; 2e-16 *** # lrc -0.27595 0.03152 -8.754 &lt; 2e-16 *** # c0 -0.87545 0.28464 -3.076 0.002109 ** # m 6.20711 1.82345 3.404 0.000668 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 5.487 on 7020 degrees of freedom # # Number of iterations to convergence: 11 # Achieved convergence tolerance: 7.11e-06 Ajoutons ce dernière modèle sur notre graphique (avec des traits un peu plus fins pour mieux distinguer les modèles les uns des autres) : urchins_plot + stat_function(fun = as.function(urchins_gomp), aes(color = &quot;Gompertz&quot;), size = 0.7) + stat_function(fun = as.function(urchins_logis), aes(color = &quot;logistique&quot;), size = 0.7) + stat_function(fun = as.function(urchins_weib), aes(color = &quot;Weibull&quot;), size = 0.7) + stat_function(fun = as.function(urchins_rich), aes(color = &quot;Richards&quot;), size = 0.7) + labs(color = &quot;Modèle&quot;) … et comparons à l’aide du critère d’Akaïke : AIC(urchins_gomp, urchins_logis, urchins_weib, urchins_rich) # df AIC # urchins_gomp 4 43861.73 # urchins_logis 4 44139.13 # urchins_weib 5 43810.72 # urchins_rich 5 43854.53 La courbe est très proche des modèles de Gompertz et Weibull aux jeunes âges, mais l’asymptote maximale est légèrement plus haute que pour les deux autres modèles (58 mm au lieu de 57 mm). Les trois courbes sont très, très proches l’une de l’autre. Le critère d’information d’Akaiké est marginalement moins bon pour le modèle de Richards que pour celui de Weibull, mais est tout juste meilleur que celui de Gompertz. En outre l’écart type pour le paramètre x0 est plus conséquent en comparaison de sa valeur, ce qui démontre une certaine instabilité de la fonction par rapport à ce paramètre, et par conséquent, une incertitude dans son estimation. Pour cette raison, det pour la difficulté à l’ajuster, le modèle de Richards sera écarté dans notre cas au benefice du modèle de Weibull, voire de celui de Gompertz plus simple. Le choix final entre Gompertz ou Weibull dépend de l’usage que l’on veut faire du modèle. Si la simplicité du modèle est primordiale, nous garderons Gompertz. Si la croissance des petits oursins est un aspect important de l’analyse, nous garderons Weibull qui semble mieux s’ajuster aux données à ce niveau. A vous de jouer ! Réalisez un cahier de laboratoire sur la croissance de bactérie en définissant un modèle non linéaire pertinent pour ces données. Vous avez à votre disposition une assignation GitHub Classroom : https://classroom.github.com/a/kSLMNWpw Lisez le README afin de prendre connaissance de l’exercice Références "],
["hierarchique.html", "Module 5 Classification hiérarchique", " Module 5 Classification hiérarchique Objectifs Comprendre la notion de distance et la matrice de distance. Appréhender la classification hiérarchique et le dendrogramme. Être capable d’effectuer un regroupement pertinent des individus d’un jeu de données multivarié à l’aide de ces techniques. Prérequis Vous devez être à l’aise avec l’utilisation de R et Rstudio, en particulier pour l’importation, le remaniement et la visualisation de données multivariées. Ceci correspond au cours SDD I. Il n’est pas nécessaire d’avoir acquis toutes les notions vue dans la partie Cours II :modélisation pour pouvoir comprendre cette seconde partie du cours. Si vous ne vous sentez pas assez à l’aise avec R et RStudio, c’est peut-être le bon moment pour refaire le premier “learnr” du package BioDataScience2 : Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant les bases de R : BioDataScience2::run(&quot;01a_rappel&quot;) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel dans la console R. "],
["analyse-de-donnees.html", "5.1 Analyse de données", " 5.1 Analyse de données L’analyse de données (on dit aussi analyse exploratoire des données, EAD ou statistiques exploratoires) mets en œuvre des méthodes statistiques multivariées visant à découvrir de l’information pertinente dans un gros jeu de données via des approches multidimensionnelles et essentiellement descriptives. Ces méthodes se regroupent en deux grandes familles : Celles visant à réduire la dimensionnalité (travailler avec des tableaux ayant moins de colonnes). Elles permettent ensuite de présenter les données de manière synthétique pour observer des relations entre les variables ou les individus via des représentations graphiques. Nous aborderons ces techniques dans les modules suivants. Celles cherchant à classifier (ou regrouper) les individus. Il s’agit ici de synthétiser le gros tableau de données dans l’autre sens, selon les lignes. L’approche via la classification hiérarchique sera détaillée ici. La vidéo suivante introduit l’EAD (jusqu’à 2:11) : "],
["distance-entre-individus.html", "5.2 Distance entre individus", " 5.2 Distance entre individus Vous êtes en train d’analyser des données concernant les échantillons de plancton que vous avez prélevé sur votre lieu de recherche. Ce plancton a été numérisé (photo de chaque organisme) et les images ont été traitée avec un logiciel qui mesure automatiquement une vingtaine de variables telles que la surface de l’objet sur l’image, son périmètre, sa longueur, … Vous vous trouvez donc face à un jeu de données qui a une taille non négligeable : 20 colonnes par 1262 lignes, soit le nombre d’individus mesurés dans vos échantillons. zoo &lt;- read(&quot;zooplankton&quot;, package = &quot;data.io&quot;) zoo # # A tibble: 1,262 x 20 # ecd area perimeter feret major minor mean mode min max std_dev # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 0.770 0.465 4.45 1.32 1.16 0.509 0.363 0.036 0.004 0.908 0.231 # 2 0.700 0.385 2.32 0.728 0.713 0.688 0.361 0.492 0.024 0.676 0.183 # 3 0.815 0.521 4.15 1.33 1.11 0.598 0.308 0.032 0.008 0.696 0.204 # 4 0.785 0.484 4.44 1.78 1.56 0.394 0.332 0.036 0.004 0.728 0.218 # 5 0.361 0.103 1.71 0.739 0.694 0.188 0.153 0.016 0.008 0.452 0.110 # 6 0.832 0.544 5.27 1.66 1.36 0.511 0.371 0.02 0.004 0.844 0.268 # 7 1.23 1.20 15.7 3.92 1.37 1.11 0.217 0.012 0.004 0.784 0.214 # 8 0.620 0.302 3.98 1.19 1.04 0.370 0.316 0.012 0.004 0.756 0.246 # 9 1.19 1.12 15.3 3.85 1.34 1.06 0.176 0.012 0.004 0.728 0.172 # 10 1.04 0.856 7.60 1.89 1.66 0.656 0.404 0.044 0.004 0.88 0.264 # # … with 1,252 more rows, and 9 more variables: range &lt;dbl&gt;, size &lt;dbl&gt;, # # aspect &lt;dbl&gt;, elongation &lt;dbl&gt;, compactness &lt;dbl&gt;, transparency &lt;dbl&gt;, # # circularity &lt;dbl&gt;, density &lt;dbl&gt;, class &lt;fct&gt; Vous voulez regrouper votre plancton en fonction de la ressemblance entre les organismes, c’est-à-dire, en fonction des écarts entre les mesures effectuées pour les 19 variables quantitatives, à l’exclusion de la vingtième colonne class qui est une variable factor). En raison de la taille du tableau, il est évident que cela ne pourra pas se faire de manière manuelle. Nous pouvons raisonnablement considérer que plus les mesures sont similaires entre deux individus, plus ils ont des chances d’être semblables, c’est-à-dire, d’appartenir au même groupe taxonomique. Mais comment faire pour synthétiser l’information de similarité ou différence contenue dans 19 paires de valeurs (une paire par variable)  ? Nous avons besoin d’une mesure de distance qui quantifie la similarité (ou à l’inverse la dissimilarité) en un seul nombre. Celle qui vient naturellement à l’esprit est la distance euclidienne. Prenons un cas simplifié. Quelle est la distance qui sépare deux individus A et B par rapport à trois variables x, y, z ? Ici, nous pouvons représenter l’information graphiquement dans un espace à trois dimensions. La distance qui nous intéresse est la distance linéaire entre les deux points dans l’espace. Autrement dit, c’est la longueur du segment de droite qui relie les deux points dans l’espace. Cette distance, nous pouvons la calculer à l’aide de la formule suivante (problème de niveau lycée, voir par exemple ici pour une résolution dans le plan) : \\[\\mathrm{D_{Euclidean}}_{A, B} = \\sqrt{(x_A - x_B)^2 + (y_A - y_B)^2 + (z_A - z_B)^2}\\] Notez que cette formule se généralise à n dimensions et s’écrit alors, pour n’importe quelle paire d’individus indicés j et k dans notre tableau et pour les différentes mesures de 1 à i notées yi : \\[\\mathrm{D_{Euclidean}}_{j, k} = \\sqrt{\\sum_{i=1}^{n}(y_{ij}-y_{ik})^2}\\] C’est la racine carré de la somme dans les n dimensions des écarts entre les valeurs au carré pour toutes les variables yi. Plus sa valeur est grande, plus les individus sont éloignés (différents). Pour cette raison, nous appelerons cette distance, une mesure de dissimilarité. 5.2.1 Matrice de distances Nous avons maintenant la possibilité de quantifier la similitude entre nos organismes plactoniques… mais nous en avons un grand nombre. Cela va être impossible à gérer autant de mesures qu’il y a de paires possibles parmi 1262 individus12. La matrice de distance est une matrice ici 1262 par 1262 qui rassemble toutes les valeurs possibles. Notez que sur la diagonale, nous comparons chaque individu avec lui-même. La distance euclidienne vaut donc systématiquement zéro sur la diagonale. \\[\\mathrm{D_{Euclidean}}_{j, j} = 0\\] De plus, de part et d’autre de cette diagonale, nous trouvons les paires complémentaires (j versus k d’un côté et k versus j de l’autre). Or qu’elle soit mesurée dans un sens ou dans l’autre, la distance du segment de droite qui relie deux points dans l’espace est toujours la même. \\[\\mathrm{D_{Euclidean}}_{j, k} = \\mathrm{D_{Euclidean}}_{k, j}\\] Par conséquent, seulement une portion (soit le triangle inférieur, soit le triangle supérieur hors diagonale) est informative. La diagonale ne porte aucune informartion utile, et l’autre triangle est redondant. Nous avons donc pour habitude de ne calculer et représenter que le triangle inférieur de cette matrice. Maintenant que cela est clair, nous pouvons créer un objet dist qui contiendra notre matrice de distances enclidiennes. Il suffit d’utiliser la fonction dist(), ou mieux vegan::vegdist() qui offre plus de possibilités13. Comme cela prendrait trop de place d’imprimer la matrice complète, nous allons réaliser le travail sur seulement les six premiers individus de notre tableau (et nous devons aussi éliminer la colonne class qui ne contient pas de données numériques et qui ne nous intéresse pas pour le moment) : zoo %&gt;.% select(., -class) %&gt;.% # Elimination de la colonne class head(., n = 6) -&gt; zoo6 # Récupération des 6 premiers individus zoo6_dist &lt;- vegan::vegdist(zoo6, method = &quot;euclidean&quot;) zoo6_dist # En pratique, on n&#39;imprime généralement ce genre d&#39;objet ! # 1 2 3 4 5 # 2 8.2185826 # 3 2.5649705 5.7320911 # 4 0.8582142 7.8813537 2.2220079 # 5 4.8478629 4.2950067 3.0119468 4.6148173 # 6 2.4269520 10.6197317 4.9228255 2.8477882 7.1766853 Nous voyons bien ici que R n’imprime que le triangle inférieur de notre matrice 6 par 6. Notez aussi que les objets dist de tailles plus réalistes que vous génèrerez dans vos analyses ne sont prévue pour être imprimées et visualisées telles quelles. Il s’agit seulement de la première étape vers une représentation utile qui sera réalisée à la page suivante, à l’aide de la classification hiérarchisée. Félicitations ! Vous venez de calculer votre première matrice de distances. Nous verrons à la page suivante comment nous pouvons utiliser l’information qu’elle contient pour regrouper les individus de manière pertinente. Mais avant cela, nous avons besoin d’un peu de théorie pour bien comprendre quelle métrique choisir pour calculer nos distances et pourquoi. On parle aussi d’indices de similarité ou dissimilarité. Attention : nous n’avons pas considéré ici les unités respectives de nos variables. Une surface (mm2) ou une longeur (mm) ne sont pas mesurées dans les mêmes unités. Nous risquons alors de donner plus de poids aux valeurs élevées. Nous aurions le même effet si nous décidions par exemple d’exprimer une mesure longitudinale en µm au leiu de l’exprimer en mm. Dans ce cas, il vaut mieux standardiser d’abord le tableau (moyenne de zéro et écart type de un) selon les colonnes avant d’effectuer le calcul. Ceci sera fait à la page suivante. 5.2.2 Indices de (dis)similarité Un indice de similarité (similarity index en anglais) est une descripteur statistique (nombre unique) de la similitude de deux échantillons ou individus représentés par plusieurs variables dans un échantillon multivarié. Un indice de similarité prend une valeur comprise entre 0 (différence totale) et 1 ou 100% (similitude totale). Un indice de dissimilarité} est le complément d’un indice de similarité (dis = 1 – sim); sa valeur est comprise entre 100% (différence totale) et 0 (similitude totale). Attention : dans certains cas, un indice de dissimilarité peut varier de 0 à +\\(\\infty\\)**. Il n’existe alors pas d’in,dice de similarité complémentaire. C’est le cas précisément de la distance euclidienne que nous avons exploré jusqu’ici. Tous les indices de similarité / dissimilarité peuvent servir à construire des matrices de distances. 5.2.2.1 Indice de Bray-Curtis L’indice de dissimilarité de Bary-Curtis, aussi appelé coefficient de Czecanowski est calculé comme suit : \\[\\mathrm{D_{Bray-Curtis}}_{j,k}=\\frac{\\sum_{i=1}^{n}\\left|y_{ij}-y_{ik}\\right|}{\\sum_{i=1}^{n}(y_{ij}+y_{ik})}\\] Dans R nous utiliserons vegan::vegdist(DF, method = &quot;bray&quot;).Il s’utilise pour mesurer la similitude entre échantillon sur base du dénombrement d’espèces. Si le nombre d’individus est très variable (espèces dominantes versus espèces rares), nous devons transformer les données pour éviter de donner trop de poids aux espèces les plus abondantes (ex: \\(log(x+1)\\), double racine carrée, …). Une caractéristique essentielle de cet indice (contrairement à la distance euclidienne) est que toute double absence n’est pas prise en compte dans le calcul. C’est souvent pertinent dans le cadre de son utilisation comme le dénombrement d’espèces. En effet, quelle information utile retire-t-on de doubles zéros dans un tableau répertoriant la faune belge pour le crocodile du Nil et le tigre de Sibérie par exemple ? Aucune ! Ils sont tous deux systématiquement absents des dénombrements, mais cette double absence n’apporte aucune information utile pour caractériser la faune belge par ailleurs. L’indices de similarité de Bray-Curtis (sim) est complémentaire à l’indices de dssimilarité correspondant (dis tel que calculé ci-dessus) : \\[sim = 1 – dis\\] 5.2.2.2 Indice de Canberra L’indice de dissimilarité de Canberra est similaire à l’indice de Bray-Curtis mais il pondère les espèces en fonction du nombre d’occurrences afin de donner le même poids à chacune dans le calcul. Il se calcule comme suit : \\[\\mathrm{D_{Canberra}}_{j,k}=\\frac{1}{nz}\\sum_{i&#39;=1}^{nz}\\frac{\\left|y_{i&#39;j}-y_{i&#39;k}\\right|}{\\left|y_{i&#39;j}\\right|+\\left|y_{i&#39;k}\\right|}\\] où \\(nz\\) est le nombre de valeurs non nulles simultanément dans le tableau de départ. Toutes les espèces contribuent ici de manière égale. C’est un point positif, mais il faut faire attention à ce que cet indice a souvent tendnace à donner une surimportance aux espècces très rares observées une seule fois ou un petit nombre de fois ! Dans R, nous utiliserons vegan::vegdist(DF, method = &quot;canberra&quot;). Toute double absence n’est pas prise en compte ici aussi. Seuls les indices ne dépendant pas des doubles zéros sont utilisables pour des dénombrements d’espèces ou des présence-absence. Ainsi pour ce type de données, notre choix se portera sur : Bray-Curtis si l’on souhaite que le résultat soit dominé par les espèces les plus abondantes. Canberra si notre souhait est de donner la même importance à toutes les espèces, mais avec un risque de domination des espèces rares. Bray-Curtis sur données transformées (\\(log(x+1)\\) ou double racine carrée) pour un compromis entre les deux avec prise en compte de toutes les espèces, mais domination partielle des espèces les plus abondantes. C’est souvent un bon compromis. Attention : Si les volumes échantillonnés entre stations ne sont pas comparables, il faut standardiser (moyenne nulle et écart type un) les données selon les échantillons avant de faire les calculs de distances. De même que pour Bray-Curtis, l’indice de similarité sim se calcule à partir de l’indice de dissimilarité dis tel que ci-dessus comme \\(sim = 1 - dis\\). 5.2.2.3 Distance Euclidienne Nous savons déjà que c’est la distance géométrique entre les points dans un espace à n dimensions : \\[\\mathrm{D_{Euclidean}}_{j,k}=\\sqrt{\\sum_{i=1}^{n}(y_{ij}-y_{ik})^2}\\] Dans R, cette distance peut être calculée avec dist(DF) ou vegan::vegdist(DF, method = &quot;euclidean&quot;). Cet indice de dissimilarité est utile pour des mesures quantitatives, pour des données environnmentales, etc. Il faut que les mesures soient toutes effectuées dans les mêmes unités. Si ce n’est pas le cas, penser alors à standardiser les mesures avant le calcul comme nous l’avons fait plus dans l’exemple sur le zooplancton. Il n’existe pas d’indice de similarité complémentaire. 5.2.2.4 Distance de Manhattan La distance de Manhattan, encore appelée “city-block distance” est un indice de dissimilarité qui, contrairement à la distance euclidienne ne mesure pas la distance géométrique entre les points en ligne droite, mais via un trajet qui suit les parallèmes aux axes. C’est comme si la distance euclidenne reliait les points à vol d’oiseau, alors qu’avec la distance de Manhattan, on devait contourner les blocs de maisons du quartier pour aller d’un point A à un point B (d’où le nom de cette métrique). Elle se calcule comme suit : \\[\\mathrm{D_{Manhattan}}_{j,k}=\\sum_{i=1}^{n}|y_{ij}-y_{ik}|\\] Dans R, nous utiliserons vegan::vegdist(DF, method = &quot;manhattan&quot;). Ici aussi, seul l’indice de dissimilarité est défini. L’indice de similarité complémentaire n’existe pas car la valeur de l’indice de dissimlarité n’est pas borné à droite et peut varier de zéro (dissimilarité nulle, les deux individus soint identiques) à l’infini pour une différence maximale. 5.2.3 Utilisation des indices Les distances euclidienne ou de Manhattan sont à préférer pour les mesures environnementales ou de manière générale pour les variables quantitatives continues. Les distances de Bray-Curtis ou Canberra sont meilleure pour les dénombrements d’espèces (nombreux double zéro), ou de manière générale, pour les variables quantitatives discrètes prenant des valeurs nulles ou positives. 5.2.4 Propriétés des indices Les indices varient en 0 et 1 (0 et 100%), mais les distances sont utilisées aussi comme indices de dissimilarité et varient entre 0 et \\(+\\infty\\). Un indice est dit métrique si : Minimum 0 : \\(I_{j, k} = 0\\) si \\(j = k\\) Positif : \\(I_{j, k}&gt;0\\) si \\(j \\neq k\\) Symétrique : \\(I_{j, k}=I_{k, j}\\) Inégalité triangulaire : \\(I_{j, k} + I_{k, l} &gt;= I_{j, l}\\) La dernière propriété d’inégalité triangulaire est la plus difficile à obtenir, et n’est pas toujours nécessaire. Nous pouvons montrer que certains indices qui ne respectent pas cette dernière propriété sont pourtant utiles dans le contexte. Nous dirons alors d’un indice que c’est une semi-métrique s’il répond à toutes les conditions sauf la quatrième. Enfin, un indice est dit non métrique dans tous les autres cas. Le tableau suivant reprend les métriques que nous avons vu jusqu’ici, et rajoute d’autres candidats potentiels (la distance Chi carré, l’indice de correlation ou de variance/covariance) en indiquant leur type : Distance Type Bray-Curtis semi-métrique Canberra métrique Euclidienne métrique Manhattan métrique Chi carré métrique (correlation) (non métrique) (variance/covariance) (non métrique) Pour en savoir plus Vous pouver aussi transposer le tableau pour calculer la distance entre les variables en utilisant la fonction t() dans R (dist(t(DF))). Par exemple, dans le cas d’un tableau “espèces - station” (dénombrement d’espèces en différentes stations), nous pouvons souhaiter comparer les stations du point de vue de la composition en espèces, mais nous pouvons aussi comparer les espèces du point de vue de leur répartition entre les stations. Pour passer d’un calcul à l’autre, nous transposerons donc le tableau (les colonnes deviennent les lignes et inversément) avant d’utiliser dist() ou vegan::vegdist(). Pour bien comprendre la logique derrière les indices, il est utile de comprendre les équations correspondantes. Si ces équations sont pour vous trop abstraites, une façon efficace de comprendre consiste à faire le calcul à la main. Par exemple dans le cas de l’indice de Canberra, la notion de nombre de données non nulles \\(nz\\) n’est pas évident. Effectuons un calcul à la main détaillé sur le tableau fictif suivant concernant trois espèces A, B, et C dénombrées en trois stations Sta1, Sta2 et Sta3 dans le tableau nommé ex1 : A B C Sta1 4 0 2 Sta2 3 0 10 Sta3 1 8 0 Pour rappel, la dissimilarité de Canberra se calcule comme suit: \\[\\mathrm{D_{Canberra}}_{j,k}=\\frac{1}{nz}\\sum_{i&#39;=1}^{nz}\\frac{\\left|y_{i&#39;j}-y_{i&#39;k}\\right|}{y_{i&#39;j}+y_{i&#39;k}}\\] où: nz est le nombre d’observations non nulles simultanément dans les deux vecteurs comparés (les doubles zéros ne sont pas pris en compte) i’ est l’itérateur sur toutes les valeurs non double zéros Voici le détail du calcul (notez bien comment le double zéro pour l’espèce B entre les stations 1 et 2 est pris en compte dans le calcul) : Sta1_2 &lt;- (1/2) * ((abs(4 - 3)) / (4 + 3) + (abs(2 - 10)) / (2 + 10)) round(Sta1_2, 2) # [1] 0.4 Sta1_3 &lt;- (1/3) * (abs(4 - 1) / (4 + 1) + abs(0 - 8) / (0 + 8) + abs(2 - 0) / (2 + 0)) round(Sta1_3, 2) # [1] 0.87 Sta2_3 &lt;- (1/3) * (abs(3 - 1) / (3 + 1) + abs(0 - 8) / (0 + 8) + abs(10 - 0) / (10 + 0)) round(Sta2_3, 2) # [1] 0.83 La matrice finale est la suivante : # Sta1 Sta2 # Sta2 0.40 # Sta3 0.87 0.83 Vérifions en laissant R faire le calcul : vegan::vegdist(ex1, method = &quot;canberra&quot;) %&gt;.% round(., 2) # Sta1 Sta2 # Sta2 0.40 # Sta3 0.87 0.83 Attention ! dist(, method = &quot;canberra&quot;) ne multiplie pas la somme par \\(1/nz\\), mais par \\(n/nz\\) (variante). Ainsi, nous obtenons des distances trois fois plus grandes ici avec dist() qu’avec vegan::vegdist(), mais les deux calculs restent valables car les écarts relatifs au sein de la matrice de distance sont les mêmes. dist(ex1, method = &quot;canberra&quot;) %&gt;.% round(., 2) # Sta1 Sta2 # Sta2 1.21 # Sta3 2.60 2.50 A vous de jouer ! Réalisez le tutoriel afin de vérifier votre bonne compréhension des matrices de distance. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant les bases de R : BioDataScience2::run(&quot;05a_distance_matrix&quot;) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel dans la console R. Réalisez un carnet de note par binôme sur le transect entre Nice et Calvi. Lisez attentivement le README Vous avez à votre disposition une assignation GitHub Classroom : https://classroom.github.com/g/R45egn-p Lisez le README afin de prendre connaissance de l’exercice Le nombre de paires uniques et distinctes (pas j, j ou k, k) possibles parmi n items est \\(n(n-1)/2\\), soit ici pour 1262 éléments nous avons 795.691 paires.↩ La fonction dist() ne propose par exemple pas la méthode de Bray-Curtis qui est fréquemment utilisée en biologie et en écologie, contrairement à vegan::vegdist(method = &quot;bray&quot;).↩ "],
["regroupement-avec-cah.html", "5.3 Regroupement avec CAH", " 5.3 Regroupement avec CAH A partir du moment où nous avons une matrice de distances entre toutes les paires d’individus dans notre jeu de données, nous pouvons les regrouper en fonction de leur ressemblance. Deux approches radicalement différentes existent. Soit nous pouvons diviser l’échantillon progressivement jusqu’à obtenir des groupes homogènes (méthodes divisives, telle que les K-moyennes que nous aborderons dans le module suivant), soit nous pouvons regrouper les items semblables petit à petit jusqu’à avoir traité tout l’échantillon (méthodes agglomératives, telle que la classification ascendante hiérarchique étudiée ici). 5.3.1 Dendrogramme Le dendrogramme est une manière très utile de représenter graphiquement un regroupement. Il s’agit de réaliser un arbre dichotomique ressemblant un peu à un arbre phylogénétique qui est familier aux biologistes (par exemple ici) : Dans l’arbre, nous représentons des divisions dichotomiques (division d’une branche en deux) matérialisant les divergences au cours de l’évolution. Dans l’arbre présenté ici, l’axe des ordonnées est utilisé pour représenter le temps et les branchements sont placées à une hauteur correspondante en face de l’axe. Le dendrogramme est une représentation similaire de la CAH avec l’axe des ordonnées indiquant à quelle distance le rassemblement se fait. Dans R, la réalisation du dendrogramme se fait en deux étapes : Son calcul par CAH à l’aide de la fonction hclust() Sa représentation sur un graphique en utilisant plot() (pour un graphique de base) ou ggdendro::ggdendrogram() (pour un graphique ggplot2). Partons, pour étayer notre raisonnement, d’une matrice de distances euclidiennes sur les données de zooplancton. Au passage, abordons quelques fonctions de R utiles dans le contexte pour préparer correctement nos données. A la section précédente, nous avons suggéré qu’il peut être utile de standardiser nos données préalablement si une distance de type euclidienne ou manhattan est ensuite calculée et si les données numériques sont mesurées dans des unités différentes, comme c’est le cas ici. La fonction scale() se charge de cette standardisation colonne par colonne dans un tableau. Comme elle renvoie une matrice, nous devons ensuite retransformer le résultat en data.frame ou tibble. Nous choisissons ici d’utiliser la fonction as_tibble(). Limitons, pour l’instant notre ambition à la comparaison de six individus. Afin d’observer tous les cas possibles dans le dendrogramme, nous ne prendrons pas les six premières lignes du tableau, mais les lignes 13 à 18. Cela peut se faire à l’aide de la fonction slice() que nous n’avons pas encore beaucoup utilisée jusqu’ici. Cette fonction permet de spécifier explicitement les numéros de lignes à conserver, contrairement à filter() qui applique un test de condition pour décider qulles ligne(s) converser. Voici donc notre matrice de distances euclidiennes sur les données ainsi traitées. Les individus initiaux 13 à 18 sont renumérotés 1 à 6. Nous n’imprimons plus ici la matrice de distance obtenue car ce n’est que la première étape du travail vers une représentation plus utile (le dendrogramme). zoo %&gt;.% select(., -class) %&gt;.% # Elimination de la colonne class scale(.) %&gt;.% # Standardisation des 19 colonnes as_tibble(.) %&gt;.% # Conversion de la matrice en data.frame +tibble slice(., 13:18) -&gt; zoo6 # Récupération des lignes 13 à 18 zoo6 %&gt;.% vegan::vegdist(., method = &quot;euclidean&quot;) -&gt; zoo6std_dist Notre objet zoo6std_dist est ensuite utilisé pour calculer notre CAH à l’aide de hclust(). Enfin, plot() (ou ggdendro::ggdendrogram()) se charge de tracer le dendrogramme. zoo6std_dist %&gt;.% hclust(.) -&gt; zoo6std_clust # Calcul du dendrogramme plot(zoo6std_clust) # ggdendro::ggdendrogram() peut aussi être utilisé ici Voici comment interpréter ce graphique. Les deux individus les plus semblables sont le 2 et le 5 (regroupepent effectué le plus bas, donc, avec la valeur de l’indice de dissimilarité le plus faible), également tous deux similaires au 3. Le 4 se rattache à ce groupe, mais bien plus haut sur l’axe, indiquant ainsi qu’il s’en différencie un peu plus, enfin, le 1 et le 6 sont rassemblés à un niveau à peu près équivalent. Finalement, le groupe de droite constitué des individus 2, 3, 4 et 5 et celui de gauche contenant le 1 et le 6 sont reliés encore plus haut suggérant ainsi la dissimilitude la plus forte entre ces deux groupes. Attention : La position des individus selon l’axe horizontal n’est pas importante. Il faut voir le dendrogramme comme un mobile qui peut tourner librement. C’est-à-dire que le groupe constitué de 2, 3, 4 et 5 aurait très bien pu être placé à la gauche de celui constitué de 1 et 6. De même, le sous-groupe 2, 3 et 5 aurait très bien pu être à la gauche du regroupement avec 4 à la droite, etc. Notez que nous n’avons pas utilisé l’information concernant les classes taxonomiques auxquelles les individus appartiennent (nous avons éliminé la variable class en tout début d’analyse). Pour cette raison, ce type de regroupement s’appelle une classification non supervisée parce que nous n’imposons pas les groupes que nous souhaitons réaliser14. Nous pouvons néanmoins révéler ces classes maintenant pour vérifier si le dendrogramme réalisé est logique (étape bien sûr facultative et souvent pas possible en pratique lorsque cette information n’est pas disponible) : zoo$class[13:18] # [1] Egg_round Poecilostomatoid Poecilostomatoid Decapod # [5] Calanoid Appendicularian # 17 Levels: Annelid Appendicularian Calanoid Chaetognath ... Protist Les individus 2, 3 et 5 (“Poecilostomatoid” ou “Calanoid”) sont des copépodes, des crustacés particulièrement abondants dans le zooplancton. Leur forme est similaire. Leur regroupement est logique. L’individu 4 est une larve de décapode, un autre crustacé. Ainsi le regroupement de 2, 3 et 5 avec 4 correspond aux crustacés ici. Enfin, les individus 1 et 6 sont très différents puisqu’il s’agit respectivement d’un œuf rond probablement de poisson et d’un appendiculaire. 5.3.2 Séparer les groupes Si notre dendrogramme est satisfaisant (nous le déterminons en l’étudiant et en vérifiant que le regroupement obtenu a un sens biologique par rapport à l’objectif de notre étude), nous concrétisons le regroupement en coupant l’arbre à une certaine hauteur à l’aide de la fonction cutree(). Nous pouvons matérialiser ce niveau de coupure en traçant un trait horizontal rouge avec abline(h = XXX, col = &quot;red&quot;) pour le graphe de base, ou en ajoutant + geom_hline(yintercept = XXX, col = &quot;red&quot;) au graphique ggplot2, avec ‘XXX’ la hauteur de coupure souhaitée. Par exemple, si nous voulons réaliser deux groupes, nous ferons : plot(zoo6std_clust) abline(h = 8, col = &quot;red&quot;) Pour réaliser trois groupes, nous couperons plus bas (ici version ggplot2) : ggdendro::ggdendrogram(zoo6std_clust) + geom_hline(yintercept = 6.5, col = &quot;red&quot;) Et ainsi de suite. A mesure que la hauteur de coupure descend, nous réaliserons quatre, puis cinq groupes. Quel est le meilleur niveau de coupure ? Il n’y en a pas un seul forcément car les différents niveaux correspondent à un point de vue de plus en plus détaillé du regroupement. Néanmoins, lorsque le saut sur l’axe d’un regroupement à l’autre est fort, nous pouvons considérer une séparation des groupes d’autant meilleure. Cela crédibilise d’autant le regroupement choisi. Ainsi la séparation en deux groupes apparait forte (entre 10,2 et 7,4, sur un intervalle de 2,8 unités donc). De même, la séparation de l’individu 4 par rapport au groupe constitué de 2, 3 et 5 se fait sur un intervalle de 4,7 unités (entre 7,4 et 2,7). En comparaison, la séparation de l’individu 3 du groupe 2 et 5 est nettement moins nette puisqu’elle apparait aux hauteurs entre 2,7 à 2.4, soit seulement sur un intervalle de 0,3 unités. Ces mesures d’intervalles n’ont aucune valeur absolue. Il faut les considérer uniquement de manière relatives les unes par rapport aux autres, et seulement au sein d’un même dendrogramme. Ici, deux niveaux de coupure se détachent. Nous utilisons aussi la fonction rect.hclust() pour matérialiser ces groupes sur le dendrogramme, et cutree() pour obtenir une nouvelle variable que nous pourrons ajouter dans notre tableau de données qui concrétise le regroupement choisi : en deux groupes, nous avons les crustacés séparés des autres. plot(zoo6std_clust) abline(h = 8, col = &quot;red&quot;) rect.hclust(zoo6std_clust, h = 8, border = c(&quot;blue&quot;, &quot;red&quot;)) (group2 &lt;- cutree(zoo6std_clust, h = 8)) # [1] 1 2 2 2 2 1 Les individus 1 et 6 sont dans le groupe 1, et les autres dans le groupe 2. en quatre groupes, nous avons les copépodes séparés des trois autres items chacun dans un groupe séparé. plot(zoo6std_clust, hang = -1) # Prolonger les tiges jusqu&#39;en bas abline(h = 5, col = &quot;red&quot;) rect.hclust(zoo6std_clust, h = 5, border = c(&quot;blue&quot;, &quot;blue&quot;, &quot;blue&quot;, &quot;red&quot;)) Le groupe des individus 2, 3 et 5 est encadré en rouge. Les trois autres groupes des individus uniques 1, 6 et 4 respectivement sont ici encadrés en bleu. (group4 &lt;- cutree(zoo6std_clust, h = 5)) # [1] 1 2 2 3 2 4 L’individu 1 est dans le groupe 1, les individus 2, 3 et 5 sont dans le groupe 2, l’individu 4 est dans le groupe 3 et enfin l’individu 6 est dans le groupe 4. Donc, cutree() numérote ses groupes en fonction du premier item présent dedans dans l’ordre du tableau de données. Nous pouvons rajouter ces regroupements dans le tableau de données, et utiliser cette information pour en faire d’autres choses utiles. Par exemple, nous représentons un nuage de point des données et colorons nos points en fonction du premier regroupement effectué comme suit : zoo6$group2 &lt;- as.factor(group2) # Utiliser une variable facteur ici chart(zoo6, area ~ circularity %col=% group2) + geom_point() Nous voyons que les copépodes (groupe 2 en turquoise) sont plus grands (area) et moins circulaires (circularity) que les deux autres particules du groupe 1 en rouge. Nous pouvons ainsi réexplorer nos données en fonction du regroupement pour mieux le comprendre. 5.3.2.1 Méthodes de CAH Tant que l’on compare des individus isolés entre eux, il n’y a pas d’ambiguïté. Par contre, dès que nous comparons un groupe avec un individu isolé, ou deux groupes entre eux, nous avons plusieurs stratégies possible pour calculer leurs distances (argument method = de hclust()) : Liens simples (single linkages en anglais) hclust(DIST, method = &quot;single&quot;) : la distance entre les plus proches voisins au sein des groupes est utilisée. zoo6std_dist %&gt;.% hclust(., method = &quot;single&quot;) %&gt;.% plot(.) Les données tendent à être aggrégées de proche en proche en montrant une gradation d’une extrême à l’autre. Dans le cas d’une variation progressive, par exemple lors d’un passage graduel d’une situation A vers une situation B le long d’un transect, le dendrogramme obtenu à l’aide de cette méthode représentera la situation au mieux. Liens complets (complete linkages) hclust(DIST, method = &quot;complete&quot;), méthode utilisée par défault si non précisée : la distance entre les plus lointains voisins est considérée. C’est le dendrogramme que nous avons obtenu au début. Le dendrogramme a tendance à effectuer des groupes séparés plus nettement les uns des autres qu’avec les liens simples. Liens moyens (group-average) hclust(DIST, method = &quot;average&quot;) encore appelée méthode UPGMA : moyenne des liens entre toutes les paires possibles intergroupes. Nous obtenons une situation intermédiaire entre liens simples et liens moyens. zoo6std_dist %&gt;.% hclust(., method = &quot;average&quot;) %&gt;.% plot(.) Dans ce cas-ci, le résultat est similaire aux liens simples, mais ce n’est pas forcément le cas à chaque fois. Méthode de Ward hclust(DIST, method = &quot;ward.D2&quot;). Considérant le partitionnement de la variance totale du nuage de points (on parle aussi de l’inertie du nuage de points) entre variance interclasse et variance intraclasse, la méthode vise à maximiser la variance interclasse et minimiser la variance intraclasse, ce qui revient d’ailleurs au même. Cette technique fonctionne souvent très bien pour obtenir des groupes bien individualisés. zoo6std_dist %&gt;.% hclust(., method = &quot;ward.D2&quot;) %&gt;.% plot(.) Ici nous obtenos un dendrogramme très proche des liens complets. Encore une fois, ce n’est pas forcément le cas dans toutes les situations. Liens médians, centroïdes, …, constituent encore d’autre méthodes possibles, mais moins utilisées. Elles ont l’inconvénient de produire parfois des inversions dans le dendrogramme, c’est-à-dire qu’un regroupement plus avant se fait parfois à une hauteur plus basse sur l’axe des ordonnées, ce qui rend le dendrogramme peu lisible et beaucoup moins esthétique. Dans notre exemple, la méthode centroïde crée une telle inversion à la hauteur de 5 environ sur l’axe des ordonnées entre l’individu 6 et l’individu 1. Alors, qui est regroupé en premier ? Pas facile à déterminer dans ce cas ! zoo6std_dist %&gt;.% hclust(., method = &quot;centroid&quot;) %&gt;.% plot(.) 5.3.3 Étude complète Voici ce que cela donne si nous effectuons une CAH sur le jeu zooplancton complet avec ses 1262 lignes. zoo %&gt;.% select(., -class) %&gt;.% # Elimination de la colonne class scale(.) %&gt;.% # Standardisation des 19 colonnes as_tibble(.) %&gt;.% # Transformation en data.frame + tibble vegan::vegdist(., method = &quot;euclidean&quot;) %&gt;.% # Matrice de distances hclust(., method = &quot;ward.D2&quot;) -&gt; zoo_clust # CAH avec Ward plot(zoo_clust, labels = FALSE) # Dendrogramme sans labels des individus abline(h = 70, col = &quot;red&quot;) # Séparation en 3 groupes Naturellement, avec 1262 branches, notre dendrogramme est très encombré ! Cependant, il reste analysable tant que nous nous intéressons aux regroupements de plus haut niveau (vers le haut du dendrogramme). Notez comme les vlaeurs sur l’axe des ordonnées ont changé par rapport à nos cas simple à six items (ne jamais comparer les hauteurs entre dendrogrammes différents). Notre CAH est terminée, mais nous pouvons matérialiser le regroupement sous forme d’une variable supplémentaire dans le tableau et l’utiliser ensuite. zoo_clust %&gt;.% cutree(., h = 70) %&gt;.% # Matérialisation des groupes as.factor(.) -&gt; # Conversion en variable facteur zoo$Groupes # Ajout au tableau comme variable Groupes chart(zoo, compactness ~ ecd %col=% Groupes) + geom_point() + # Exploration visuelle des groupes (exemple) coord_trans(x = &quot;log10&quot;, y = &quot;log10&quot;) # Axes en log10 Et de manière optionnelle, si un classement est disponible par ailleurs (c’est le cas ici avec la variable class), nous pouvons réaliser un tableau de contingence à double entrées entre le regroupement obtenu et le classement pour comparaison. table(zoo$class, zoo$Groupes) # # 1 2 3 # Annelid 31 13 6 # Appendicularian 0 36 0 # Calanoid 9 237 42 # Chaetognath 0 17 34 # Cirriped 1 21 0 # Cladoceran 47 3 0 # Cnidarian 3 5 14 # Cyclopoid 0 50 0 # Decapod 121 5 0 # Egg_elongated 2 48 0 # Egg_round 44 0 5 # Fish 4 46 0 # Gastropod 48 2 0 # Harpacticoid 0 39 0 # Malacostracan 27 66 28 # Poecilostomatoid 20 136 2 # Protist 0 50 0 Ainsi, nous pouvons constater que le groupe 1 contient une fraction importante des annélides, des cladocères,des décapodes, des œufs ronds et des gastéropodes. Le groupe 2 contient un maximum des copépodes représentés par les calanoïdes, les cyclopoïdes, les harpacticoïdes et les poecilostomatoïdes. Il contient aussi tous les appendiculaires, tous les protistes, presque tous les œufs allongés, les poissons, et une majorité des malacostracés. Enfin, le groupe 3 contient une majorité des chaetognathes et des cnidaires. Le regroupement a été réalisé uniquement en fonction de mesures effectuées sur les images. Il n’est pas parfait, mais des tendances se dégagent tout de même. Pour en savoir plus La vidéo suivante présente la matière que nous venons d’étudier de manière légèrement différente. Plus d’explications sont également apportées concernant la méthode de Ward. Une autre explication encore ici. Pour bien comprendre la façon dont une CAH est réalisée, il est utile de détailler le calcul étape par étape sur un exemple simple. Voici une matrice de distances euclidiennes fictive entre 6 stations  : A B C D E B 15 C 6.4 10.86 D 5.2 13.04 5.48 E 5.1 12.37 7.28 7.81 F 10.39 7.42 5.57 9.64 9.49 Effectuons une CAH manuellement par liens complets et traçons le dendrogramme correspondant. Etape 1 : Nous repérons dans la matrice de distance la paire qui a l’indice de dissimilarité le plus petit et effectuons un premier regroupement. A_E = groupe I à la distance 5,1. La matrice de distances est simplifiées par rapport à ce groupe I en considérant la règle utilisée (ici, liens complets, donc on garde la plus grande distance entre toutes les paires possibles lorsqu’il y a des groupes). * Distance entre B et I = B_A = 15 * Distance entre C et I = C_E = 7,28 * Distance entre D et I = D_E = 7,81 * Distance entre F et I = F_A = 10,39 Matrice de distance recalculée : I B C D B 15.00 C 7.28 10.86 D 7.81 13.04 5.48 F 10.39 7.42 5.57 9.64 Etape 2 : on répète le processus. C_D = groupe II à la distance 5,48. * Distance entre I et II = I_D = 7,81 * Distance entre B et II = B_D = 13,04 * Distance entre F et II = F_D = 9,64 Matrice de distance recalculée : I B II B 15.00 II 7.81 13.04 F 10.39 7.42 9.64 Etape 3 : B_F = groupe III à la distance 7,42 * Distance entre I et III = I_B = 15 * Distance entre II et III = II_B = 13,04 Matrice de distance recalculée  : I III III 15.00 II 7.81 13.04 Etape 4 : I_II = groupe IV à la distance 7,81 * Distance entre III et IV = III_I = 15 Matrice de distance recalculée : III IV 15 Etape 5 : III - IV = groupe V à la distance 15. fini ! Voici le dendrogramme résultant : Les techniques complémentaires de classification supervisées, recommandées dans le cas du jeu de données zooplancton seront abordées dans le cours de Science des Données Biologiques III l’an prochain.↩ "],
["k-moyenne-mds-som.html", "Module 6 K-moyenne, MDS &amp; SOM", " Module 6 K-moyenne, MDS &amp; SOM Objectifs Maîtriser la technique de classification par les k-moyennes comme alternative à la CAH pour les gros jeux de données. Comprendre la représentation d’une matrice de distances sur un carte (ordination) et la réduction de dimensions via le positionnement multidimensionnel MDS. Être capable de créer des cartes auto-adaptatives ou SOM, de les interpréter et de les utiliser comme autre technique de classification. Prérequis Ces techniques étant basées sur des matrices de distances et complémentaires à la classification ascendante hiérarchique, le module 5 doit être assimilé avant de s’attaquer au présent module. "],
["k-moyennes.html", "6.1 K-moyennes", " 6.1 K-moyennes Les k-moyennes (ou “k-means” en anglais) représentent une autre façon de regrouper les individus d’un tableau multivarié. Par rapport à la CAH, cette technique est généralement moins efficace, mais elle a l’avantage de permettre le regroupement d’un très grand nombre d’individus (gros jeu de données), là où la CAH nécessiterait trop de temps de calcul et de mémoire vive. Il est donc utile de connaitre cette seconde technique à utiliser comme solution de secours lorsque le dendrogramme de la CAH devient illisible sur de très gros jeux de données. Le principe des k-moyennes est très simple15 : L’utilisateur choisi le nombre de groupes k qu’il veut obtenir à l’avance. La position des k centres est choisie au hasard au début. Les individus sont attribués aux k groupes en fonction de leurs distances aux centres (attribution au groupe de centre le plus proche). Les k centres sont replacés au centre de gravité des groupes ainsi obtenus. Les individus sont réaffectés en fonction de leurs distances à ces nouveaux centres. Si au moins un individu a changé de groupe, le calcul est réitéré. Sinon, nous considérons avoir atteint la configuration finale. La technique est superbement expliquée et illustrée dans la vidéo suivante : Essayez par vous même via l’application ci-dessous qui utilise le célèbre jeu de données iris. Notez que vous devez utiliser des variables numériques. Par exemple, Species étant une variable qualitative, vous verrez que cela ne fonctionne pas dans ce cas. 6.1.1 Exemple simple Afin de comparer la classification par k-moyennes à celle par CAH, nous reprendrons ici le même jeu de données zooplankton. zoo &lt;- read(&quot;zooplankton&quot;, package = &quot;data.io&quot;) zoo # # A tibble: 1,262 x 20 # ecd area perimeter feret major minor mean mode min max std_dev # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 0.770 0.465 4.45 1.32 1.16 0.509 0.363 0.036 0.004 0.908 0.231 # 2 0.700 0.385 2.32 0.728 0.713 0.688 0.361 0.492 0.024 0.676 0.183 # 3 0.815 0.521 4.15 1.33 1.11 0.598 0.308 0.032 0.008 0.696 0.204 # 4 0.785 0.484 4.44 1.78 1.56 0.394 0.332 0.036 0.004 0.728 0.218 # 5 0.361 0.103 1.71 0.739 0.694 0.188 0.153 0.016 0.008 0.452 0.110 # 6 0.832 0.544 5.27 1.66 1.36 0.511 0.371 0.02 0.004 0.844 0.268 # 7 1.23 1.20 15.7 3.92 1.37 1.11 0.217 0.012 0.004 0.784 0.214 # 8 0.620 0.302 3.98 1.19 1.04 0.370 0.316 0.012 0.004 0.756 0.246 # 9 1.19 1.12 15.3 3.85 1.34 1.06 0.176 0.012 0.004 0.728 0.172 # 10 1.04 0.856 7.60 1.89 1.66 0.656 0.404 0.044 0.004 0.88 0.264 # # … with 1,252 more rows, and 9 more variables: range &lt;dbl&gt;, size &lt;dbl&gt;, # # aspect &lt;dbl&gt;, elongation &lt;dbl&gt;, compactness &lt;dbl&gt;, transparency &lt;dbl&gt;, # # circularity &lt;dbl&gt;, density &lt;dbl&gt;, class &lt;fct&gt; Commençons par l’exemple simplissime de la réalisation de deux groupes à partir de six individus issus de ce jeu de données, comme nous l’avons fait avec la CAH : zoo %&gt;.% select(., -class) %&gt;.% # Elimination de la colonne class slice(., 13:18) -&gt; zoo6 # Récupération des lignes 13 à 18 zoo6_kmeans &lt;- kmeans(zoo6, centers = 2) zoo6_kmeans # K-means clustering with 2 clusters of sizes 3, 3 # # Cluster means: # ecd area perimeter feret major minor mean # 1 1.1926500 1.1279667 10.346667 2.201133 1.677067 0.8596333 0.3217333 # 2 0.6292647 0.3188667 3.224133 1.159200 1.096433 0.4023333 0.1871667 # mode min max std_dev range size aspect # 1 0.3533333 0.00400000 0.8986667 0.2620000 0.8946667 1.2683500 0.5149422 # 2 0.1026667 0.01066667 0.5400000 0.1166667 0.5293333 0.7493833 0.4753843 # elongation compactness transparency circularity density # 1 23.046713 7.987806 0.06173831 0.1357000 0.37630000 # 2 6.333315 2.727708 0.14732060 0.4900333 0.06943333 # # Clustering vector: # [1] 2 1 2 1 1 2 # # Within cluster sum of squares by cluster: # [1] 200.03837 54.18647 # (between_SS / total_SS = 68.1 %) # # Available components: # # [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; # [5] &quot;tot.withinss&quot; &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; # [9] &quot;ifault&quot; Nous voyons que la fonction kmeans() effectue notre classification. Nous lui fournissons le tableau de départ et spécifions le nombre k de groupes souhaités via l’argument centers =. Ne pas oublier d’assigner le résultat du calcul à une nouvelle variable, ici zoo6_kmeans, pour pouvoir l’inspecter et l’utiliser par la suite. L’impression du contenu de l’objet nous donne plein d’information dont : le nombre d’individus dans chaque groupe (ici 3 et 3), la position des centres pour les k groupes dans Cluster means, l’appartenance aux groupes dans Cluster vectors (dans le même ordre que les lignes du tableau de départ), la sommes des carrés des distances entre les individus et la moyenne au sein de chaque groupe dans Within cluster sum of squares ; le calcul between_SS / total_SS est à mettre en parallèle avec le \\(R^2\\) de la régression linéaire : c’est une mesure de la qualité de regroupement des données (plus la valeur est proche de 100% mieux c’est, mais attention que cette valeur augmente d’office en même temps que k), et enfin, la liste des composants accessibles via l’opérateur $ ; par exemple, pour obtenir les groupes (opération similaire à cutree() pour la CAH), nous ferons : zoo6_kmeans$cluster # [1] 2 1 2 1 1 2 Le package broom contient trois fonctions complémentaires qui nous seront utiles : tidy(), augment() et glance(). broom::glance() retourne un data.frame avec les statistiques permettant d’évaluer la qualité de la classification obtenue : broom::glance(zoo6_kmeans) # # A tibble: 1 x 4 # totss tot.withinss betweenss iter # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; # 1 796. 254. 542. 1 De plus, le package factoextra propose une fonction fviz_nbclust() qui réalise un graphique pour aider au choix optimal de k : factoextra::fviz_nbclust(zoo6, kmeans, method = &quot;wss&quot;, k.max = 5) Le graphique obtenu montre la décroissance de la somme des carrés des distances intra-groupes en fonction de k. Avec k = 1, nous considérons toutes les données dans leur ensemble et nous avons simplement la somme des carrés des distances euclidiennes entre tous les individus et le centre de gravité du nuage de points dont les coordonnées sont les moyennes de chaque variable. C’est le point de départ qui nous indique de combien les données sont dispersées (la valeur absolue de ce nombre n’est pas importante). Ensuite, avec k croissant, notre objectif est de faire des regroupement qui diminuent la variance intra-groupe autant que possible, ce que nous notons par la diminution de la somme des carrés intra-groupes (la variance du groupe est, en effet, la somme des carrés des distances enclidiennes entre les points et le centre du groupe, divisée par les degrés de liberté). Nous recherchons ici des sauts importants dans la décroissance de la somme des carrés, tout comme dans le dendrogramme obtenu par la CAH nous recherchions des sauts importants dans les regroupements (hauteur des barres verticales du dendrogramme). Nous observons ici un saut important pour k = 2, puis une diminution moins forte de k = 3 à k = 5. Ceci suggère que nous pourrions considérer deux groupes. Le nombre de groupes proposé par factoextra::fviz_nbclust() n’est qu’indicatif ! Si vous avez par ailleurs d’autres informations qui vous suggèrent un regroupement différent, ou si vous voulez essayer un regroupement plus ou moins détaillé par rapport à ce qui est proposé, c’est tout aussi correct. La fonction factoextra::fviz_nbclust() propose d’ailleurs deux autres méthodes pour déterminer le nombre optimal de groupes k, avec method = &quot;silhouette&quot; ou method = &quot;gap_stat&quot;. Voyez l’aide en ligne de cette fonction ?factoextra::fviz_nbclust. Ces différentes méthodes peuvent d’ailleurs suggérer des regroupements différents pour les mêmes données… preuve qu’il n’y a pas une et une seule solution optimale ! A ce stade, nous pouvons collecter les groupes et les ajouter à notre tableau de données. Pour la CAH, vous avez déjà remarqué que rajouter ces groupes dans le tableau de départ peut mener à des effets surprenants si nous relançons ensuite l’analyse sur le tableau ainsi complété16. Donc, nous prendrons soin de placer les données ainsi complétées de la colonne cluster dans un tableau différent nommé zoo6b. Pour se faire, nous pouvons utiliser broom::augment(). broom::augment(zoo6_kmeans, zoo6) %&gt;.% rename(., cluster = .cluster) -&gt; zoo6b names(zoo6b) # [1] &quot;ecd&quot; &quot;area&quot; &quot;perimeter&quot; &quot;feret&quot; # [5] &quot;major&quot; &quot;minor&quot; &quot;mean&quot; &quot;mode&quot; # [9] &quot;min&quot; &quot;max&quot; &quot;std_dev&quot; &quot;range&quot; # [13] &quot;size&quot; &quot;aspect&quot; &quot;elongation&quot; &quot;compactness&quot; # [17] &quot;transparency&quot; &quot;circularity&quot; &quot;density&quot; &quot;cluster&quot; Comme vous pouvez le constater, une nouvelle colonne nommée .cluster a été ajoutée au tableau en dernière position, que nous avons renommée immédiatement en cluster ensuite (c’est important pour le graphique plus loin). Elle contient ceci : zoo6b$cluster # [1] 2 1 2 1 1 2 # Levels: 1 2 C’est le contenu de zoo6_kmeans$cluster, mais transformé en variable factor. class(zoo6b$cluster) # [1] &quot;factor&quot; Nous pouvons enfin utiliser broom::tidy() pour obtenir un tableau avec les coordonnées des k centres. Nous l’enregistrerons dans la variable zoo6_centers, en ayant bien pris soin de nommer les variables du même nom que dans le tableau original zoo6 (argument col.names = names(zoo6), cela sera important pour le graphique ci-dessous) : zoo6_centers &lt;- broom::tidy(zoo6_kmeans, col.names = names(zoo6)) zoo6_centers # # A tibble: 2 x 21 # ecd area perimeter feret major minor mean mode min max std_dev # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 1.19 1.13 10.3 2.20 1.68 0.860 0.322 0.353 0.004 0.899 0.262 # 2 0.629 0.319 3.22 1.16 1.10 0.402 0.187 0.103 0.0107 0.540 0.117 # # … with 10 more variables: range &lt;dbl&gt;, size &lt;int&gt;, aspect &lt;dbl&gt;, # # elongation &lt;dbl&gt;, compactness &lt;dbl&gt;, transparency &lt;dbl&gt;, # # circularity &lt;dbl&gt;, density &lt;dbl&gt;, withinss &lt;dbl&gt;, cluster &lt;fct&gt; La dernière colonne de ce tableau est également nommée cluster. C’est le lien entre le tableau zoo6b augmenté et zoo6_centers. Nous avons maintenant tout ce qu’il faut pour représenter graphiquement les regroupements effectués par les k-moyennes en colorant les points en fonction de la nouvelle variable cluster. chart(data = zoo6b, area ~ circularity %col=% cluster) + geom_point() + # Affiche les points représentant les individus geom_point(data = zoo6_centers, size = 5, shape = 17) # Ajoute les centres Comparez avec le graphique équivalent au module précédent consacré à la CAH. Outre que l’ordre des groupes est inversé et que les données n’ont pas été standardisées ici, un point est classé dans un groupe différent par les deux méthodes. Il s’agit du point ayant environ 0.25 de circularité et 0.5 de surface. Comme nous connaissons par ailleurs la classe à laquelle appartient chaque individu, nous pouvons la récupérer comme colonne supplémentaire du tableau zoo6b et ajouter cette information sur notre graphique. zoo6b$class &lt;- zoo$class[13:18] zoo6_centers$class &lt;- &quot;&quot; # Ceci est nécessaire pour éviter le label des centres chart(data = zoo6b, area ~ circularity %col=% cluster %label=% class) + geom_point() + ggrepel::geom_text_repel() + # Ajoute les labels intelligemment geom_point(data = zoo6_centers, size = 5, shape = 17) Nous constatons que le point classé différemment est un “Poecilostomatoïd”. Or, l’autre groupe des k-moyennes contient aussi un individu de la même classe. Donc, CAH a mieux classé notre plancton que les k-moyennes dans le cas présent. Ce n’est pas forcément toujours le cas, mais souvent. Un dernier point est important à mentionner. Comme les k-moyennes partent d’une position aléatoire des k centres, le résultat final peut varier et n’est pas forcément optimal. Pour éviter cela, nous pouvons indiquer à kmeans() d’essayer différentes situations de départ via l’argument nstart =. Par défaut, nous prenons une seule situation aléatoire de départ nstart = 1, mais en indiquant une valeur plus élevée pour cet argument, il est possible d’essayer plusieurs situations de départ et ne garder que le meilleur résultat final. Cela donne une analyse plus robuste et plus reproductible… mais le calcul est naturellement plus long. kmeans(zoo6, centers = 2, nstart = 50) # 50 positions de départ différentes # K-means clustering with 2 clusters of sizes 3, 3 # # Cluster means: # ecd area perimeter feret major minor mean # 1 0.6292647 0.3188667 3.224133 1.159200 1.096433 0.4023333 0.1871667 # 2 1.1926500 1.1279667 10.346667 2.201133 1.677067 0.8596333 0.3217333 # mode min max std_dev range size aspect # 1 0.1026667 0.01066667 0.5400000 0.1166667 0.5293333 0.7493833 0.4753843 # 2 0.3533333 0.00400000 0.8986667 0.2620000 0.8946667 1.2683500 0.5149422 # elongation compactness transparency circularity density # 1 6.333315 2.727708 0.14732060 0.4900333 0.06943333 # 2 23.046713 7.987806 0.06173831 0.1357000 0.37630000 # # Clustering vector: # [1] 1 2 1 2 2 1 # # Within cluster sum of squares by cluster: # [1] 54.18647 200.03837 # (between_SS / total_SS = 68.1 %) # # Available components: # # [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; # [5] &quot;tot.withinss&quot; &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; # [9] &quot;ifault&quot; Dans ce cas simple, cela ne change pas grand chose. Mais avec un plus gros jeu de données plus complexe, cela peut être important. 6.1.2 Classification du zooplancton Maintenant que nous savons utiliser kmeans() et les fonctions annexes, nous pouvons classer le jeu de données zoo tout entier. zoo %&gt;.% select(., -class) %&gt;.% factoextra::fviz_nbclust(., kmeans, method = &quot;wss&quot;, k.max = 10) Nous observons un saut maximal pour k = 2, mais le saut pour k = 3 est encore conséquent. Afin de comparer avec ce que nous avons fait par CAH, nous utiliserons donc k = 3. Enfin, comme un facteur aléatoire intervient, qui définira au final le numéro des groupes, nous utilisons set.seed() pour rendre l’analyse reproductible. Pensez à donner une valeur différente à cette fonction pour chaque utilisation ! Et pensez aussi à éliminer les colonnes non numériques à l’aide de select(). set.seed(562) zoo_kmeans &lt;- kmeans(select(zoo, -class), centers = 3, nstart = 50) zoo_kmeans # K-means clustering with 3 clusters of sizes 786, 91, 385 # # Cluster means: # ecd area perimeter feret major minor mean # 1 0.6664955 0.431915 3.575374 1.134705 0.9744768 0.4780780 0.2388065 # 2 1.3774670 1.998097 19.653860 4.063837 2.1465758 0.9602846 0.1488495 # 3 0.9715857 1.009902 9.197299 2.668022 1.8468984 0.6194652 0.1723774 # mode min max std_dev range size aspect # 1 0.09256997 0.007094148 0.7269109 0.1842660 0.7198168 0.7262774 0.5372808 # 2 0.02470330 0.004000000 0.7013187 0.1472286 0.6973187 1.5534302 0.5362249 # 3 0.04455065 0.004207792 0.6315844 0.1512922 0.6273766 1.2331818 0.5349924 # elongation compactness transparency circularity density # 1 7.184451 3.002093 0.07385014 0.42917214 0.09349338 # 2 61.837019 20.325398 0.09737903 0.05186813 0.31140879 # 3 27.898079 9.529156 0.11719954 0.11197351 0.16938468 # # Clustering vector: # [1] 1 1 1 1 1 1 2 1 2 1 1 3 1 3 1 3 3 1 2 1 1 1 1 3 3 1 1 3 1 1 1 1 1 1 # [35] 3 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 3 1 1 3 1 3 3 1 # [69] 3 1 1 1 1 2 1 1 1 1 1 3 1 1 1 1 1 2 1 1 3 1 1 3 1 2 1 1 1 1 1 1 1 1 # [103] 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 # [137] 1 1 1 1 1 3 1 1 2 3 2 3 1 1 1 1 1 3 3 3 3 3 1 2 1 1 1 1 1 1 1 3 1 1 # [171] 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 # [205] 1 3 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 # [239] 1 1 1 1 1 3 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 3 # [273] 3 2 1 1 3 1 3 3 1 3 1 1 2 1 1 2 3 2 3 3 1 1 1 1 3 3 2 1 3 1 3 3 1 3 # [307] 3 3 2 1 3 3 2 1 3 2 2 1 2 3 3 3 1 1 3 1 1 3 1 1 1 1 1 3 1 2 1 1 1 3 # [341] 1 1 1 1 1 1 3 3 3 1 3 3 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 3 1 3 1 1 1 1 # [375] 3 1 1 1 1 1 1 3 3 1 1 1 1 1 3 3 1 1 3 1 3 3 1 3 3 1 1 1 1 1 1 1 1 3 # [409] 1 1 1 3 3 1 1 1 1 1 1 1 3 3 3 2 1 3 3 1 1 1 3 3 3 1 1 1 1 2 1 1 3 3 # [443] 2 1 3 1 3 1 3 1 3 1 1 3 1 3 1 3 3 1 1 1 1 1 3 1 1 3 1 1 1 3 1 1 3 1 # [477] 1 1 3 1 1 1 1 1 1 1 1 3 1 3 3 3 1 3 1 3 1 3 3 3 1 1 1 1 1 1 1 1 3 3 # [511] 1 3 2 3 1 2 1 1 3 3 3 1 3 1 1 1 2 1 2 2 3 1 3 1 1 1 3 1 1 1 1 3 3 1 # [545] 1 1 3 3 1 1 3 1 3 1 1 3 2 3 1 1 1 1 2 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 # [579] 1 2 3 3 3 2 1 2 3 3 2 1 3 3 3 1 1 3 3 2 1 1 1 1 1 1 1 1 1 3 1 3 3 1 # [613] 1 1 1 1 1 1 2 1 1 1 1 1 3 3 2 3 1 3 2 3 1 3 1 3 3 1 1 1 1 3 3 3 3 3 # [647] 3 3 1 1 3 1 1 1 1 1 1 1 3 3 3 3 3 3 1 2 3 3 3 3 1 1 3 3 1 1 1 1 3 3 # [681] 3 1 1 1 1 1 1 3 1 1 1 3 1 1 3 3 3 3 3 1 1 2 1 1 2 3 1 3 1 1 3 2 2 3 # [715] 1 1 2 3 1 2 3 3 3 2 3 1 3 2 1 3 1 1 2 3 1 2 1 2 3 3 3 1 2 1 3 3 2 3 # [749] 3 1 1 3 1 3 3 1 1 1 1 1 3 3 3 1 1 1 1 3 3 3 1 1 1 1 3 1 1 1 3 1 1 3 # [783] 1 3 1 1 1 3 1 1 3 1 1 3 1 1 3 1 1 1 3 1 3 2 3 1 1 3 3 2 1 3 1 1 3 3 # [817] 3 1 1 1 3 1 1 1 1 1 1 1 1 1 3 1 3 3 1 1 1 3 1 3 3 1 3 1 1 1 1 1 1 1 # [851] 3 3 3 1 1 3 3 3 1 1 1 1 1 1 3 1 3 2 3 1 3 1 3 1 1 2 1 2 1 3 3 3 3 3 # [885] 3 3 1 1 3 1 1 1 3 2 1 3 1 1 1 1 3 1 3 1 1 1 3 3 1 3 1 1 3 1 1 3 1 2 # [919] 1 3 3 1 1 3 3 1 1 1 2 3 2 3 2 3 3 3 2 1 3 2 3 3 3 1 3 3 3 3 3 3 2 3 # [953] 3 1 3 1 1 1 1 1 1 3 1 1 1 1 1 1 3 1 1 2 1 2 1 1 1 1 3 3 3 3 1 1 1 2 # [987] 2 1 3 3 1 2 1 3 3 2 3 3 3 3 1 3 2 3 2 3 2 1 3 1 1 1 1 3 3 1 3 1 2 3 # [1021] 1 1 3 3 3 3 3 1 3 3 3 1 1 3 3 1 2 3 3 3 2 1 3 2 1 3 3 2 1 1 3 1 1 1 # [1055] 3 1 1 1 1 1 1 1 1 3 3 3 3 3 2 3 3 3 3 1 3 3 3 1 3 2 1 3 3 3 2 3 1 1 # [1089] 1 1 1 2 1 3 3 1 2 2 3 1 1 3 3 1 1 2 3 3 3 3 3 1 1 1 2 1 1 1 1 1 1 1 # [1123] 3 1 1 1 3 1 3 3 1 1 3 1 1 1 1 1 1 3 3 1 3 3 3 3 2 1 1 1 1 3 1 1 1 1 # [1157] 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 1 1 3 3 1 1 1 3 1 1 1 1 3 1 1 1 1 1 # [1191] 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 # [1225] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 # [1259] 3 3 3 3 # # Within cluster sum of squares by cluster: # [1] 24356.96 43792.25 39813.31 # (between_SS / total_SS = 77.0 %) # # Available components: # # [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; # [5] &quot;tot.withinss&quot; &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; # [9] &quot;ifault&quot; Récupérons les clusters dans zoob broom::augment(zoo_kmeans, zoo) %&gt;.% rename(., cluster = .cluster) -&gt; zoob Et enfin, effectuons un graphique similaire à celui réalisé pour la CAH au module précédent. À noter que nous pouvons ici choisir n’importe quelle paire de variables quantitatives pour représenter le nuage de points. Nous ajoutons des ellipses pour matérialiser les groupes à l’aide de stat_ellipse(). Elles contiennent 95% des points du groupe à l’exclusion des extrêmes. Enfin, comme il y a beaucoup de points, nous choisissons de les rendre semi-transparents avec l’argument alpha = 0.2 pour plus de lisibilité du graphique. chart(data = zoob, compactness ~ ecd %col=% cluster) + geom_point(alpha = 0.2) + stat_ellipse() + geom_point(data = broom::tidy(zoo_kmeans, col.names = names(zoo6)), size = 5, shape = 17) Nous observons ici un regroupement beaucoup plus simple qu’avec la CAH, essentiellement stratifié de bas en haut en fonction de la compacité des points (Compactness). La tabulation des clusters en fonction des classes connues par ailleurs montre aussi que les k-moyennes les séparent moins bien que ce qu’a pu faire la CAH : table(zoob$class, zoob$cluster) # # 1 2 3 # Annelid 38 6 6 # Appendicularian 21 0 15 # Calanoid 82 41 165 # Chaetognath 6 0 45 # Cirriped 14 0 8 # Cladoceran 50 0 0 # Cnidarian 13 3 6 # Cyclopoid 5 5 40 # Decapod 117 0 9 # Egg_elongated 50 0 0 # Egg_round 49 0 0 # Fish 50 0 0 # Gastropod 50 0 0 # Harpacticoid 1 9 29 # Malacostracan 54 26 41 # Poecilostomatoid 143 1 14 # Protist 43 0 7 Le cluster numéro 2 n’est pas vraiment défini en terme des classes de plancton car aucune classe ne s’y trouve de manière majoritaire. Le groupe numéro 1 contient la majorité des items de diverses classes, alors que le groupe 3 a une majorité de calanoïdes et d’harpacticoïdes (différents copépodes). Globalement, le classement a un sens, mais est moins bien corrélé avec les classes de plancton que ce que la CAH nous a fourni. Notez que, si nous avions standardisé les données avant d’effectuer les k-moyennes comme nous l’avons fait pour la CAH, nous aurions obtenu d’autres résultats. La transformation des variables préalablement à l’analyse reste une approche intéressante pour moduler l’importance des différentes variables entre elles dans leur impact sur le calcul des distances, et donc, des regroupements réalisés. Nous vous laissons réaliser les k-moyennes sur les données zoo standardisées à l'aide de la fonctionscale()` comme pour la CAH comme exercice. A vous de jouer ! Réalisez le tutoriel afin de vérifier votre bonne compréhension de la méthode des k-moyennes. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant les bases de R : BioDataScience2::run(&quot;06a_kmeans&quot;) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel dans la console R. Complétez votre carnet de note par binôme sur le transect entre Nice et Calvi débuté lors du module 5. Lisez attentivement le README (Ce dernier a été mis à jour). Completez votre projet. Lisez attentivement le README. La dernière version du README est disponible via le lien suivant : https://github.com/BioDataScience-Course/spatial_distribution_zooplankton_ligurian_sea Pour en savoir plus Il existe une approche mixte qui mèle la CAH et les k-moyennes. Cette approche est intéressante pour les gros jeux de données. Le problématique est expliquée ici, et l’implémentation dans la fonction factoextra::hkmeans() est détaillée ici (en anglais). Cet article explique dans le détail kmeans() et hclust() dans R, et montre aussi comment on peut calculer les k-moyennes à la main pour bien en comprendre la logique (en anglais). En pratique, différents algorithmes avec diverses optimisations existent. Le plus récent et le plus sophistiqué est celui de Hartigan-Wong. Il est utilisé par défaut par la fonction kmeans(). En pratique, il y a peu de raison d’en changer.↩ Nous vous avons proposé exprès de rajouter les groupes dans le tableau de départ pour que vous soyez confronté à ce problème. Ici, nous proposons donc une autre façon de travailler qui l’évite en assignant le résultat dans une autre variable.↩ "],
["positionnement-multidimensionnel-mds.html", "6.2 Positionnement multidimensionnel (MDS)", " 6.2 Positionnement multidimensionnel (MDS) Le positionnement multidimensionnel, ou “multidimensional scaling” en anglais, d’où son acronyme fréquemment utilisé en français également : le MDS, est une autre façon de représenter clairement l’information contenue dans une matrice de distances. Ici, l’objectif n’est pas de regrouper ou de classifier les individus du tableau, mais de les ordonner sur un graphique en nuage de points en deux ou trois dimensions. Ce graphique s’appelle une “carte”, et la technique qui la réalise est une méthode d’ordination. Au départ, nous avons p colonnes et n lignes dans le tableau cas par variables, c’est-à-dire, p variables quantitatives mesurées sur n individus distincts. Nous voulons déterminer les similitudes ou différences de ces n individus en les visualisant sur une carte où la distance d’un individu à l’autre représente cette similitude. Plus deux individus sont proches, plus ils sont semblables. Plus les individus sont éloignés, plus ils diffèrent. Ces distances entre paires d’individus, nous les avons déjà calculées dans la matrice de distances. Mais comment les représenter ? En effet, une représentation exacte ne peut se faire que dans un espace à p dimensions (même nombre de dimensions que de variables initiales). Donc, afin de réduire les dimensions à seulement 2 ou 3, nous allons devoir “tordre” les données et accepter de perdre un peu d’information. Ce que nous allons faire avec la MDS correspond exactement à cela : nous allons littéralement “écraser” les données dans un plan (deux dimensions) ou dans un espace à trois dimensions. C’est donc ce qu’on appelle une technique de réduction de dimensions. Il existe, en réalité, plusieurs techniques de MDS. Elle répondent toutes au schéma suivant : A partir d’un tableau multivarié de n lignes et p colonnes, nous calculons une matrice de distances (le choix de la transformation initiale éventuelle et de la métrique de distance utilisée sont totalement libres ici17). Nous souhaitons représenter une carte (nuage de points) à k dimensions (k = 2, éventuellement k = 3) où les n individus seront placés de telle façon que les proximités exprimées par des valeurs faibles dans la matrice de dissimilarité soient respectées autant que possible entre tous les points. Pour y arriver les points sont placés successivement sur la carte et réajustés afin de minimiser une fonction de coût, encore appelée fonction de stress qui quantifie de combien nous avons dû “tordre” le réseau à p dimensions initial représentant les distances entre toutes les paires. C’est en adoptant différentes fonctions de stress que nous aboutissons aux différentes variantes de MDS. La fonction de stress est représentée graphiquement (voir ci-dessous) pour diagnostiquer le traitement réaliser et décider si la représentation est utilisable (pas trop tordue) ou non. Le positionnement des points faisant intervenir un facteur aléatoire (choix des points à placer en premier, réorganisation ensuite pour minimiser la fonction de stress), le résutat final peut varier d’une fois à l’autre sur les mêmes données, voir ne pas converger vers une solution stable. Il faut en être conscient. Nous vous épargnons ici les développements mathématiques qui mènent à la définition de la fonction de stress. Nous nous concentrerons sur les principales techniques et sur leurs propriétés utiles en pratique. 6.2.1 MDS simplifiée sous SciViews::R Dans R, il existe plus d’une dizaine de fonctions différentes pour réaliser le MDS. Afin de vous simplifier le travail et de pouvoir traiter votre MDS comme d’autres analyses similaires nous vous propoons les fonctions supplémentaites suivante. Ces fonctions sont à copier-coller en haut de vos scripts R, ou dans un chunk de “setup” à l’intérieur de vos documents R Markdown/Notebook. SciViews::R() library(broom) # function mds for several multidimensionnal scaling functions ------ mds &lt;- function(dist, k = 2, type = c(&quot;metric&quot;, &quot;nonmetric&quot;, &quot;cmdscale&quot;, &quot;wcmdscale&quot;, &quot;sammon&quot;, &quot;isoMDS&quot;, &quot;monoMDS&quot;, &quot;metaMDS&quot;), p = 2, ...) { type &lt;- match.arg(type) res &lt;- switch(type, metric = , wcmdscale = structure(vegan::wcmdscale(d = dist, k = k, eig = TRUE, ...), class = c(&quot;wcmdscale&quot;, &quot;mds&quot;, &quot;list&quot;)), cmdscale = structure(stats::cmdscale(d = dist, k = k, eig = TRUE, ...), class = c(&quot;cmdscale&quot;, &quot;mds&quot;, &quot;list&quot;)), nonmetric = , metaMDS = structure(vegan::metaMDS(comm = dist, k = k, ...), class = c(&quot;metaMDS&quot;, &quot;monoMDS&quot;, &quot;mds&quot;, &quot;list&quot;)), isoMDS = structure(MASS::isoMDS(d = dist, k = k, ...), class = c(&quot;isoMDS&quot;, &quot;mds&quot;, &quot;list&quot;)), monoMDS = structure(vegan::monoMDS(dist = dist, k = k, ...), class = c(&quot;monoMDS&quot;, &quot;mds&quot;, &quot;list&quot;)), sammon = structure(MASS::sammon(d = dist, k = k, ...), class = c(&quot;sammon&quot;, &quot;mds&quot;, &quot;list&quot;)), stop(&quot;Unknown &#39;mds&#39; type &quot;, type) ) # For non-metric MDS, we add also data required for the Shepard plot if (type %in% c(&quot;nonmetric&quot;, &quot;sammon&quot;, &quot;isoMDS&quot;, &quot;monoMDS&quot;, &quot;metaMDS&quot;)) res$Shepard &lt;- MASS::Shepard(d = dist, x = res$points, p = p) res } class(mds) &lt;- c(&quot;function&quot;, &quot;subsettable_type&quot;) # plot.mds : MDS2 ~ MDS1 -------------------------------- plot.mds &lt;- function(x, y, ...) { points &lt;- tibble::as_tibble(x$points, .name_repair = &quot;minimal&quot;) colnames(points) &lt;- paste0(&quot;mds&quot;, 1:ncol(points)) plot(data = points, mds2 ~ mds1,...) } autoplot.mds &lt;- function(object, labels, ...) { points &lt;- tibble::as_tibble(object$points, .name_repair = &quot;minimal&quot;) colnames(points) &lt;- paste0(&quot;mds&quot;, 1:ncol(points)) if (!missing(labels)) { if (length(labels) != nrow(points)) stop(&quot;You must provide a character vector of length &quot;, nrow(points), &quot; for &#39;labels&#39;&quot;) points$.labels &lt;- labels chart::chart(points, mds2 ~ mds1 %label=% .labels, ...) + geom_point() + ggrepel::geom_text_repel() + coord_fixed(ratio = 1) } else {# Plot without labels chart::chart(points, mds2 ~ mds1, ...) + geom_point() + coord_fixed(ratio = 1) } } shepard &lt;- function(dist, mds, p = 2) structure(MASS::Shepard(d = dist, x = mds$points, p = p), class = c(&quot;shepard&quot;, &quot;list&quot;)) plot.shepard &lt;- function(x, y, l.col = &quot;red&quot;, l.lwd = 1, xlab = &quot;Observed Dissimilarity&quot;, ylab = &quot;Ordination Distance&quot;, ...) { she &lt;- tibble::as_tibble(x, .name_repair = &quot;minimal&quot;) plot(data = she, y ~ x, xlab = xlab, ylab = ylab, ...) lines(data = she, yf ~ x, type = &quot;S&quot;, col = l.col, lwd = l.lwd) } autoplot.shepard &lt;- function(object, alpha = 0.5, l.col = &quot;red&quot;, l.lwd = 1, xlab = &quot;Observed Dissimilarity&quot;, ylab = &quot;Ordination Distance&quot;, ...) { she &lt;- tibble::as_tibble(object) chart(data = she, y ~ x) + geom_point(alpha = alpha) + geom_step(chart::f_aes(yf ~ x), direction = &quot;vh&quot;, col = l.col, lwd = l.lwd) + labs(x = xlab, y = ylab) } # augment.mds ------------------------------------------- augment.mds &lt;- function(x, data, ...){ points &lt;- as_tibble(x$points) colnames(points) &lt;- paste0(&quot;.mds&quot;, 1:ncol(points)) bind_cols(data, points) } # glance.mds ------------------------------------------- glance.mds &lt;- function(x, ...){ if (&quot;GOF&quot; %in% names(x)) {# Probably cmdscale() or wcmdscale() =&gt; metric MDS tibble::tibble(GOF1 = x$GOF[1], GOF2 = x$GOF[2]) } else {# Non metric MDS # Calculate linear and non linear R^2 from the Shepard (stress) plot tibble::tibble( linear_R2 = cor(x$Shepard$y, x$Shepard$yf)^2, nonmetric_R2 = 1 - sum(vegan::goodness(x)^2) ) } } 6.2.2 MDS métrique ou PCoA La forme classique, aussi appelée MDS métrique ou analyse en coordonnées principales (Principal Coordinates Analysis en anglais ou PCoA), va projetter le nuage de points à p dimensions dans un espace réduit à k = 2 dimensions (voire éventuellement à 3 dimensions). Cette projection se fait de manière similaire à une ombre chinoise projettée d’un objet tridimensionnel sur une surface plane en deux dimensions. Ombre chinoise : un placement astucieux des mains dans le faisceau lumineux permet de projetter l’ombre d’un animal ou d’un objet sur une surface plane. La PCoA fait de même avec vos données. Considérons un relevé de couverture végétale en 24 stations concernant 44 plantes répertoriées sur le site de l’étude, par exemple, Callvulg est Calluna vulgaris, Empenigr est Empetrum nigrum, etc. Les valeurs sont les couvertures végétales observées pour chaque plante sur le site, expérimées en pourcents. La première colonne nommée rownames à l’importation contient les identifiants des stations (chaînes de caractères). Nous la renommons donc pour un intitulé plus explicite : station. read(&quot;varespec&quot;, package = &quot;vegan&quot;) %&gt;.% rename(., station = rownames) -&gt; veg veg # # A tibble: 24 x 45 # station Callvulg Empenigr Rhodtome Vaccmyrt Vaccviti Pinusylv Descflex # &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 18 0.55 11.1 0 0 17.8 0.07 0 # 2 15 0.67 0.17 0 0.35 12.1 0.12 0 # 3 24 0.1 1.55 0 0 13.5 0.25 0 # 4 27 0 15.1 2.42 5.92 16.0 0 3.7 # 5 23 0 12.7 0 0 23.7 0.03 0 # 6 19 0 8.92 0 2.42 10.3 0.12 0.02 # 7 22 4.73 5.12 1.55 6.05 12.4 0.1 0.78 # 8 16 4.47 7.33 0 2.15 4.33 0.1 0 # 9 28 0 1.63 0.35 18.3 7.13 0.05 0.4 # 10 13 24.1 1.9 0.07 0.22 5.3 0.12 0 # # … with 14 more rows, and 37 more variables: Betupube &lt;dbl&gt;, # # Vacculig &lt;dbl&gt;, Diphcomp &lt;dbl&gt;, Dicrsp &lt;dbl&gt;, Dicrfusc &lt;dbl&gt;, # # Dicrpoly &lt;dbl&gt;, Hylosple &lt;dbl&gt;, Pleuschr &lt;dbl&gt;, Polypili &lt;dbl&gt;, # # Polyjuni &lt;dbl&gt;, Polycomm &lt;dbl&gt;, Pohlnuta &lt;dbl&gt;, Ptilcili &lt;dbl&gt;, # # Barbhatc &lt;dbl&gt;, Cladarbu &lt;dbl&gt;, Cladrang &lt;dbl&gt;, Cladstel &lt;dbl&gt;, # # Cladunci &lt;dbl&gt;, Cladcocc &lt;dbl&gt;, Cladcorn &lt;dbl&gt;, Cladgrac &lt;dbl&gt;, # # Cladfimb &lt;dbl&gt;, Cladcris &lt;dbl&gt;, Cladchlo &lt;dbl&gt;, Cladbotr &lt;dbl&gt;, # # Cladamau &lt;dbl&gt;, Cladsp &lt;dbl&gt;, Cetreric &lt;dbl&gt;, Cetrisla &lt;dbl&gt;, # # Flavniva &lt;dbl&gt;, Nepharct &lt;dbl&gt;, Stersp &lt;dbl&gt;, Peltapht &lt;dbl&gt;, # # Icmaeric &lt;dbl&gt;, Cladcerv &lt;dbl&gt;, Claddefo &lt;dbl&gt;, Cladphyl &lt;dbl&gt; Typiquement ce genre de données ne contient pas d’information constructive lorsqu’une plante est simultanément absente de deux stations (double zéros). Donc, les métriques de type euclidienne ou Manhattan ne conviennent pas ici. Nous devons choisir entre distance de Bray-Curtis ou Canberra en fonction de l’importance que nous souhaitons donner aux plantes les plus rares (avec couverture végétale faible et/ou absentes de la majorité des stations). Afin de décider quelle métrique utiliser, visualisons à présent l’abondance ou la rareté des différentes plantes : veg %&gt;.% select(., -station) %&gt;.% # Colonne &#39;station&#39; pas utile ici gather(., key = &quot;espèce&quot;, value = &quot;couverture&quot;) %&gt;.% # Tableau en format long chart(., couverture ~ espèce) + geom_boxplot() + # Boites de dispersion labs(x = &quot;Espèce&quot;, y = &quot;Couverture [%]&quot;) + coord_flip() # Labels plus lisibles si sur l&#39;axe Y Comme nous pouvions nous y attendre, sept ou huit espèces dominent la couverture végétales et les autres données sont complètement écrasées à zéro sur l’axe pour la majorité des stations. Si nous utilisons la distance de Bray-Curtis, l’analyse sera pratiquement réalisée sur seulement ces quelques espèces dominantes. Avec Canberra, nous risquons par contre de donner beaucoup trop d’importance aux espèces extrêmement rares (toutes les espèces ont une importance égale avec cette métrique). Une solution intermédiaire est de transformer les données pour réduire l’écart d’importance entre les espèces abondantes et les rares, soit avec \\(log(x + 1)\\), soit avec \\(\\sqrt{\\sqrt{x}}\\). Voyons ce que donne la transformation logarithmique ici en utilisant la fonction log1p() dans R. veg %&gt;.% select(., -station) %&gt;.% gather(., key = &quot;espèce&quot;, value = &quot;couverture&quot;) %&gt;.% chart(., log1p(couverture) ~ espèce) + # Transformation log(couverture + 1) geom_boxplot() + labs(x = &quot;Espèce&quot;, y = &quot;Couverture [%]&quot;) + coord_flip() C’est nettement mieux car les données concernant les espèces rares ne sont plus totalement écrasées vers zéro sur l’axe horizontal ! La matrice de distances de Bray-Curtis sur nos données transformées log est la première étape de l’analyse : veg %&gt;.% select(., -station) %&gt;.% log1p(.) %&gt;.% vegan::vegdist(., method = &quot;bray&quot;) -&gt; veg_dist N’imprimez pas le contenu de veg_dist ! C’est de toutes façons illisible. La PCoA va visualiser son contenu de manière bien plus utile. La seconde étape consiste à calculer notre MDS métrique en utilisant mds$metric() veg_mds &lt;- mds$metric(veg_dist) Ensuite, troisième étape, le but étant de visualiser les distances nous effectons immédiatement un graphique comme suit : autoplot(veg_mds, labels = veg$station) Ce graphique s’interprète comme suit : Des stations proches l’une de l’autre sur la carte ont des indices de dissimilarité faibles. Ces stations sont semblables du point de vue de la couverture végétale. Plus les stations sont éloignées les unes des autres, plus elles sont dissemblables. Si des regroupements apparaissent sur la carte, il se peut que ce soit des biotopes semblables, et qui diffèrent des autres regroupements. Par exemple ici, les stations 14–15, 20 et 22–25 forment un groupe relativement homogène en haut à gauche du graphique qui s’individualise du reste. Au contraire, les stations 5, 21, ou encore 27 ou 28 sont relativement isolées et constituent donc des assemblages végétaux uniques. Les stations aux extrémités sont des configurations extrêmes ; celles au centre sont des configurations plus courantes. Par contre, ni l’orientation des axes, ni les valeurs absolues sur ces axes n’ont de significations particulières ici. N’en tenez pas compte. Attention : rien ne garantit que notre MDS métrique projettée en deux dimensions soit suffisamment représentative des données dans leur ensemble. Si la méthode n’a pas réussi à représenter fidèlement les données, c’est que ces dernières sont trop complexes et ne s’y prêtent pas. Contrôlez donc toujours les indicateurs que sont les valeurs de “Goodness-of-fit” (GOF, qualité d’ajustement). Les indicateurs “GOF” sont obtenus via la fonction glance() : glance(veg_mds) # # A tibble: 1 x 2 # GOF1 GOF2 # &lt;dbl&gt; &lt;dbl&gt; # 1 0.527 0.554 Ici GOF1 est la somme des valeurs propres obtenues lors du calcul (ces valeurs propres vous seront expliquées dans le module suivant consacré à l’ACP). Retenez simplement que c’est une mesure de la part de variance du jeu de données initial qui a pu être représentée sur la carte. Plus la valeur se rapproche de 1, mieux c’est, avec des valeurs &gt; 0.7 ou 0.8 qui restent acceptables. Le second indicateur, GOF2 est la somme uniquement des valeurs propres positives. Certains préfèrent ce dernier indicateur. En principe, les deux sont proches ou égaux. Donc, le choix de l’un ou de l’autre ne devrait pas fondamentalement modifier vos conclusions. Ici, avec des valeurs de goodness-of-fit à peine supérieures à 50% nous pouvons considérer que la carte n’est pas suffisamment représentative. Soit nous tentons de la représenter en trois dimensions (mais c’est rarement plus lisible car il faut quand même se résigner à présenter ce graphique 3D dans un plan à deux dimensions -l’écran de l’ordinateur, ou une feuille de papier- au final). Une autre solution lorsque la MDS métrique ne donne pas satisfaction est de se tourner vers la MDs non métrique. Ce que nous allons faire ci-dessous. A noter que la PCoA sur matrice euclidienne après standardisation ou non des données est équivalente à une Analyse en Composantes Principales (ACP) que nous étudierons dans le module suivant, … mais avec un calcul nettement moins efficace. Dans ce contexte, la PCoA n’a donc pas grand intérêt. Elle est surtout utile lorsque vous voulez représenter des métriques de distances différentes de la distance euclidienne comme c’est le cas ici avec un choix de distances de Bray-Curtis. 6.2.3 MDS non métrique La version non métrique de la MDS vise à réaliser une carte sur base de la matrice de distances, mais en autorisant des écarts plus flexibles entre les individus… pour autant que des individus similaires restent plus proches les uns des autres que des individus plus différents, et ce, partout sur la carte. Donc, une dissimilarité donnée pourra être “compressée” ou “dilatée”, pour autant que la distortion garde l’ordre des points intacts. Cela signifie que la distortion se fera via une fonction monotone croissante (une dissimilarité plus grande ne pouvant pas être représentée par une distance plus petite sur la carte). La distortion ainsi introduite est appelée un stress. C’est un peu comme si vous écrasiez par la force un objet 3D sur une surface plane, au lieu de juste en projeter l’ombre. Comme il existe différentes fonctions de stress, il existe donc différentes versions de MDS non métriques. Ici, nous nous attacherons à maitriser une version implémentée dans mds$nonmetric(). Il s’agit de l’une des premières formes de MDS non métriques qui a été proposée par le statisticien Joseph Kruskal (on parle aussi du positionnement multidimensionnel de Kruskal). La logique est la même que pour la MDS métrique : étape 1 : construction d’une matrice de distances, étape 2 : calcul du positionnement des points, étape 3 : réalisation de la carte et vérification de sa validité. Repartons de la même matrice de distances déjà réalisée pour la MDS métrique qui se nomme veg_dist. Le calcul est itératif. Comme il n’est pas garanti de converger, ni de donner la meilleure réponse, nous utilisons ici une fonction “intelligente” qui va effectuer une recherche plus poussée de la solution optimale, notamment en partant de différentes configurations au départ. Pour les détails et les paramètres de cet algorithme, voyez l’aide en ligne de la fonction ?vegan::metaMDS. Dans le cadre de ce cours, nous ferons confiance au travail réalisé et vérifierons juste qu’une solution est trouvée (indication *** Solution reached à la fin). Notez toutefois que le stress est quantifié. Il tourne ici autour de 0,126. Plus la valeur de stress est basse, mieux c’est naturellement. veg_nmds &lt;- mds$nonmetric(veg_dist) # Calcul # Run 0 stress 0.1256617 # Run 1 stress 0.1262346 # Run 2 stress 0.1262346 # Run 3 stress 0.1262346 # Run 4 stress 0.1256617 # ... Procrustes: rmse 1.056302e-05 max resid 3.244521e-05 # ... Similar to previous best # Run 5 stress 0.1256617 # ... Procrustes: rmse 6.34123e-06 max resid 1.783762e-05 # ... Similar to previous best # Run 6 stress 0.1256617 # ... Procrustes: rmse 1.768348e-05 max resid 4.243365e-05 # ... Similar to previous best # Run 7 stress 0.1262346 # Run 8 stress 0.1262346 # Run 9 stress 0.1256617 # ... Procrustes: rmse 1.668964e-05 max resid 5.282751e-05 # ... Similar to previous best # Run 10 stress 0.1262347 # Run 11 stress 0.1912667 # Run 12 stress 0.1262346 # Run 13 stress 0.1256617 # ... Procrustes: rmse 1.797304e-05 max resid 5.269659e-05 # ... Similar to previous best # Run 14 stress 0.1256617 # ... Procrustes: rmse 1.209131e-05 max resid 3.718001e-05 # ... Similar to previous best # Run 15 stress 0.2004491 # Run 16 stress 0.1256617 # ... New best solution # ... Procrustes: rmse 9.079683e-06 max resid 3.527497e-05 # ... Similar to previous best # Run 17 stress 0.1262347 # Run 18 stress 0.1262346 # Run 19 stress 0.2250581 # Run 20 stress 0.2105936 # *** Solution reached A présent, nous pouvons représenter la carte. autoplot(veg_nmds, labels = veg$station) Nous avons une représentation assez différente de celle de la MDS métrique. Les stations 5, 21, 27 et 28 sont toujours isolées, mais le reste est regroupé de manière plus homogène. Comment savoir si cette représentation est meilleure que la version métrique qui avait une “goodness-of-fit” décevante ? En visualisant les indicateurs de qualité d’ajustement, ainsi que la fonction de stress sur un graphique dit graphique de Shepard. Comme d’habitude, glance() nous donne les statistiques voulues. glance(veg_nmds) # # A tibble: 1 x 2 # linear_R2 nonmetric_R2 # &lt;dbl&gt; &lt;dbl&gt; # 1 0.919 0.984 Le premier indicateur (R2 linéaire) est le coefficient de corrélation linéaire de Pearson entre les distances ajustées et les distances sur la carte au carré. Plus cette valeur est proche de un, moins les distances sont tordues. Le second indicateur, le R2 non métrique est calculé comme 1 - S2 où S est le stress (tel que quantifié plus haut lors de l’appel à la fonction mds$nonmetric()). Cette dernière statistique indique si l’ordre des points respecte l’ordre des distances partout sur le graphique. Avec 0,98, la valeur est excellente ici. Ensuite le R2 linéaire nous indique de combien les différentes distances sont éventuellement distordues. Avec une valeur de 0,92, la distortion n’est pas trop forte ici. Le diagramme de Shepard permet de visualiser dans le détail la distortion introduite pour parvenir à réaliser la carte en deux dimensions. veg_sh &lt;- shepard(veg_dist, veg_nmds) autoplot(veg_sh) Sur l’axe des abscisses, nous avons les valeurs de dissimilarité présentes dans la matrice de distances. Sur l’axe des ordonnées, le graphique représente les distances de l’ordination, c’est-à-dire, les distances entre les paires de points sur la carte. Chaque point correspond à la dissimilarité d’une paire d’individus sur X, et à la distance entre cette paire sur la carte en Y. Enfin, le trait en escalier rouge matérialise la fonction monotone croissante choisie pour distordre les distances. C’est la fonction de stress. Ce diagramme se lit comme suit : Plus les points sont proches de la fonction de stress, mieux c’est. Le R2 non métrique sera également d’autant plus élevé que les points sont proches de la fonction. Plus la fonction de stress est linéaire, plus les distances respectent les valeurs de dissimilarités. Le R2 linéaire est lié à la plus ou moins bonne linéarité de la fonction de stress. Vous pouvez très bien décider que seul l’ordre des individus sur la carte compte. Dans ce cas, la forme de la fonction de stress et la valeur du R2 linéaire importent peu. Seul compte la proximité la plus forte possible des points par rapport à la fonction de stress sur le diagramme de Shepard, ainsi donc que la valeur du R2 non métrique. Si par contre, vous voulez être plus contraignant, alors, les distances seront considérées également comme importantes. Vous rechercherez alors une fonction de stress pas trop éloignée d’une droite, ainsi qu’un R2 linéaire élevé. Dans ce cas, nous nous rapprochons des exigences de la MDS métrique. Ici, nous pouvons constater que les deux critères sont bons. Nous pouvons donc nous fier à la carte obtenue à l’aide de la MDs non métrique de Kruskal. Restez toujours attentif à la taille du jeu de données que vous utilisez pour réaliser une MDS, en particuliers une MDS non métrique. Quelques centaines de lignes, ça dois passer, plusieurs dizaines de milliers ou plus, ça ne passera pas ! La limite dépend bien sûr de la puissance de votre ordinateur et de la quantité de mémoire vive disponible. Retenez toutefois que la quantité de calculs augmente drastiquement avec la taille du jeu de données à traiter. Pour en savoir plus La fonction mds() donne accès à d’autres versions de MDS non métriques également. Ainsi, mds$isoMDS() ou mds$monoMDS() correspondent toutes deux à la version de Kruskal mais en utilisant une seule configuration de départ (donc, moins robustes mais plus rapides à calculer). La mds$sammon() est une autre forme de MDS non métrique décrite dans l’aide en ligne de ?MASS::sammon. Des techniques existent pour déterminer la dimension k idéale de la carte. Le graphique des éboulis (screeplot en anglais) sera abordé au module suivante dans le cadre de l’ACP. Il en existe une version pour le MDS, voyez ici (en anglais). A vous de jouer ! Réalisez le tutoriel afin de vérifier votre bonne compréhension de la mds. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant les bases de R : BioDataScience2::run(&quot;06b_mds&quot;) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel dans la console R. Complétez votre carnet de note par binôme sur le transect entre Nice et Calvi débuté lors du module 5. Lisez attentivement le README (Ce dernier a été mis à jour). Completez votre projet. Lisez attentivement le README. La dernière version du README est disponible via le lien suivant : https://github.com/BioDataScience-Course/spatial_distribution_zooplankton_ligurian_sea Chaque métrique de distance offre un éclairage différent sur les données. Elles agissent comme autant de filtres différents à votre disposition pour explorer vos données multivariées.↩ "],
["cartes-auto-adaptatives-som.html", "6.3 Cartes auto-adaptatives (SOM)", " 6.3 Cartes auto-adaptatives (SOM) Le positionnement multidimensionnel faisant appel à une matrice de distances entre tous les individus, les calculs deviennent vite pénalisants au fur et à mesure que le jeu de données augmente en taille. En général, les calculs sont assez lents. Nous verrons au module suivant que l’analyse en composantes principales apporte une réponse intéressante à ce problème, mais nous contraint à étudier des corrélations linéaires et des distances de type euclidiennes. Une approche radicalement différente, qui reste plus générale car non linéaire, est la méthode des cartes auto-adaptatives, ou encore, cartes de Kohonen du nom de son auteur se désigne par “self-organizing map” en anglais. L’acronyme SOM est fréquemment utilisé, même en français. Cette technique va encore une fois exploiter une matrice de distances dans le but de représenter les individus sur une carte. Cette fois-ci, la carte contient un certain nombre de cellules qui forment une grille, ou mieux, une disposition en nid d’abeille (nous verrons plus loin pourquoi cette disposition particulière est intéressante). De manière similaire au MDS, nous allons faire en sorte que des individus similaires soient proches sur la carte, et des individus différents soient éloignés. La division de la carte en différentes cellules permet de regrouper les individus. Ceci permet une classification comme pour la CAH ou les k-moyennes. Les SOM apparaissent donc comme une technique hybride entre ordination (représentation sur des cartes) et classification (regroupement des individus). La théorie et les calculs derrière les SOM sont très complexes. Elles font appel aux réseaux de neurones adaptatifs et leur fonctionnement est inspiré de celui du cerveau humain. Tout comme notre cerveau, les SOM vont utiliser l’information en entrée pour aller assigner une zone de traitement de l’information (pour notre cerveau) ou une cellule dans la carte (pour les SOM). Étant donné la complexité du calcul, les développement mathématiques n’ont pas leur place dans ce cours. Ce qui importe, c’est de comprendre le concept, et d’être ensuite capable d’utiliser les SOM à bon escient. Uniquement pour ceux d’entre vous qui désirent comprendre les détails du calcul, vous pouvez lire ici ou visionner la vidéo suivante (facultative et en anglais) : Plutôt que de détailler les calculs, nous vous montrons ici comment un ensemble de pixels de couleurs différentes est organisé sur une carte SOM de Kohonen en un arrangement infiniment plus cohérent… automatiquement (cet exemple est proposé par Frédéric De Lène Mirouze dans son blog). Image créée artificiellement avec disposition aléatoire des pixels. Carte SOM obtenue à partir de l’image précédente : les pixels sont automatiquement triés par couleur sur la carte. Ce qui est évident sur un exemple aussi visuel que celui-ci fonctionne aussi très bien pour ranger les individus dans un tableau multivarié a priori cahotique comme ceux que nous rencontrons régulièrement en statistiques multivariées en biologie. 6.3.1 SOM sur le zooplancton Reprenons notre exemple du zooplankton. zoo &lt;- read(&quot;zooplankton&quot;, package = &quot;data.io&quot;) zoo # # A tibble: 1,262 x 20 # ecd area perimeter feret major minor mean mode min max std_dev # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 0.770 0.465 4.45 1.32 1.16 0.509 0.363 0.036 0.004 0.908 0.231 # 2 0.700 0.385 2.32 0.728 0.713 0.688 0.361 0.492 0.024 0.676 0.183 # 3 0.815 0.521 4.15 1.33 1.11 0.598 0.308 0.032 0.008 0.696 0.204 # 4 0.785 0.484 4.44 1.78 1.56 0.394 0.332 0.036 0.004 0.728 0.218 # 5 0.361 0.103 1.71 0.739 0.694 0.188 0.153 0.016 0.008 0.452 0.110 # 6 0.832 0.544 5.27 1.66 1.36 0.511 0.371 0.02 0.004 0.844 0.268 # 7 1.23 1.20 15.7 3.92 1.37 1.11 0.217 0.012 0.004 0.784 0.214 # 8 0.620 0.302 3.98 1.19 1.04 0.370 0.316 0.012 0.004 0.756 0.246 # 9 1.19 1.12 15.3 3.85 1.34 1.06 0.176 0.012 0.004 0.728 0.172 # 10 1.04 0.856 7.60 1.89 1.66 0.656 0.404 0.044 0.004 0.88 0.264 # # … with 1,252 more rows, and 9 more variables: range &lt;dbl&gt;, size &lt;dbl&gt;, # # aspect &lt;dbl&gt;, elongation &lt;dbl&gt;, compactness &lt;dbl&gt;, transparency &lt;dbl&gt;, # # circularity &lt;dbl&gt;, density &lt;dbl&gt;, class &lt;fct&gt; Les 19 premières colonnes représentent des mesures réalisées sur notre plancton et la vingtième est la classe. Nous nous débarasserons de la colonne classe et transformons les données numériques en matrice après avoir standardisé les données (étapes obligatoires) pour stocker le résultat dans zoo_mat. zoo %&gt;.% select(., -class) %&gt;.% scale(.) %&gt;.% as.matrix(.) -&gt; zoo_mat Avant de pouvoir réaliser notre analyse, nous devons décider d’avance la topologie de la carte, c’est-à-dire, l’arrangement des cellules ainsi que le nombre de lignes et de colonnes. Le nombre de cellules totales choisies dépend à la fois du niveau de détails souhaité, et du nombre d’individus dans votre jeu de données (il faut naturellement plus de données que de cellules, disons, au moins 5 à 10 fois plus). Pour l’instant, considérons les deux topologies les plus fréquentes : la grille rectangulaire et la grille hexagonale. Plus le nombre de cellules est important, plus la carte sera détaillée, mais plus il nous faudra de données pour la calculer et la “peupler”. Considérons par exemple une grille 7 par 7 qui contient donc 49 cellules au total. Sachant que nous avons plus de 1200 particules de plancton mesurées dans zoo, le niveau de détail choisi est loin d’être trop ambitieux. La grille rectangulaire est celle qui vous vient probablement immédiatement à l’esprit. Il s’agit d’arranger les cellules en lignes horizontales et colonnes verticales. La fonction somgrid() du package kohonen permet de créer une telle grille. library(kohonen) # Charge le package kohonen # # Attaching package: &#39;kohonen&#39; # The following object is masked from &#39;package:purrr&#39;: # # map rect_grid_7_7 &lt;- somgrid(7, 7, topo = &quot;rectangular&quot;) # Crée la grille Il n’y a pas de graphique chart ou ggplot2 dans le package kohonen. Nous utiliserons ici les graphiques de base de R. Pour visualiser la grille, il faut la transformer en un objet kohonen. Nous pouvons ajouter plein d’information sur la grille. Ici, nous rajoutons une propriété calculée à l’aide de unit.distances() qui est la distance des cellules de la carte par référence à la cellule centrale. Les cellules sont numérotées de 1 à n en partant en bas à gauche, en progressant le long de la ligne du bas vers la droite, et en reprenant à gauche à la ligne au dessus. Donc, la ligne du bas contient de gauche à droite les cellules n°1 à 7. La ligne au dessus contient les cellules n°8 à 14, et ainsi de suite. La cellule du centre de la grille en en 4ème ligne en partant du bas et en position 4 sur cette ligne, soit trois lignes complètes plus quatre (\\(3*7+4=25\\)). C’est la cellule n°25. rect_grid_7_7 %&gt;.% # Transformation en un objet de classe kohonen qui est une liste structure(list(grid = .), class = &quot;kohonen&quot;) %&gt;.% # Objet de classe kohonen plot(., type = &quot;property&quot;, # Graphique de propriété property = unit.distances(rect_grid_7_7)[25, ], # distance à la cellule 25 main = &quot;Distance depuis la cellule centrale&quot;) # Titre du graphique Les cellules de la grille ne sont pas disposées au hasard dans la carte SOM. Des relations de voisinage sont utilisées pour placer les individus à représenter dans des cellules adjacentes s’ils se ressemblent. Avec une grille rectangulaire, nous avons donc deux modalités de variation : en horizontal et en vertival, ce qui donne deux gradients possibles qui, combinés donnent des extrêmes dans les coins opposés. Une cellule possède huits voisins directs. L’autre topologie possible est la grille hexagonale. Voyons ce que cela donne : hex_grid_7_7 &lt;- somgrid(7, 7, topo = &quot;hexagonal&quot;) hex_grid_7_7 %&gt;.% # Transformation en un objet de classe kohonen qui est une liste structure(list(grid = .), class = &quot;kohonen&quot;) %&gt;.% # Objet de classe kohonen plot(., type = &quot;property&quot;, # Graphique de propriété property = unit.distances(hex_grid_7_7)[25, ], # distance à la cellule 25 main = &quot;Distance depuis la cellule centrale&quot;) # Titre du graphique Ici, nous n’avons que six voisins directs, mais trois directions dans lesquelles les gradients peuvent varier : en horizontal, en diagonale vers la gauche et en diagonale vers la droite. Cela offre plus de possibilités pour l’agencement des individus. Nous voyons aussi plus de nuances dans les distances (il y a plus de couleurs différentes), pour une grille de même taille 7 par 7 que dans le cas de la grille rectangulaire. Nous utiliserons donc préférentiellement la grille hexagonale. Effectuons maintenant le calcul de notre SOM à l’aide de la fonction som() du package kohonen. Comme l’analyse fait intervenir le générateur pseudo-aléatoire, nous pouvons utiliser de manière optionnelle set.seed() avec un nombre choisi au hasard (et toujours différent à chaque utilisation) pour que cette analyse particulière-là soit reproductible. Sinon, à chaque exécution, nous obtiendrons un résultat légèrement différent. set.seed(8657) zoo_som &lt;- som(zoo_mat, grid = somgrid(7, 7, topo = &quot;hexagonal&quot;)) summary(zoo_som) # SOM of size 7x7 with a hexagonal topology and a bubble neighbourhood function. # The number of data layers is 1. # Distance measure(s) used: sumofsquares. # Training data included: 1262 objects. # Mean distance to the closest unit in the map: 2.519. Le résumé de l’objet ne nous donne pas beaucoup d’info. C’est normal. La technique étant visuelle, ce qui est important, c’est de représenter graphiquement la carte. Avec les graphiques R de base, la fonction utilisée est plot(). Nous avons plusieurs types disponibles et une large palette d’options. Voyez l’aide en ligne de?plot.kohonen. Le premier graphique (type = &quot;changes&quot;) montre l’évolution de l’apprentissage au fil des itérations. L’objectif est de descendre le plus possible sur l’axe des ordonnées pour réduire au maximum la distance des individus par rapport aux cellules (unit en anglais) où ils devraient se placer. Idéalement, nous souhaitons tendre vers zéro. En pratique, nous pourrons arrêter les itérations lorsque la courbe ne diminue plus de manière significative. plot(zoo_som, type = &quot;changes&quot;) Ici, il semble que nous ne diminuons plus vraiment à partir de la 85ème itération environ. Nous pouvons nous en convaincre en relançant l’analyse avec un plus grand nombre d’itérations (avec l’argument rlen = de som()). set.seed(954) zoo_som &lt;- som(zoo_mat, grid = somgrid(7, 7, topo = &quot;hexagonal&quot;), rlen = 200) plot(zoo_som, type = &quot;changes&quot;) Vous serez sans doute surpris de constater que la diminution de la courbe se fait plus lentement maintenant. En fait som() va adapter son taux d’apprentissage en fonction du nombre d’itérations qu’on lui donne et va alors “peaufiner le travail” d’autant plus. Au final, la valeur n’est pas plus basse pour autant. Donc, nous avons aboutit probablement à une solution. Le second graphique que nous pouvons réaliser consiste à placer les individus dans la carte, en utilisant éventuellement une couleur différente en fonction d’une caractéristique de ces individus (ici, leur classe). Ce graphique est obtenu avec type = &quot;mapping&quot;. Si vous ne voulez pas représenter la grille hexagonale à l’aide de cercles, vous pouvez spécifier shape = &quot;straight&quot;. Nous avons 17 classes de zooplancton et il est difficile de représenter plus de 10-12 couleurs distinctes, mais ce site propose une palette de 20 couleurs distinctes. Nous en utiliserons les 17 premières… colors17 &lt;- c(&quot;#e6194B&quot;, &quot;#3cb44b&quot;, &quot;#ffe119&quot;, &quot;#4363d8&quot;, &quot;#f58231&quot;, &quot;#911eb4&quot;, &quot;#42d4f4&quot;, &quot;#f032e6&quot;, &quot;#bfef45&quot;, &quot;#fabebe&quot;, &quot;#469990&quot;, &quot;#e6beff&quot;, &quot;#9A6324&quot;, &quot;#fffac8&quot;, &quot;#800000&quot;, &quot;#aaffc3&quot;, &quot;#808000&quot;, &quot;#ffd8b1&quot;) plot(zoo_som, type = &quot;mapping&quot;, shape = &quot;straight&quot;, col = colors17[zoo$class]) Nous n’avons pas ajouté de légende qui indique à quelle classe correspond quelle couleur. Ce que nous voulons voir, c’est si les cellules arrivent à séparer les classes. Nous voyons que la séparation est imparfaite, mais des tendances apparaissent avec certaines couleurs qui se retrouvent plutôt dans une région de la carte. Nous voyons donc ici que, malgré que l’information contenue dans class n’ait pas été utilisées. Les différents individus de zooplancton ne se répartissent pas au hasard en fonction de ce critère. Nous pouvons également voir les cellules qui contiennent plus ou moins d’individus, mais si l’objectif est de visionner uniquement le remplissage des cellules, le type = &quot;counts&quot; est plus adapté. plot(zoo_som, type = &quot;counts&quot;, shape = &quot;straight&quot;) Nous pouvons obtenir la cellule dans laquelle chaque individu est mappé comme suit : zoo_som$unit.classif # [1] 19 6 25 17 15 19 40 17 40 19 41 39 6 32 17 14 32 22 40 29 26 24 26 # [24] 20 38 19 20 39 19 5 25 2 19 19 43 20 29 23 36 10 23 5 30 16 17 18 # [47] 23 17 12 18 11 1 16 17 12 16 10 45 16 17 39 2 19 32 6 45 32 5 32 # [70] 16 6 2 9 46 4 14 14 6 17 32 19 4 6 4 6 40 9 12 45 16 6 45 # [93] 7 49 5 10 10 16 10 1 10 17 10 10 36 10 16 36 25 9 31 2 11 12 10 # [116] 23 15 10 10 25 1 1 10 16 13 16 17 17 38 10 16 25 10 2 10 16 25 11 # [139] 18 10 1 36 14 7 40 41 48 40 24 6 4 31 12 32 19 35 39 45 19 40 25 # [162] 24 1 30 4 7 18 43 18 12 30 30 17 17 19 31 36 30 36 30 11 11 16 26 # [185] 16 11 11 25 18 11 11 19 29 11 30 36 14 24 14 18 14 18 25 26 4 39 25 # [208] 10 19 11 18 10 15 19 11 32 20 30 6 36 6 12 14 18 10 1 11 18 12 18 # [231] 1 7 30 30 30 18 23 23 11 3 30 16 1 38 30 1 1 30 11 30 25 36 1 # [254] 30 11 11 36 1 30 1 36 30 30 1 5 18 30 30 34 34 47 20 20 47 25 38 # [277] 27 26 39 20 19 34 19 17 34 11 18 47 27 34 39 27 26 19 19 10 32 32 34 # [300] 31 31 19 33 33 20 36 27 31 47 20 32 39 46 11 33 34 47 19 34 27 30 38 # [323] 29 25 31 36 25 32 1 29 1 11 11 32 31 28 19 30 31 32 36 12 17 36 18 # [346] 17 39 38 38 25 36 36 19 10 37 30 18 11 19 18 19 19 30 17 18 31 6 33 # [369] 12 20 18 11 18 20 20 18 11 23 20 19 11 27 45 19 5 20 19 14 20 20 20 # [392] 5 29 11 26 20 18 39 20 18 4 23 18 25 10 11 11 38 11 18 17 38 43 18 # [415] 18 11 18 5 26 24 45 43 32 45 7 38 39 18 3 25 45 39 41 17 19 15 3 # [438] 46 10 26 45 33 28 22 39 8 30 3 43 20 33 7 41 39 16 39 22 30 38 7 # [461] 3 25 30 3 38 3 17 37 3 3 18 37 38 15 39 22 15 5 39 3 16 16 16 # [484] 30 23 3 3 22 31 39 38 45 15 28 3 28 15 43 39 38 3 29 23 3 23 29 # [507] 18 16 42 42 24 42 40 35 6 44 23 3 42 26 45 35 42 26 18 8 44 3 44 # [530] 44 49 15 28 16 3 5 43 10 29 10 8 26 43 16 23 14 42 33 3 12 35 41 # [553] 33 22 32 35 28 42 3 31 18 24 44 24 49 16 22 25 15 7 8 23 23 29 37 # [576] 1 23 15 3 34 44 44 37 40 29 46 43 43 44 41 20 42 43 24 4 28 35 49 # [599] 3 3 23 15 3 15 15 23 17 28 15 43 43 23 3 23 3 3 3 3 28 3 17 # [622] 3 17 15 28 42 28 39 3 28 44 32 33 28 9 33 39 41 22 22 9 38 28 28 # [645] 42 28 28 43 2 30 38 1 36 9 23 17 17 25 28 39 39 28 28 30 3 28 30 # [668] 32 26 37 30 22 39 28 22 14 30 30 46 35 28 3 3 3 22 27 30 43 3 3 # [691] 15 29 25 3 37 29 37 29 29 23 3 34 10 24 34 27 17 24 9 8 33 47 40 # [714] 32 2 2 34 33 20 34 33 38 33 47 26 9 33 34 9 39 2 32 34 27 8 47 # [737] 26 34 27 33 28 8 40 2 45 24 34 39 43 17 31 32 23 37 27 9 9 17 9 # [760] 15 45 37 37 31 17 8 17 45 28 28 19 29 25 7 39 19 9 9 43 41 24 40 # [783] 9 29 8 24 2 42 8 24 43 8 2 48 8 8 14 24 20 17 28 8 37 40 45 # [806] 7 7 37 32 46 21 37 7 41 45 40 39 9 17 23 37 7 10 16 16 17 23 30 # [829] 16 9 38 15 43 38 15 16 15 38 23 36 37 7 29 9 23 9 17 17 17 17 37 # [852] 39 24 19 32 35 35 44 20 19 23 20 19 17 44 42 45 40 20 24 44 33 45 19 # [875] 33 46 19 44 33 39 32 39 26 39 38 30 23 30 37 23 20 17 38 39 31 31 29 # [898] 19 12 23 37 30 38 25 30 16 38 37 12 45 16 23 38 31 7 39 25 46 26 44 # [921] 35 14 19 39 42 19 19 38 40 14 44 45 40 24 35 39 28 21 48 46 45 32 32 # [944] 16 44 22 39 43 38 39 46 32 32 25 38 7 23 23 12 23 30 43 22 30 29 23 # [967] 16 23 38 37 37 40 24 40 26 19 24 22 37 14 28 46 6 26 27 44 44 24 44 # [990] 45 24 46 26 32 24 45 44 37 39 32 24 42 40 30 40 40 46 23 33 15 5 23 # [1013] 23 37 44 12 43 23 44 42 16 26 44 35 38 42 45 24 35 43 26 20 23 42 43 # [1036] 33 40 44 45 45 44 24 43 46 25 32 42 46 4 24 32 7 23 25 37 17 7 22 # [1059] 23 29 23 15 10 29 38 37 37 35 40 42 39 45 42 24 42 42 44 26 35 46 35 # [1082] 39 42 20 46 42 26 26 14 5 19 46 24 42 35 26 40 40 33 26 24 42 35 24 # [1105] 12 46 42 45 42 42 42 19 24 11 46 5 13 8 13 12 10 17 32 10 15 7 28 # [1128] 11 39 20 10 7 28 32 18 4 11 18 12 45 28 18 45 33 26 28 28 5 11 7 # [1151] 18 28 7 5 5 7 7 18 18 18 18 7 16 18 5 5 16 28 43 32 45 27 5 # [1174] 22 29 29 7 36 6 29 5 5 7 5 29 11 16 5 7 11 7 7 7 7 31 2 # [1197] 8 4 9 8 28 6 2 30 9 8 4 10 8 8 4 9 31 20 11 4 45 2 4 # [1220] 8 1 2 1 31 1 11 10 17 5 8 8 25 9 8 1 1 10 1 1 1 1 23 # [1243] 36 25 10 1 1 1 10 10 1 36 1 25 6 36 2 36 37 43 45 38 Par conséquent, nous pouvons créer un tableau de contingence qui répertorie le nombre d’iundividus mappés dans chaque cellule à l’aide de table(). Nous l’enregistrons dans zoo_som_nb car nous la réutiliserons plus tard. zoo_som_nb &lt;- table(zoo_som$unit.classif) zoo_som_nb # # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # 33 17 40 15 24 17 32 24 23 38 38 18 3 16 24 35 42 43 44 31 2 17 46 33 30 # 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 # 28 13 37 26 45 20 35 23 17 18 24 29 33 42 26 9 30 27 26 34 20 8 3 4 6.3.2 Interprétation d’un SOM De nombreuses autres présentations graphiques sont possibles sur cette base. Nous allons explorer deux aspects complémentaires : (1) représentation des variables, et (2) réalisation et représentation de regroupements. 6.3.2.1 Représentation des variables La carte SOM est orientée. C’est-à-dire que les cellules représentent des formes différentes de plancton telles qu’exprimées à travers les 19 variables utilisées ici (quantification de la taille, de la forme, de la transparence, …). Le graphique type = &quot;codes&quot; permet de visualiser ces différences de manière générale : plot(zoo_som, type = &quot;codes&quot;, codeRendering = &quot;segments&quot;) Ce graphique est riche en informations. Nous voyons que : les très grands individus (ecd, area, perimeter, etc.), soit les segments verts sont en haut à droite de la carte et les petits sont à gauche, les individus opaques (variables mean, mode, max, etc.18), soit des segments dans les tons jaunes sont en bas à droite. Les organismes plus transparents sont en haut à gauche, au delà de ces deux principaux critères qui se dégagement prioritairement, les aspects de forme (segments rose-rouges) se retrouvent exprimés moins nettement le long de gradients. La circularity mesure la silhouette plus ou moins arrondie des items (sa valeur est d’autant plus élevée que la forme se rapproche d’un cercle). Les organismes circulaires se retrouvent dans le bas de la carte. L’elongation et l’aspect mesurent l’allongement de la particule et se retrouvent plutôt exprimés positivement vers le haut de la carte. Nous pouvons donc orienter notre carte SOM en indiquant l’information relative aux variables. Lorsque le nombre de variables est élevé ou relativement élevé comme ici, cela devient néanmoins difficile à lire. Il est aussi possible de colorer les cartes en fonction d’une et une seule variable pour en faciliter la lecture à l’aide de type = &quot;property&quot;. Voici quelques exemples (notez la façon de diviser une page graphique en lignes et colonnes à l’aide de par(mfrow = )) en graphiques R de base, ensuite une boucle for réalise les six graphiques l’un après l’autre) : par(mfrow = c(2, 3)) for (var in c(&quot;size&quot;, &quot;mode&quot;, &quot;range&quot;, &quot;aspect&quot;, &quot;elongation&quot;, &quot;circularity&quot;)) plot(zoo_som, type = &quot;property&quot;, property = zoo_som$codes[[1]][, var], main = var, palette.name = viridis::inferno) Nous pouvons plus facilement inspecter les zones d’influence de différentes variables ciblées. Ici, size est une mesure de la taille des particules, mode est le niveau d’opacité moyen, range est la variation d’opacité (un range important indique que la particule a des parties très transparentes et d’autres très opaques), aspect est le rapport longueur/largeur, elongation est une indication de la complexité du périmètre de la particule, et circularity est sa forme plus ou moins circulaire. Pour une explication détaillée des 19 variables, faites ?zooplankton. 6.3.2.2 Regroupements Lorsque nous avons réalisé une CAH sur le jeu de données zooplankton, nous étions obligés de choisir deux variables parmi les 19 pour visualiser le regroupement sur un graphique nuage de points. C’est peu, et cela ne permet pas d’avoir une vision synthétique sur l’ensemble de l’information. Les méthodes d’ordination permettent de visualiser plus d’information sur un petit nombre de dimensions grâce aux techniques de réduction des dimensions qu’elles implémentent. Les cartes SOM offrent encore un niveau supplémentaire de raffinement. Nous pouvons considérer que chaque cellule est un premier résumé des données et nous pouvons effectuer ensuite une CAH sur ces cellules afin de dégager un regroupement et le visualiser sur la carte SOM. L’intérêt est que l’on réduit un jeu de données potentiellement très volumineux à un nombre plus restreint de cellules (ici 7x7 = 49), ce qui est plus “digeste” pour la CAH. Voici comment ça fonctionne : zoo_som_dist &lt;- dist(zoo_som$codes[[1]]) # Distance euclidienne entre cellules zoo_som_cah &lt;- hclust(zoo_som_dist, method = &quot;ward.D2&quot;, members = zoo_som_nb) Notre CAH a été réalisée ici avec la méthode D2 de Ward. L’argument members = est important. Il permet de pondérer chaque cellule en fonction du nombre d’individus qui y sont mappés. Toutes les cellules n’ont pas un même nombre d’individus, et nous souhaitons mettre plus de poids dans l’analyse aux cellules les plus remplies. Voici le dendrogramme : plot(zoo_som_cah, hang = -1) abline(h = 11.5, col = &quot;red&quot;) # Niveau de coupure proposé Les V1 à V49 sont les numéros de cellules. Nous pouvons couper à différents endroits dans ce dendrogramme, mais si nous décidons de distringuer les cinq groupes correspondants au niveau de coupure à une hauteur de 11,5 (comme sur le graphique), voici ce que cela donne : groupes &lt;- cutree(zoo_som_cah, h = 11.5) groupes # V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 # 1 1 1 1 2 2 2 1 1 1 2 2 2 2 1 1 1 2 # V19 V20 V21 V22 V23 V24 V25 V26 V27 V28 V29 V30 V31 V32 V33 V34 V35 V36 # 2 2 2 1 1 1 1 2 3 3 1 1 1 3 3 3 4 1 # V37 V38 V39 V40 V41 V42 V43 V44 V45 V46 V47 V48 V49 # 3 3 3 3 4 4 3 3 3 3 4 4 5 Visualisons ce découpage sur la carte SOM (l’argument bgcol = colorie le fond des cellules en fonction des groupes19, et add.cluster.boudaries() individualise des zones sur la carte en fonction du regroupement choisi). plot(zoo_som, type = &quot;mapping&quot;, pch = &quot;.&quot;, main = &quot;SOM zoo, 5 groupes&quot;, bgcol = RColorBrewer::brewer.pal(5, &quot;Set2&quot;)[groupes]) add.cluster.boundaries(zoo_som, clustering = groupes) Grâce à la topographie des variables que nous avons réalisée plus haut, nous savons que : le groupe vert bouteille en bas à gauche reprend les petites particules plutôt transparentes, le groupe orange en bas à droite est constituée de particules très contrastées avec des parties opaques et d’autres transparentes (range important), le groupe du dessus en bleu est constitué de particules petites à moyennes ayant une forme complexe (variable elongation), le groupe rose est constitué des particules moyennes à grandes, le groupe vert clair d’une seule cellule en haut à droite reprend les toutes grandes particules. Nous n’avons fait qu’effleurer les nombreuses possibilités des cartes auto-adaptatives SOM… Il est par exemple possible d’aller mapper des nouveaux individus dans cette carte (données supplémentaires), ou même de faire une classification sur base d’exemples (classification supervisée) que nous verrons au cours de Science des Données Biologiques III. Nous espérons que cela vous donnera l’envie et la curiosité de tester cette méthode sur vos données et d’explorer plus avant ses nombreuses possibilités. Pour en savoir plus Une explication très détaillée en français accompagnée de la résolution d’un exemple fictif dans R. Une autre explication détaillée en français avec exemple dans R. Si vous êtes aventureux, vous pouvez vous lancer dans la réimplémentation des graphiques du package kohonen en chartou ggplot2. Voici un bon point de départ (en anglais). A vous de jouer ! Réalisez le tutoriel afin de vérifier votre bonne compréhension de la som. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant les bases de R : BioDataScience2::run(&quot;06c_som&quot;) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel dans la console R. Complétez votre carnet de note par binôme sur le transect entre Nice et Calvi débuté lors du module 5. Lisez attentivement le README (Ce dernier a été mis à jour). Completez votre projet. Lisez attentivement le README. La dernière version du README est disponible via le lien suivant : https://github.com/BioDataScience-Course/spatial_distribution_zooplankton_ligurian_sea Attention : la variable transparency, contrairement à ce que son nom pourrait suggérer n’est pas une mesure de la transparence de l’objet, mais de l’aspect plus ou moins régulier et lisse de sa silhouette.↩ Nous avons choisi ici encore une autre palette de couleurs provenant du package RColorBrewer, voir ici.↩ "],
["acp-afc.html", "Module 7 ACP &amp; AFC", " Module 7 ACP &amp; AFC Objectifs Apprendre à réaliser une ordination de données quantitatives à l’aide de l’ACP. Savoir ordiner des variables qualitatives sous forme de tavbleaux cas par variables ou de tables de contingences à double entrée à l’aide de l’AFC. Prérequis Le module 6, et en particulier la partie sur le MDS doivent être assimilés avant d’attaquer le présent module. "],
["analyse-en-composantes-principales.html", "7.1 Analyse en composantes principales", " 7.1 Analyse en composantes principales Notre première approche d’ordination avec le MDS dans le précédent module nous a permis de comprendre l’intérêt de représenter des données multivariées sur des cartes. Malheureusement, les techniques itératives et basées sur les matrices de distances du MDS rendent cette technique peu propice pour analyser des gros jeux de données. En effet, le temps de calcul et le besoin en mémoire vive grandissent de manière exponentielle avec la taille des jeux de données. Heureusement, il existe aussi des techniques d’ordination qui se calculent plus facilement et plus rapidement sur de très gros jeux de données. L’Analyse en Composantes Principales ou ACP (“Principal Component Analysis” ou PCA en anglais) est une méthode de base qu’il est indispensable de connaitre et de comprendre. La plupart des autres techniques d’ordination plus sophistiquées sont des variante de l’ACP. Des relations linéaires sont suspectées entres les variables (si elles ne sont pas linéaires, penser à transformer les données auparavant pour les linéairser). Ces relations conduisent à une répartition des individus (le nuage de points) qui forme une structure que l’on cherchera à interpréter. Pour visualiser cette structure, les données sont simplifiées (réduites) de N variables à n (n &lt; N et n = 2 ou 3 généralement). La représentation sous forme d’un nuage de points s’appelle une carte. La réduction des dimensions se fait avec une perte minimale d’information au sens de la variance des données. 7.1.1 ACP dans SciViews::R L’ACP est facilitée dans SciViews::R, mais au stade actuel, tout le code nécessaire (en particulier pour réaliser les graphiques avec chart()) n’est pas encore complètement intégré dans les packages. Ainsi, vous pouvez copier-coller le code du chunk suivant au début de vos scripts ou dans un chunk de setup dans vos documenbts R Markdown/Notebook. SciViews::R() library(broom) # broom implements only methods for prcomp objects, not princomp, while pcomp # is compatible with princomp... but prcomp is simpler. So, conversion is easy as.prcomp &lt;- function(x, ...) UseMethod(&quot;as.prcomp&quot;) as.prcomp.default &lt;- function(x, ...) stop(&quot;No method to convert this object into a &#39;prcomp&#39;&quot;) as.prcomp.prcomp &lt;- function(x, ...) x as.prcomp.princomp &lt;- function(x, ...) structure(list(sdev = as.numeric(x$sdev), rotation = unclass(x$loadings), center = x$center, scale = x$scale, x = as.matrix(x$scores)), class = &quot;prcomp&quot;) # Comparison of pcomp() -&gt; as.prcomp() with prcomp() directly # Almost the same, only no rownames for x (is it important?) #iris_prcomp_pcomp &lt;- as.prcomp(pcomp(iris[, -5], scale = TRUE)) #iris_prcomp &lt;- prcomp(iris[, -5], scale = TRUE) # Now, broom methods can be defined simply by converting into prcomp objects augment.princomp &lt;- function(x, data = NULL, newdata, ...) if (missing(newdata)) { augment(as.prcomp(x), data = data, ...) } else { augment(as.prcomp(x), data = data, newdata = newdata, ...) } tidy.princomp &lt;- function(x, matrix = &quot;u&quot;, ...) tidy(as.prcomp(x), matrix = matrix, ...) # There is no glance.prcomp() method # There is a problem with pcomp() that returns a data.frame in scores, # while it is a matrix in the original princomp object. pca() corrects this pca &lt;- function(x, ...) { res &lt;- SciViews::pcomp(x, ...) # Change scores into a matrix res$scores &lt;- as.matrix(res$scores) res } scale_axes &lt;- function(data, aspect.ratio = 1) { range_x &lt;- range(data[, 1]) span_x &lt;- abs(max(range_x) - min(range_x)) range_y &lt;- range(data[, 2]) span_y &lt;- abs(max(range_y) - min(range_y)) if ((span_y / aspect.ratio) &gt; span_x) { # Adjust range_x span_x_2 &lt;- span_y / aspect.ratio / 2 range_x_mid &lt;- sum(range_x) / 2 range_x &lt;- c(range_x_mid - span_x_2, range_x_mid + span_x_2) } else { # Adjust range_y span_y_2 &lt;- span_x * aspect.ratio / 2 range_y_mid &lt;- sum(range_y) / 2 range_y &lt;- c(range_y_mid - span_y_2, range_y_mid + span_y_2) } list(x = range_x, y = range_y) } autoplot.pcomp &lt;- function(object, type = c(&quot;screeplot&quot;, &quot;altscreeplot&quot;, &quot;loadings&quot;, &quot;correlations&quot;, &quot;scores&quot;, &quot;biplot&quot;), choices = 1L:2L, name = deparse(substitute(object)), ar.length = 0.1, circle.col = &quot;gray&quot;, col = &quot;black&quot;, fill = &quot;gray&quot;, scale = 1, aspect.ratio = 1, repel = FALSE, labels, title, xlab, ylab, ...) { type = match.arg(type) if (missing(title)) title &lt;- paste(name, type, sep = &quot; - &quot;) contribs &lt;- paste0(names(object$sdev), &quot; (&quot;, round((object$sdev^2/object$totdev^2) * 100, digits = 1), &quot;%)&quot;)[choices] scores &lt;- as.data.frame(object$scores[, choices]) names(scores) &lt;- c(&quot;x&quot;, &quot;y&quot;) if (!missing(labels)) { if (length(labels) != nrow(scores)) stop(&quot;You must provide a character vector of length &quot;, nrow(scores), &quot; for &#39;labels&#39;&quot;) scores$labels &lt;- labels } else {# Default labels are row numbers scores$labels &lt;- 1:nrow(scores) } lims &lt;- scale_axes(scores, aspect.ratio = aspect.ratio) if (!missing(col)) { if (length(col) != nrow(scores)) stop(&quot;You must provide a vector of length &quot;, nrow(scores), &quot; for &#39;col&#39;&quot;) scores$color &lt;- col scores_formula &lt;- y ~ x %col=% color %label=% labels } else { if (missing(labels)) { scores_formula &lt;- y ~ x %label=% labels } else { scores_formula &lt;- y ~ x %col=% labels %label=% labels } } res &lt;- switch(type, screeplot = object %&gt;.% # Classical screeplot tidy(., &quot;pcs&quot;) %&gt;.% chart(data = ., std.dev^2 ~ PC) + geom_col(col = col, fill = fill) + labs(y = &quot;Variances&quot;, title = title), altscreeplot = object %&gt;.% # screeplot represented by dots and lines tidy(., &quot;pcs&quot;) %&gt;.% chart(data = ., std.dev^2 ~ PC) + geom_line(col = col) + geom_point(col = &quot;white&quot;, fill = col, size = 2, shape = 21, stroke = 3) + labs(y = &quot;Variances&quot;, title = title), loadings = object %&gt;.% # Plots of the variables tidy(., &quot;variables&quot;) %&gt;.% spread(., key = PC, value = value) %&gt;.% #rename_if(., is.numeric, function(x) paste0(&quot;PC&quot;, x)) %&gt;.% select(., c(1, choices + 1)) %&gt;.% set_names(., c(&quot;labels&quot;, &quot;x&quot;, &quot;y&quot;)) %&gt;.% chart(data = ., y ~ x %xend=% 0 %yend=% 0 %label=% labels) + annotate(&quot;path&quot;, col = circle.col, x = cos(seq(0, 2*pi, length.out = 100)), y = sin(seq(0, 2*pi, length.out = 100))) + geom_hline(yintercept = 0, col = circle.col) + geom_vline(xintercept = 0, col = circle.col) + geom_segment(arrow = arrow(length = unit(ar.length, &quot;inches&quot;), ends = &quot;first&quot;)) + ggrepel::geom_text_repel(hjust = &quot;outward&quot;, vjust = &quot;outward&quot;) + coord_fixed(ratio = 1) + labs(x = contribs[1], y = contribs[2], title = title), correlations = object %&gt;.% # Correlations plot Correlation(.) %&gt;.% as_tibble(., rownames = &quot;labels&quot;) %&gt;.% select(., c(1, choices + 1)) %&gt;.% set_names(., c(&quot;labels&quot;, &quot;x&quot;, &quot;y&quot;)) %&gt;.% chart(data = ., y ~ x %xend=% 0 %yend=% 0 %label=% labels) + annotate(&quot;path&quot;, col = circle.col, x = cos(seq(0, 2*pi, length.out = 100)), y = sin(seq(0, 2*pi, length.out = 100))) + geom_hline(yintercept = 0, col = circle.col) + geom_vline(xintercept = 0, col = circle.col) + geom_segment(arrow = arrow(length = unit(ar.length, &quot;inches&quot;), ends = &quot;first&quot;)) + ggrepel::geom_text_repel(hjust = &quot;outward&quot;, vjust = &quot;outward&quot;) + coord_fixed(ratio = 1) + labs(x = contribs[1], y = contribs[2], title = title), scores = scores %&gt;.% # Plot of the individuals chart(data = ., scores_formula) + geom_hline(yintercept = 0, col = circle.col) + geom_vline(xintercept = 0, col = circle.col) + coord_fixed(ratio = 1, xlim = lims$x, ylim = lims$y, expand = TRUE) + labs(x = contribs[1], y = contribs[2], title = title) + theme(legend.position = &quot;none&quot;), biplot = object %&gt;.% # Biplot using ggfortify function as.prcomp(.) %&gt;.% ggfortify:::autoplot.prcomp(., x = choices[1], y = choices[2], scale = scale, size = -1, label = TRUE, loadings = TRUE, loadings.label = TRUE) + geom_hline(yintercept = 0, col = circle.col) + geom_vline(xintercept = 0, col = circle.col) + theme_sciviews() + labs(x = contribs[1], y = contribs[2], title = title), stop(&quot;Unrecognized type, must be &#39;screeplot&#39;, &#39;altscreeplot&#39;, loadings&#39;, &#39;correlations&#39;, &#39;scores&#39; or &#39;biplot&#39;&quot;) ) if (type == &quot;scores&quot;) { if (isTRUE(repel)) { res &lt;- res + geom_point() + ggrepel::geom_text_repel() } else {# Use text res &lt;- res + geom_text() } } if (!missing(xlab)) res &lt;- res + xlab(xlab) if (!missing(ylab)) res &lt;- res + ylab(ylab) res } chart.pcomp &lt;- function(data, choices = 1L:2L, name = deparse(substitute(data)), ..., type = NULL, env = parent.frame()) autoplot.pcomp(data, choices = choices, name = name, ..., type = type, env = env) class(chart.pcomp) &lt;- c(&quot;function&quot;, &quot;subsettable_type&quot;) 7.1.2 Indiens diabétiques Les indiens Pimas sont des amérindiens originaires du nord du Mexique qui sont connus pour compter le plus haut pourcentage d’obèses et de diabétiques de toutes les éthnies. Ils ont fait l’objet de plusieurs études scientifiques d’autant plus que les Pimas en Arizona développent principalement cette obésité et ce diabète, alors que les Pimas mexicains les ont plus rarement. Il est supposé que leur mode de vie différent aux Etats_Units pourrait en être la raison. Voici un jeu de données qui permet d’explorer un peu ceci : pima &lt;- read(&quot;PimaIndiansDiabetes2&quot;, package = &quot;mlbench&quot;) pima # # A tibble: 768 x 9 # pregnant glucose pressure triceps insulin mass pedigree age diabetes # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; # 1 6 148 72 35 NA 33.6 0.627 50 pos # 2 1 85 66 29 NA 26.6 0.351 31 neg # 3 8 183 64 NA NA 23.3 0.672 32 pos # 4 1 89 66 23 94 28.1 0.167 21 neg # 5 0 137 40 35 168 43.1 2.29 33 pos # 6 5 116 74 NA NA 25.6 0.201 30 neg # 7 3 78 50 32 88 31 0.248 26 pos # 8 10 115 NA NA NA 35.3 0.134 29 neg # 9 2 197 70 45 543 30.5 0.158 53 pos # 10 8 125 96 NA NA NA 0.232 54 pos # # … with 758 more rows Ce jeu de données contient des vlaeurs manquantes. Le graphique suivant permet de visualiser l’importance des “dégâts” : naniar::vis_miss(pima) Moins de 10% des données sont manquantes, et c’est principalement dans les variables insulin et triceps. Si nous souhaitons un tableau sans variables manquantes, nous pouvons décider d’éliminer des lignes et ou des colonnes (variables), mais ici nous souhaitons garder toutes les variables et réduisons donc uniquement le nombre de lignes avec la fonction drop_na(). pima &lt;- drop_na(pima) pima # # A tibble: 392 x 9 # pregnant glucose pressure triceps insulin mass pedigree age diabetes # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; # 1 1 89 66 23 94 28.1 0.167 21 neg # 2 0 137 40 35 168 43.1 2.29 33 pos # 3 3 78 50 32 88 31 0.248 26 pos # 4 2 197 70 45 543 30.5 0.158 53 pos # 5 1 189 60 23 846 30.1 0.398 59 pos # 6 5 166 72 19 175 25.8 0.587 51 pos # 7 0 118 84 47 230 45.8 0.551 31 pos # 8 1 103 30 38 83 43.3 0.183 33 neg # 9 1 115 70 30 96 34.6 0.529 32 pos # 10 3 126 88 41 235 39.3 0.704 27 neg # # … with 382 more rows Notre tableau est presque amputé de la moitié, mais il nous reste tout de même encore 392 cas, soit assez pour notre analyse. Avant de nous lancer dans une ACP, nous devons décrire les données, repérer les variables quantitatives d’intérêt, et synthétiser les corrélations linéaires (coefficients de corrélation de Pearson) entre ces variables. skimr::skim(pima) # Skim summary statistics # n obs: 392 # n variables: 9 # # ── Variable type:factor ──────────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n n_unique top_counts ordered # diabetes 0 392 392 2 neg: 262, pos: 130, NA: 0 FALSE # # ── Variable type:numeric ─────────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n mean sd p0 p25 p50 p75 # age 0 392 392 30.86 10.2 21 23 27 36 # glucose 0 392 392 122.63 30.86 56 99 119 143 # insulin 0 392 392 156.06 118.84 14 76.75 125.5 190 # mass 0 392 392 33.09 7.03 18.2 28.4 33.2 37.1 # pedigree 0 392 392 0.52 0.35 0.085 0.27 0.45 0.69 # pregnant 0 392 392 3.3 3.21 0 1 2 5 # pressure 0 392 392 70.66 12.5 24 62 70 78 # triceps 0 392 392 29.15 10.52 7 21 29 37 # p100 hist # 81 ▇▂▂▁▁▁▁▁ # 198 ▁▅▇▆▅▃▂▂ # 846 ▇▆▂▁▁▁▁▁ # 67.1 ▂▆▇▅▂▁▁▁ # 2.42 ▇▆▃▁▁▁▁▁ # 17 ▇▃▂▁▁▁▁▁ # 110 ▁▁▂▆▇▆▁▁ # 63 ▃▆▇▇▆▃▁▁ Nous avons une variable facteur diabetes à exclure de l’analyse, mais la variable pregnant, est une variable numérique discrète (nombre d’enfants portés). Nous l’éliminerons aussi de l’analyse. La fonction correlation() du package SciViews nous permet d’inspecter les corrélations entre les variables choisies (donc toutes à l’exception de pregnant et diabetes qui ne sont pas quantitatives continues) : pima_cor &lt;- correlation(pima[, 2:8]) knitr::kable(pima_cor, digits = 2) glucose pressure triceps insulin mass pedigree age glucose 1.00 0.21 0.20 0.58 0.21 0.14 0.34 pressure 0.21 1.00 0.23 0.10 0.30 -0.02 0.30 triceps 0.20 0.23 1.00 0.18 0.66 0.16 0.17 insulin 0.58 0.10 0.18 1.00 0.23 0.14 0.22 mass 0.21 0.30 0.66 0.23 1.00 0.16 0.07 pedigree 0.14 -0.02 0.16 0.14 0.16 1.00 0.09 age 0.34 0.30 0.17 0.22 0.07 0.09 1.00 plot(pima_cor) Quelques corrélations positives d’intensités moyennes se dégagent ici, notamment entre mass et triceps (épaisseur du pli cutané au niveau du triceps), ainsi qu’entre glucose (taux de glucose dans le sang), insulin (taux d’insuline dans le sang) et age. Par contre, la pression artérielle (pressure) et le pedigree (variable qui quantifie la susceptibilité au diabète en fonction de la parenté) semblent peu corrélés avec les autres variables. L’ACP est en fait équivalente à une Analyse en Coordonnées Principales sur une matrice de distances euclidiennes (MDS métrique), mais en plus efficace en terme de calculs. Nous pouvons donc nous lancer dans l’analyse et en comprendre les résultats en gardant ceci à l’esprit. Nous utiliserons la fonction pca() qui prend un argument data = et une formule du type ~ var1 + var2 + .... + varn, ou plus simplement, directement un tableau contenant uniquement les variables à analyser comme argument unique. Comme les différentes variables sont mesurées dans des unités différentes, nous devons les standardiser (écart type ramené à un pour toutes). Ceci est réalisé par la fonction pca() en lui indiquant scale = TRUE. Donc : pima_pca &lt;- pca(data = pima, ~ glucose + pressure + triceps + insulin + mass + pedigree + age, scale = TRUE) Ou alors, nous sélectionnons les variables d’intérêt avec select() et appliquons pca() directement sur ce tableau, ce qui donnera le même résultat. pima %&gt;.% select(., glucose:age) %&gt;.% pca(., scale = TRUE) -&gt; pima_pca Le nuage de points dans l’espace initial à sept dimensions a été centré (origine ramenée au centre de gravité du nuage de points = moyenne des variables) par l’ACP. Ensuite une rotation des axes a été réalisée pour orienter son plus grand axe selon un premier axe principal 1 ou PC1 . Ensuite PC2 est construit orthogonal au premier et dans la seconde direction de plus grande variabilité du nuage de points, et ainsi de suite pour les autres axes. Ainsi les axes PC1, PC2, PC3, … représentent une part de variance de plus en plus faible par rapport à la variance totale du jeu de données. Ceci est présenté dans le résumé : summary(pima_pca) # Importance of components (eigenvalues): # PC1 PC2 PC3 PC4 PC5 PC6 PC7 # Variance 2.412 1.288 1.074 0.878 0.6389 0.399 0.3098 # Proportion of Variance 0.345 0.184 0.153 0.126 0.0913 0.057 0.0443 # Cumulative Proportion 0.345 0.529 0.682 0.807 0.8988 0.956 1.0000 # # Loadings (eigenvectors, rotation matrix): # PC1 PC2 PC3 PC4 PC5 PC6 PC7 # glucose 0.441 -0.455 -0.198 0.736 # pressure 0.329 0.101 -0.613 0.206 0.654 -0.171 # triceps 0.439 0.488 -0.367 -0.644 # insulin 0.402 -0.418 0.263 -0.388 0.123 -0.642 -0.129 # mass 0.446 0.506 -0.181 0.711 # pedigree 0.198 0.625 0.711 0.251 # age 0.325 -0.337 -0.384 0.471 -0.592 -0.168 0.179 Le premier tableau Importance of components (eigenvalues): montre la part de variance présentée sur chacun des sept axes de l’ACP (PC1, PC2, …, PC7). Le fait qu’il s’agit de valeurs propres (eigenvalues en anglais) apparaitra plus clair lorsque vous aurez lu les explications détaillées plus bas. Ces parts de variance s’additionnent pour donner la variance totale du nuage de points dans les sept dimensions (propriété d’additivité des variances). Pour facilité la lecture, la Proportion de Variance en % est reprise également, ainsi que les proportions cumulées. Ainsi, les deux premiers axes de l’ACP capturent ici 53% de la variance totale. Et il faudrait considérer les cinq premiers axes pour capturer 90% de la variance totale. Cependant, les trois premiers axes cumulent tout de même plus des 2/3 de la variance. Nous pouvons restreindre notre analyse à ces trois axes-là. Le second tableau Loadings (eigenvectors, rotation matrix): est la matrice de transformation des coordonnées initiales sur les lignes en coordonnées PC1 à PC7 en colonnes. Nous pouvons y lire l’importante des variables initiales sur les axes de l’ACP. Par exemple, l’axe PC3 contraste essentiellement pressure et pedigree. Le graphique des éboulis sert à visualiser la “chute” de la variance d’un axe principal à l’autre, et aide à choisir le nombre d’axes à conserver (espace à dimensions réduites avec perte minimale d’information). Deux variantes en diagramme en barres versticales chart$screeplot() ou chart$scree() ou sous forme d’une ligne brisée chart$altscree() sont disponibles : chart$scree(pima_pca, fill = &quot;cornsilk&quot;) chart$altscree(pima_pca) La diminution est importante entre le premier et le second axe, mais plus progressive ensuite. Ceci traduit une structure plus complexe dans les données qui ne se réduit pas facilement à un très petit nombre d’axes. Nous pouvons visualiser le premier plan principal constitué par PC1 et PC2, tout en gardant à l’esprit que seulement 53% de la variance totale y est capturée. Donc, nous pouvons nous attendre à des déformations non négligeables des données dans ce plan, et d’autres aspects qui n’y sont pas (correctement) représentés. Nous verrons qu’il est porteur, toutefois, d’information utile. Deux types de représentations peuvent être réalisées à partir d’ici : la représentation dans l’espace des variables, et la représentation complémentaire dans l’espace des individus. Ces deux représentations sont complémentaires et s’analysent conjointement. L’espace des variables représente les axes initiaux projettés comme des ombres dans le plan choisi de l’ACP (rappelez-vous l’analogie avec les ombres chinoises). Il se réalise à l’aide de chart$loadings(). Par exemple pour PC1 et PC2 nous indiquons choices = c(1, 2) (ou rien du tout, puisque ce sont les valeurs par défaut)) : chart$loadings(pima_pca, choices = c(1, 2)) Ce graphique s’interpète comme suit : Plus la norme (longueur) du vecteur qui représente une variable est grande et se rapporche de un (matérialisé par le cer cle gris), plus la variable est bien représentée dans le plan choisi. On évitera d’interpréter ici les variables qui ont des normes petites, comme pedigree ou pressure. Des vecteurs qui pointent dans la même direction représentent des variables directement corrélés entre elles. C’est le cas de glucose, insulin et aged’une part, et par ailleurs aussi de mass et triceps. Des vecteurs qui pointent en directions opposées représentent des variables inversément proportionnelles. Il n’y en a pas ici. Des vecteurs orthogonaux représentent des variables non corrélées entre elles. ainsi le groupoe glucose/insulin/age n’est pas corrélé avec le groupe mass/triceps Les PCs sont orientés en fonction des variables initiales, ou à défaut, les zones du graphique sont orientés. Ici, les gros sont dans le haut à droite du graphique, alors que ceux qui sont agés, et ont beaucoup de sucre et d’insuline dans le sang sont en bas à droite. A l’opposé, on trouve les plus maigres en bas à gauche et les jeunes ayant moins de glucose et d’insuline dans le sang en haut à gauche du graphique. Cela donne déjà une vision synthétique des différentes corrélations entre la variables. Naturellement, on peut très bien choisir d’autres axes, pour peu qu’ils représentent une part de variance relativement importante. Par exemple, ici, nous pouvons représente le plan constitué par PC1 et PC3, puisque nous avons décidé de retenir les 3 premiers axes : chart$loadings(pima_pca, choices = c(1, 3)) Nous voyons que pedigree et pressure (inversément proportionnels) sont bien mieux représentés le long de PC3. Ici l’axe PC3 est plus facile à orienter : en haut les pédigrées élevés et les pressions qartérielles basses, et en bas le contraire. Nous avons déjà lu cette informatioin dans le tableau des vecteurs propres de summary(). Le graphice entre PC2 et PC3 complète l’analyse, mais n’apportant rien de plus, il peut être typiquement éliminé de votre rapport. chart$loadings(pima_pca, choices = c(2, 3)) La seconde représentation se fait dans l’espace des individus. Ici, nous allons projeter les points relatifs à chaque individu dans le plan de l’ACP choisi. Cela se réalise à l’aide de chart$scores() (l’aspect ratio est le rapport hauteur/largeur peut s’adapter) : chart$scores(pima_pca, choices = c(1, 2), aspect.ratio = 3/5) Ce graphique est peu lisible tel quel. Généralement, nous représentons d’autres informations utiles sous forme de labels et ou de couleurs différentes. Nous pouvons ainsi contraster les individus qui ont le diabète de ceux qui ne l’ont pas sur ce graphique et aussi ajouter des ellipses de confiance à 95% autour des deux groupes pour aider à la cerner à l’aide de stat_ellipse() : chart$scores(pima_pca, choices = c(1, 2), labels = pima$diabetes) + stat_ellipse() Ce graphique est nettement plus intéressant. Il s’interprète comme suit : Nous savons que les individus plus âgés et ayant plus de glucose et d’insuline dans le sang sont dans le bas à droite du graphique. Or le groupe des diabétique, s’il ne se détache pas complètement tend à s’étaler plus dans cette région. A l’inverse, le groupe des non diabétiques s’étale vers la gauche, c’est-à-dire dans une région reprenant les individus les plus jeunes et les moins gros. Le graphique entre PC1 et PC3 (analyse du troisième axe) donne ceci : chart$scores(pima_pca, choices = c(1, 3), labels = pima$diabetes) + stat_ellipse() Ici, la séparation se fait essentiellement sur l’axe horizontal (PC1). Donc, les différentes de pédigrée (élevé dans le haut du graphique) et de pression artérielle (élevée dans le bas du graphique) semblent être moins liés au diabète. Le graphique PC3 versus PC2 peut aussi être réalisé, mais il n’apporte rien de plus (et en pratique, nous l’éliminerions d’un rapport). chart$scores(pima_pca, choices = c(2, 3), labels = pima$diabetes) + stat_ellipse() Etant donné que les deux graphiques (variables et individus) s’interprètent conjointement, nous pourrions être tentés de les superposer, cela s’appelle un biplot. Mais se pose alors un problème : celui de mettre à l’échelle les deux représentations pour qu’elles soient cohérentes entre elles. Ceci n’est pas facile et différentes représentations coexistent. L’argument scale = de la fonction chart$biplot() permet d’utiliser différentes mises à l’échelle. Enfin, ce type de graphique tend à être souvent bien trop encombré. Il est donc plus difficile à lire que les deux graphiques des variables et individus séparés. Voici ce que cela donne pour notre jeu de données exemple : chart$biplot(pima_pca) Bien moins lisible, en effet ! 7.1.3 Biométrie d’oursin Analysons à présent un autre jeu de données qui nous montrera l’importance de la transformation (linéarisation), du choix de réduire ou non (argument scale =), et l’effet d’un effet saturant, et comment s’en débarrasser. Il s’agit de la biométrie effectuée sur deux populations de l’oursin violet Paracentrotus lividus, une en élevage et une oautre provenant du milieu naturel. Nous avons abondamment utilisé ce jeu de données en SDD I dans la section visualisation. Nous le connaissons bien, mais reprenons certains éléments essentiels ici… urchin &lt;- read(&quot;urchin_bio&quot;, package = &quot;data.io&quot;, lang = &quot;FR&quot;) urchin # # A tibble: 421 x 19 # origin diameter1 diameter2 height buoyant_weight weight solid_parts # &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 Pêche… 9.9 10.2 5 NA 0.522 0.478 # 2 Pêche… 10.5 10.6 5.7 NA 0.642 0.589 # 3 Pêche… 10.8 10.8 5.2 NA 0.734 0.677 # 4 Pêche… 9.6 9.3 4.6 NA 0.370 0.344 # 5 Pêche… 10.4 10.7 4.8 NA 0.610 0.559 # 6 Pêche… 10.5 11.1 5 NA 0.610 0.551 # 7 Pêche… 11 11 5.2 NA 0.672 0.605 # 8 Pêche… 11.1 11.2 5.7 NA 0.703 0.628 # 9 Pêche… 9.4 9.2 4.6 NA 0.413 0.375 # 10 Pêche… 10.1 9.5 4.7 NA 0.449 0.398 # # … with 411 more rows, and 12 more variables: integuments &lt;dbl&gt;, # # dry_integuments &lt;dbl&gt;, digestive_tract &lt;dbl&gt;, # # dry_digestive_tract &lt;dbl&gt;, gonads &lt;dbl&gt;, dry_gonads &lt;dbl&gt;, # # skeleton &lt;dbl&gt;, lantern &lt;dbl&gt;, test &lt;dbl&gt;, spines &lt;dbl&gt;, # # maturity &lt;int&gt;, sex &lt;fct&gt; Ici aussi nous avons des valeurs manquantes : naniar::vis_miss(urchin) Ces valeurs manquantes sont rassemblées essentiellement dans les variables buoyant_weight, dry_integuments, les mesures relatives au squelette (skeleton, lantern, test et spines), et surtout au niveau de sex (impossible de déterminer le sexe des individus les plus jeunes). Si nous éliminons purement et simplement les lignes qui ont au moins une valeur manquante, nous perdons tous les individus jeunes, et c’est dommage. Nous allons donc d’abord éliminer les variables sex, ainsi que les quatres variables liées au squelette. Dans un second temps, nous appliquerons drop_na() sur ce qui reste : urchin %&gt;.% select(., -(skeleton:spines), -sex) %&gt;.% drop_na(.) -&gt; urchin2 urchin2 # # A tibble: 319 x 14 # origin diameter1 diameter2 height buoyant_weight weight solid_parts # &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 Pêche… 16.7 16.8 8.4 0.588 2.58 2.04 # 2 Pêche… 19.9 20 9.2 1.10 4.26 3.66 # 3 Pêche… 19.9 19.2 8.5 0.629 2.93 2.43 # 4 Pêche… 19.3 19.8 10.2 0.781 3.71 3.09 # 5 Pêche… 18.8 20 9.3 0.761 3.59 2.99 # 6 Pêche… 21.5 20.9 9.6 1.13 4.98 4.42 # 7 Pêche… 17.4 16.5 7.8 0.477 2.33 1.97 # 8 Pêche… 21 21.2 10.8 1.23 5.4 4.55 # 9 Pêche… 17.8 18.8 8.6 0.548 2.58 2.07 # 10 Pêche… 19.7 19.6 9.7 0.862 3.59 3.08 # # … with 309 more rows, and 7 more variables: integuments &lt;dbl&gt;, # # dry_integuments &lt;dbl&gt;, digestive_tract &lt;dbl&gt;, # # dry_digestive_tract &lt;dbl&gt;, gonads &lt;dbl&gt;, dry_gonads &lt;dbl&gt;, # # maturity &lt;int&gt; Il nous reste 319 lignes des 421 initiales. Nous n’avons perdu qu’un quart des données, tout en nous privant seulement de quatres variables quantitatives liées au squelette (sexétant une variable qualitative, elle ne peut de toutes façons pas être introduite dans l’analyse, mais elle aurait pu servir pour colorer les individus). skimr::skim(urchin2) # Skim summary statistics # n obs: 319 # n variables: 14 # # ── Variable type:factor ──────────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n n_unique top_counts ordered # origin 0 319 319 2 Cul: 188, Pêc: 131, NA: 0 FALSE # # ── Variable type:integer ─────────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n mean sd p0 p25 p50 p75 p100 hist # maturity 0 319 319 0.37 0.71 0 0 0 0 2 ▇▁▁▁▁▁▁▂ # # ── Variable type:numeric ─────────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n mean sd p0 p25 p50 # buoyant_weight 0 319 319 4.27 3.84 0.31 1.35 3.18 # diameter1 0 319 319 32.78 11.71 14.6 23.25 31.5 # diameter2 0 319 319 32.71 11.67 15 23.45 31.6 # digestive_tract 0 319 319 1.9 2.03 0.034 0.45 1.21 # dry_digestive_tract 0 319 319 0.23 0.21 0.015 0.075 0.17 # dry_gonads 0 319 319 0.51 0.82 0 0.029 0.17 # dry_integuments 0 319 319 7.16 6.3 0.58 2.22 5.42 # gonads 0 319 319 1.72 2.65 0 0.1 0.63 # height 0 319 319 16.78 6.25 7.3 11.1 16.2 # integuments 0 319 319 12.32 10.64 1.09 4 9.4 # solid_parts 0 319 319 16.52 15.27 1.46 4.96 11.73 # weight 0 319 319 21.8 21.37 1.61 6.08 15.25 # p75 p100 hist # 5.67 17.73 ▇▃▂▂▁▁▁▁ # 39.65 65.6 ▆▇▆▆▃▃▂▁ # 39.6 65.6 ▇▇▆▆▃▃▂▁ # 2.54 10.37 ▇▃▂▁▁▁▁▁ # 0.31 1.02 ▇▅▂▁▁▁▁▁ # 0.64 5 ▇▂▁▁▁▁▁▁ # 9.42 28.8 ▇▃▂▂▁▁▁▁ # 2.2 15.93 ▇▂▁▁▁▁▁▁ # 21.5 32.2 ▇▆▆▆▅▃▂▁ # 16.07 47.22 ▇▃▃▁▁▁▁▁ # 21.69 73.14 ▇▅▂▂▁▁▁▁ # 28.14 100.51 ▇▅▁▁▁▁▁▁ Nous avons 12 variables quatitatives continues. Notez la distribution très asymétrique et similaire (voir colonne hist) de toutes ces variables. les variables origin et maturity ne pourront pas être utilisées, mais seront éventuellement utiles pour colorer les points dans nos graphiques. Qu’en est-il des corrélations entre les 12 variables ? urchin2_cor &lt;- correlation(urchin2[, 2:13]) knitr::kable(urchin2_cor, digits = 2) diameter1 diameter2 height buoyant_weight weight solid_parts integuments dry_integuments digestive_tract dry_digestive_tract gonads dry_gonads diameter1 1.00 1.00 0.98 0.95 0.96 0.96 0.97 0.96 0.91 0.93 0.80 0.79 diameter2 1.00 1.00 0.97 0.95 0.96 0.96 0.97 0.96 0.91 0.93 0.80 0.79 height 0.98 0.97 1.00 0.93 0.92 0.93 0.94 0.93 0.88 0.91 0.76 0.75 buoyant_weight 0.95 0.95 0.93 1.00 0.99 0.99 0.99 1.00 0.92 0.94 0.88 0.87 weight 0.96 0.96 0.92 0.99 1.00 0.99 0.99 0.99 0.95 0.96 0.88 0.87 solid_parts 0.96 0.96 0.93 0.99 0.99 1.00 0.99 0.99 0.95 0.95 0.91 0.90 integuments 0.97 0.97 0.94 0.99 0.99 0.99 1.00 1.00 0.93 0.95 0.87 0.85 dry_integuments 0.96 0.96 0.93 1.00 0.99 0.99 1.00 1.00 0.92 0.94 0.87 0.86 digestive_tract 0.91 0.91 0.88 0.92 0.95 0.95 0.93 0.92 1.00 0.98 0.81 0.81 dry_digestive_tract 0.93 0.93 0.91 0.94 0.96 0.95 0.95 0.94 0.98 1.00 0.82 0.82 gonads 0.80 0.80 0.76 0.88 0.88 0.91 0.87 0.87 0.81 0.82 1.00 0.99 dry_gonads 0.79 0.79 0.75 0.87 0.87 0.90 0.85 0.86 0.81 0.82 0.99 1.00 plot(urchin2_cor) Toutes les corrélations sont positives, et certaines sont très élevées. Cela indique que plusieurs variables sont (pratiquement complètement) redondantes, par exemple, diameter1 et diameter2. Un effet principal semble dominer. Si nous refaisons quelques graphiques, nous nous rappelons que les relations ne sont pas linéaires, par exemple, entre diameter1 et weight : chart(data = urchin2, weight ~ diameter1) + geom_point() Ce type de relation, dite allométrique se linéarise très bien en effectuant une transformation double-log, comme nous pouvons le constater sur le graphique suivant : chart(data = urchin2, log(weight) ~ log(diameter1)) + geom_point() Il est crucial de bien nettoyer son jeu de données avant une ACP, et aussi, de vérifier que les relations sont linéaires. Sinon il faut transformer les données de manière appropriée. Rappelez-vous que l’ACP s’intéresse aux corrélations linéaires entre vos variables. Attention toutefois à la transformation logarithmique appliquée sur des données qui peuvent contenir des zéros (par exemple, gonads ou dry_gonads). Dans ce cas, la transformartion logarithme(x + 1) réalisée avec la fonction log1p() est plus indiquée. Nous allons ici trtansformer toutes les variables en log(x + 1). C’est assez fastidieux à faire avec mutate(), mais nous pouvons l’utiliser directemnt sur le tableau entier réduit aux variables quantitatives continues seules lors de l’appel à pca() comme suit : urchin2 %&gt;.% select(., -origin, -maturity) %&gt;.% # Elimine les variables non quantitatives log1p(.) %&gt;.% # Transforme toutes les autres en log(x + 1) pca(., scale = TRUE) -&gt; urchin2_pca # Effectue l&#39;ACP après standardisation Nous avons standardisé les données puisqu’elles sont mesurées dans des unités différentes (longueurs en mm, masses en g). Voici ce que donne notre ACP : summary(urchin2_pca) # Importance of components (eigenvalues): # PC1 PC2 PC3 PC4 PC5 PC6 # Variance 11.219 0.5010 0.1813 0.03862 0.02601 0.01657 # Proportion of Variance 0.935 0.0418 0.0151 0.00322 0.00217 0.00138 # Cumulative Proportion 0.935 0.9767 0.9918 0.99503 0.99720 0.99858 # PC7 PC8 PC9 PC10 PC11 PC12 # Variance 0.00931 0.00336 0.00210 0.00108 0.00082 0.00034 # Proportion of Variance 0.00078 0.00028 0.00017 0.00009 0.00007 0.00003 # Cumulative Proportion 0.99936 0.99964 0.99981 0.99990 0.99997 1.00000 # # Loadings (eigenvectors, rotation matrix): # PC1 PC2 PC3 PC4 PC5 PC6 PC7 # diameter1 0.295 -0.162 -0.441 0.237 -0.174 0.177 # diameter2 0.295 -0.166 -0.449 0.249 -0.178 0.157 # height 0.291 -0.218 0.154 -0.106 -0.902 # buoyant_weight 0.296 0.120 0.509 0.124 # weight 0.296 -0.149 # solid_parts 0.297 -0.106 0.127 0.234 # integuments 0.296 -0.159 0.157 0.153 0.125 # dry_integuments 0.296 -0.115 0.160 0.455 0.114 # digestive_tract 0.288 -0.571 -0.187 0.485 -0.519 # dry_digestive_tract 0.283 -0.702 0.217 -0.278 0.513 # gonads 0.271 0.568 0.226 0.575 0.430 # dry_gonads 0.259 0.697 -0.104 -0.465 -0.453 # PC8 PC9 PC10 PC11 PC12 # diameter1 0.242 -0.706 # diameter2 0.124 0.688 0.263 # height # buoyant_weight 0.530 0.265 -0.504 # weight -0.145 0.116 -0.912 # solid_parts -0.594 0.216 0.638 # integuments -0.396 0.148 -0.702 -0.371 # dry_integuments 0.105 0.128 -0.133 0.774 # digestive_tract 0.201 # dry_digestive_tract -0.161 # gonads 0.143 # dry_gonads -0.111 Whaaa ! Plus de 93% de la variance représentée sur le premier axe. Ça parait parfait ! Voici le graphique des éboulis : chart$scree(urchin2_pca) Ne vous réjousissez pas trop vite. Nous avons ici un effet saturant lié au fait que toutes les variables sont positivement corrélées entre elles. Cet effet est évident. Ici, c’est la taille. Nous allons conclure que plus un oursin est gros, plus ses dimensions et ses masses sont importante. C’est trivial et d’un intérêt très limité, avouons-le. Puisque l’ACP optimise la variance sur le premier axe, un effet saturant aura tendance à occulter d’autres effets intéressants. Nous pouvons nous en débarrasser en identifiant une des variables représentant le mieux cet effet, et en calculant les ratios entre toutes les autres variables et celle-là. Ainsi, nous passons de quantification de la taille sur toutes les variables à des ratios qui quantifient beaucoup mieux des effets de forme plus subtils. Notez aussi les valeurs relativement faibles, mais homogènes de toutes les variables sur l’axe PC1 dans le tableau des vecteurs propres, avec des valeurs comprises entre 0,26 et 0,30. Le graphique des variables est également très moche dans le premier plan de l’ACP, même si un effet différent relatif aux gonades apparait tout de même sur l’axe PC2, il ne compte que pour 4,2% de la variance totale : chart$loadings(urchin2_pca) Recommençons tout de suite l’analyse en éliminant l’effet saturant. Nous pourrons considérer comme référence de la taille, par exemple, la masse immergée (buoyant weight) connue comme étant une mesure pouvant être mesurée très précisément. Elle fait partie des variables les mieux corrélées sur l’axe PC1, représentant ainsi très bien cet effet saturant que nous voulons éliminer. Voici notre calcul : urchin2 %&gt;.% select(., -origin, -maturity, -buoyant_weight) %&gt;.% # Elimination des variables inutiles (. / urchin2$buoyant_weight) %&gt;.% # Division par buoyant_weight log1p(.) -&gt; urchin3 # Transformation log(x + 1) head(urchin3) # diameter1 diameter2 height weight solid_parts integuments # 1 3.380877 3.386644 2.726760 1.683990 1.497119 1.388714 # 2 2.953357 2.958109 2.240741 1.587131 1.468302 1.345385 # 3 3.485925 3.451231 2.675524 1.733496 1.582091 1.478861 # 4 3.247200 3.271795 2.643585 1.749467 1.600897 1.441601 # 5 3.247291 3.306831 2.582396 1.744070 1.595668 1.424509 # 6 3.000850 2.973973 2.254397 1.690963 1.594759 1.406850 # dry_integuments digestive_tract dry_digestive_tract gonads # 1 1.030481 0.1039141 0.02667720 0.009140213 # 2 1.022630 0.1806157 0.04368131 0.040178983 # 3 1.051165 0.1683868 0.03608357 0.000000000 # 4 1.049797 0.2061975 0.04764294 0.023167059 # 5 1.048737 0.3154008 0.06613980 0.028901124 # 6 1.034084 0.3464496 0.05538973 0.016565715 # dry_gonads # 1 0.001529182 # 2 0.007821777 # 3 0.000000000 # 4 0.002174887 # 5 0.004984271 # 6 0.003104904 Refaisons notre ACP sur urchin3 ainsi calculé : urchin3_pca &lt;- pca(urchin3, scale = TRUE) summary(urchin3_pca) # Importance of components (eigenvalues): # PC1 PC2 PC3 PC4 PC5 PC6 PC7 # Variance 4.687 3.353 1.273 0.9666 0.3668 0.1724 0.10547 # Proportion of Variance 0.426 0.305 0.116 0.0879 0.0333 0.0157 0.00959 # Cumulative Proportion 0.426 0.731 0.847 0.9345 0.9678 0.9835 0.99308 # PC8 PC9 PC10 PC11 # Variance 0.04761 0.01943 0.00834 0.00068 # Proportion of Variance 0.00433 0.00177 0.00076 0.00006 # Cumulative Proportion 0.99741 0.99918 0.99994 1.00000 # # Loadings (eigenvectors, rotation matrix): # PC1 PC2 PC3 PC4 PC5 PC6 PC7 # diameter1 -0.425 0.145 -0.297 -0.101 # diameter2 -0.425 -0.101 0.143 -0.296 -0.103 # height -0.427 0.131 -0.300 # weight 0.189 -0.428 0.216 0.497 -0.672 -0.145 # solid_parts -0.495 0.254 -0.117 0.335 -0.245 # integuments -0.142 -0.463 0.152 0.283 0.165 0.345 0.667 # dry_integuments -0.259 -0.242 0.173 0.533 -0.669 -0.161 -0.277 # digestive_tract 0.214 -0.370 -0.448 -0.102 0.388 -0.462 # dry_digestive_tract -0.360 -0.485 -0.401 -0.429 -0.338 0.382 # gonads 0.374 0.438 -0.257 -0.223 # dry_gonads 0.371 0.440 -0.269 -0.162 -0.122 # PC8 PC9 PC10 PC11 # diameter1 0.101 0.400 0.714 # diameter2 0.425 -0.700 # height 0.141 0.197 -0.790 # weight 0.109 # solid_parts -0.662 -0.223 # integuments 0.266 # dry_integuments # digestive_tract 0.468 0.148 # dry_digestive_tract -0.155 # gonads 0.725 0.130 # dry_gonads 0.446 -0.589 chart$scree(urchin3_pca) Maintenant que l’effet saturant est éliminé, la répartition des variances sur les axes se fait mieux. L’axe PC1 contraste les diamètres avec les gonades, l’axe PC2 représente les masses somatiques (dans l’ordre inverse), et l’axe PC3 contraste de manière intéressante les masses du tube digestif avec celles des gonades (le tout en ratios sur la masse immergée, ne l’oublions pas). Les deux premiers axes reprennent 73% de la variance, mais il semble qu’un effet intéressant se marque également sur PC3 avec 85% de la variance totale sur les trois premiers axes. Tout ceci est également visible sur les graphiques dans l’espace des variables (plans PC1 - PC2 et PC2 - PC3 représentés ici). chart$loadings(urchin3_pca, choices = c(1, 2)) chart$loadings(urchin3_pca, choices = c(2, 3)) Enfin, dans l’espace des individus, avec l’origine reprise en couleur, nous observons ceci dans le prmeier plan de l’ACP : chart$scores(urchin3_pca, choices = c(1, 2), col = urchin2$origin, labels = urchin2$maturity, aspect.ratio = 3/5) + theme(legend.position = &quot;right&quot;) + stat_ellipse() Et pour le plan PC2 - PC3 : chart$scores(urchin3_pca, choices = c(2, 3), col = urchin2$origin, labels = urchin2$maturity, aspect.ratio = 3/5) + theme(legend.position = &quot;right&quot;) + stat_ellipse() Vous devriez pouvoir interpréter ces résultats par vous-même maintenant. 7.1.4 Visualisation de données quantitatives 7.1.4.1 Deux dimensions Le nuage de points est le graphe idéal pour visualiser la distribution des données bivariées pour deux vaeriavbles quantitatives. Il permet de visualiser également une association entre deux variables. Il permet aussi de visualiser comment deux ou plusieurs groupes peuvent être séparés en fonction de ces deux variables. chart(data = pima, glucose ~ insulin %col=% diabetes) + geom_point() 7.1.4.2 Trois dimensions Le nuage de points en pseudo-3D est l’équivalent pour visualiser trois variables quantitatives simultanément. Il est nécessaire de rendre l’effet de la troisième dimension (perspective, variation de taille des objets, …). La possibilité de faire tourner l’objet 3D virtuel est indispensable pour concrétiser l’effet 3D et pour le visionner sous différents angles Le package rgl permet de réaliser ce genre de graphique 3D interactif (que vous pouvez faire tourner dans l’orientation que vous voulez à la souris) : rgl::plot3d(pima$insulin, pima$glucose, pima$triceps, col = as.integer(pima$diabetes)) 7.1.4.3 Plus de trois dimensions Déjà à trois dimensions la visualisation devient délicate, mais au delà, cela devient pratiquement mission impossible. La matrice de nuages de points peut rendre service ici, mais dans certaines limites (tous les angles de vue ne sont pas accessibles). GGally::ggscatmat(pima, 2:6, color = &quot;diabetes&quot;) Nous voyons qu’ici nous atteignons les limites des possibilités. C’est pour cela que, pour des données multivariées comportant beaucoup de variables quantitatives, les techniques de réduction des dimensions comme l’ACP sont indispensables. 7.1.5 ACP : mécanisme Nous allons partir d’un exemple presque trivial pour illuster le principe de l’ACP. Comment réduire un tableau bivarié en une représentation des individus en une seule dimension (classement sur une droite) avec perte minimale d’information ? Par exemple, en partant de ces données fictives : Voic une représentation graphique 2D de ces données : Si nous réduisons à une seule dimension en laissant tomber une des deux variables, voici ce que cela donne (ici on ne garde que Var1, donc, on projette les points sur l’axe des abscisses). Au final, nous avons ordonné nos individus en une dimension comme suit : C’est une mauvaise solution car il y a trop de perte d’information. Regardez l’écart entre 7 et 9 sur le graphqie en deux dimensions et dans celui à une dimension : les points sont trop près. Comparez sur les deux graphiques les distances 7 - 9 avec 9 - 8 et 1 - 2 versus 1 - 3. Tout cela est très mal représenté en une seule dimension. Une autre solution serait de projeter le long de la droite de “tendance générale”, c’est-à-dire le long de l’axe de plus grand allongement du nuage de points. Cela donne ceci en une seule dimension : C’est une bien meilleure solution car la perte d’information est ici minimale. Regardez à nouveau la distance entre 7 et 9 sur le graphique initial à deux dimensions et sur le nouveau graphique réduit à une dimension : c’est mieux qu’avant. Comparez aussi les distances respectives entre les paires 7 - 9 et 9 - 8, ainsi que 1 - 2 par rapport à 1 - 3. Tout cela est bien pieux représenté à présent. L’ACP effectue précisément la projection que nous venons d’imaginer. La droite de projection est appelée composante principale 1. La composante principale 1 présente la plus grande variabilité possible sur un seul axe. Ensuite on calcule la composante 2 comme étant orthogonale (i.e., perpendiculaire) à PC1 et présentant la plus grande variabilité non encore capturée par la composante 1. Le mécanisme revient à projeter les points sur des axes orientés différemment dans l’e plan’espace à N dimensions (pour N variables intiales). En effet, mathématiquement ce mécanisme se généralise facilement à trois, puis à N dimensions. 7.1.6 Calcul matriciel ACP La rotation optimale des axes vers les PC1 à PCN se résoud par un calcul matriciel. Nous allons maintenant le détailler. Mais auparavant, nous devons nous rafraîchir l’esprit concernant quelques notions. Multiplication matricielle : \\(\\begin{pmatrix} 2 &amp; 3\\\\ 2 &amp; 1 \\end{pmatrix} \\times \\begin{pmatrix} 1\\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 11\\\\ 5 \\end{pmatrix}\\) Vecteurs propres et valeurs propres (il en existe autant qu’il y a de colonnes dans la matrice de départ) : \\[ \\begin{pmatrix} 2 &amp; 3\\\\ 2 &amp; 1 \\end{pmatrix} \\times \\begin{pmatrix} 3\\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 12\\\\ 8 \\end{pmatrix} = 4 \\times \\begin{pmatrix} 3\\\\ 2 \\end{pmatrix} \\] La constante (4) est une valeur propre et la matrice multipliée (à droite) est la matrice des vecteurs propres. La rotation d’un système d’axes à deux dimensions d’un angle \\(\\alpha\\) peut se représenter sous forme d’un calcul matriciel : \\[ \\begin{pmatrix} \\cos \\alpha &amp; \\sin \\alpha\\\\ -\\sin \\alpha &amp; \\cos \\alpha \\end{pmatrix} \\times \\begin{pmatrix} x\\\\ y \\end{pmatrix} = \\begin{pmatrix} x&#39;\\\\ y&#39; \\end{pmatrix} \\] Dans le cas particulier de l’ACP, la matrice de transformation qui effectue la rotation voulue pour obtenir les axes principaux est la matrice rassemblant tous les vecteurs propres calculés après diagonalisation de la matrice de corrélation ou de variance/covariance (réduction ou non, respectivement). Le schéma suivant visualise la rotation depuis les axes initiaux X et Y (variables de départ) en bleu royal vers les PC1, PC2 en rouge. Un individu p est représenté par les coordonnées {x, y} dans le système d’axes initial XY. Les nouvelles coordonnées {x’, y’} sont recalculées par projection sur les nouveaux axes PC1-PC2. Les flèches bleues sont représentées dans l’espace des variables, tandis que les points reprojettés sur PC1-PC2 sont représentés dans l’espace des individus selon les coordonnées primes en rouge. 7.1.6.1 Résolution numérique simple Effectuons une ACP sur matrice var/covar sans réduction des données (mais calcul très similaire lorsque les données sont réduites) sur un exemple numérique simple. Étape 1 : centrage des données \\[ \\mathop{\\begin{pmatrix} 2 &amp; 1 \\\\ 3 &amp; 4 \\\\ 5 &amp; 0 \\\\ 7 &amp; 6 \\\\ 9 &amp; 2 \\end{pmatrix}}_{\\text{Tableau brut}} \\xrightarrow{\\phantom{---}\\text{centrage}\\phantom{---}} \\mathop{\\begin{pmatrix} -3.2 &amp; -1.8 \\\\ -2.2 &amp; \\phantom{-}1.4 \\\\ -0.2 &amp; -2.6 \\\\ \\phantom{-}1.8 &amp; \\phantom{-}3.4 \\\\ \\phantom{-}3.8 &amp; -0.6 \\end{pmatrix}}_{\\text{Tableau centré (X)}} \\] Étape 2 : calcul de la matrice de variance/covariance \\[ \\mathop{\\begin{pmatrix} -3.2 &amp; -1.8 \\\\ -2.2 &amp; \\phantom{-}1.4 \\\\ -0.2 &amp; -2.6 \\\\ \\phantom{-}1.8 &amp; \\phantom{-}3.4 \\\\ \\phantom{-}3.8 &amp; -0.6 \\end{pmatrix}}_{\\text{Tableau centré (X)}} \\xrightarrow{\\phantom{---}\\text{var/covar}\\phantom{---}} \\mathop{\\begin{pmatrix} 8.2 &amp; 1.6 \\\\ 1.6 &amp; 5.8 \\end{pmatrix}}_{\\text{Matrice carrée (A)}} \\] Étape 3 : diagonalisation de la matrice var/covar \\[ \\mathop{\\begin{pmatrix} 8.2 &amp; 1.6 \\\\ 1.6 &amp; 5.8 \\end{pmatrix}}_{\\text{Matrice carrée (A)}} \\xrightarrow{\\phantom{---}\\text{diagonalisation}\\phantom{---}} \\mathop{\\begin{pmatrix} 9 &amp; 0 \\\\ 0 &amp; 5 \\end{pmatrix}}_{\\text{Matrice diagonalisée (B)}} \\] La trace des deux matrices A et B (somme des éléments sur la diagonale) est égale à : 8.2 + 5.8 = 14 = 9 + 5. 8.2 est la part de variance exprimée sur le premier axe initial (X) 5.8 est la part de variance exprimée sur le second axe initial (Y) 14 est la variance totale du jeu de données La matrice diagonale B est la solution exprimant la plus grande part de variance possible sur le premier axe de l’ACP : 9, soit 64,3% de la variance totale. Les éléments sur la diagonale sont les valeurs propres \\(\\lambda_i\\) ! Vous vous rappelez les fameuses “eigenvalues” dans la sortie de summary(pima_pca). Étape 4 : calcul de la matrice de rotation des axes (en utilisant la propriété des valeurs propres \\(\\text{A}.\\text{U} = \\text{B}.\\text{U}\\)). \\[ \\mathop{\\begin{pmatrix} 8.2 &amp; 1.6 \\\\ 1.6 &amp; 5.8 \\end{pmatrix}}_{\\text{Matrice A}} \\times \\text{U} = \\mathop{\\begin{pmatrix} 9 &amp; 0 \\\\ 0 &amp; 5 \\end{pmatrix}}_{\\text{Matrice B}} \\times \\text{U} \\rightarrow \\text{U} = \\mathop{\\begin{pmatrix} \\phantom{-}0.894 &amp; -0.447 \\\\ \\phantom{-}0.447 &amp; \\phantom{-}0.894 \\end{pmatrix}}_{\\text{Matrice des vecteur propres (U)}} \\] La matrice des vecteurs propres (U) (“eigenvectors” en anglais) effectue la transformation (rotation des axes) pour obtenir les composantes principales. L’angle de rotation se déduit en considérant que cette matrice contient des sin et cos d’angles de rotation des axes : \\[ \\begin{pmatrix} \\phantom{-}0.894 &amp; -0.447 \\\\ \\phantom{-}0.447 &amp; \\phantom{-}0.894 \\end{pmatrix} = \\begin{pmatrix} \\phantom{-}\\cos(-26.6°) &amp; \\phantom{-}\\sin(-26.6°) \\\\ -\\sin(-26.6°) &amp; \\phantom{-}\\cos(-26.6°) \\end{pmatrix} \\] Étape 5 : représentation dans l’espace des variables. C’est une représentation dans un cercle de la matrice des vecteurs propres U sous forme de vecteurs. Étape 6 : représentation dans l’espace des individus. On recalcule les coordonnées des individus dans le système d’axe après rotation. \\[ \\mathop{\\begin{pmatrix} -3.2 &amp; -1.8 \\\\ -2.2 &amp; \\phantom{-}1.4 \\\\ -0.2 &amp; -2.6 \\\\ \\phantom{-}1.8 &amp; \\phantom{-}3.4 \\\\ \\phantom{-}3.8 &amp; -0.6 \\end{pmatrix}}_{\\text{Tableau centré (X)}} \\times \\mathop{\\begin{pmatrix} \\phantom{-}0.894 &amp; -0.447 \\\\ \\phantom{-}0.447 &amp; \\phantom{-}0.894 \\end{pmatrix}}_{\\text{Matrice des vecteur propres (U)}} \\xrightarrow{\\phantom{---}\\text{X}.\\text{U} = \\text{X&#39;}\\phantom{---}} \\mathop{\\begin{pmatrix} -3.58 &amp; \\phantom{-}0.00 \\\\ -1.34 &amp; \\phantom{-}2.24 \\\\ -1.34 &amp; -2.24 \\\\ \\phantom{-}3.13 &amp; \\phantom{-}2.24 \\\\ \\phantom{-}3.13 &amp; -2.24 \\end{pmatrix}}_{\\text{Tableau avec rotation (X&#39;)}} \\] Ensuite, on représente ces individus à l’aide d’un graphique en nuage de points. Tout ces calculs se généralisent facilement à trois, puis à N dimensions. Pour aller plus loin N’hésitez pas à combiner plusieurs techniques. Par exemple, vous pouvez représenter les groupes créés par classification ascendante hiérarchiques sur un graphique de l’ACP dans l’espace des individus en faisant varier les couleurs ou les labels des individus en fonction des groupes de la CAH. Une autre explication de l’ACP en utilsant quelques autres fonctions de R pour visualiser le résultat Une explication détaillée de la PCA en anglais. Une page qui reprend une série de vidéos qui présentent les différentes facettes de l’A CP (en franglais). "],
["analyse-factorielle-des-correspondances.html", "7.2 Analyse factorielle des correspondances", " 7.2 Analyse factorielle des correspondances Comme l’ACP s’intéresse à des corrélations linéaires entre variables quantitatives, elle n’est absolument pas utilisable pour traiter des variables qualitatives. L’Analyse Factorielle des Correspondances sera utile dans ce dernier cas (AFC, ou en anglais “Correspondence Analysis” ou CA). 7.2.1 AFC dans SciViews::R L’AFC utilises la fonction ca() du package ca dans SciViews::R, mais au stade actuel, tout le code nécessaire (en particulier pour réaliser les graphiques avec chart()) n’est pas encore complètement intégré dans les packages. Ainsi, vous pouvez copier-coller le code du chunk suivant au début de vos scripts ou dans un chunk de setup dans vos documenbts R Markdown/Notebook. SciViews::R library(broom) # Au lieu de MASS::corresp(, nf = 2), nous préférons ca::ca() ca &lt;- ca::ca scale_axes &lt;- function(data, aspect.ratio = 1) { range_x &lt;- range(data[, 1]) span_x &lt;- abs(max(range_x) - min(range_x)) range_y &lt;- range(data[, 2]) span_y &lt;- abs(max(range_y) - min(range_y)) if ((span_y / aspect.ratio) &gt; span_x) { # Adjust range_x span_x_2 &lt;- span_y / aspect.ratio / 2 range_x_mid &lt;- sum(range_x) / 2 range_x &lt;- c(range_x_mid - span_x_2, range_x_mid + span_x_2) } else { # Adjust range_y span_y_2 &lt;- span_x * aspect.ratio / 2 range_y_mid &lt;- sum(range_y) / 2 range_y &lt;- c(range_y_mid - span_y_2, range_y_mid + span_y_2) } list(x = range_x, y = range_y) } plot3d &lt;- rgl::plot3d plot3d.ca &lt;- ca:::plot3d.ca autoplot.ca &lt;- function(object, choices = 1L:2L, type = c(&quot;screeplot&quot;, &quot;altscreeplot&quot;, &quot;biplot&quot;), col = &quot;black&quot;, fill = &quot;gray&quot;, aspect.ratio = 1, repel = FALSE, ...) { type = match.arg(type) res &lt;- switch(type, screeplot = object %&gt;.% # Classical screeplot `[[`(., &quot;sv&quot;) %&gt;.% tibble(Dimension = 1:length(.), sv = .) %&gt;.% chart(data = ., sv^2 ~ Dimension) + geom_col(col = col, fill = fill) + labs(y = &quot;Inertia&quot;), altscreeplot = object %&gt;.% # screeplot represented by dots and lines `[[`(., &quot;sv&quot;) %&gt;.% tibble(Dimension = 1:length(.), sv = .) %&gt;.% chart(data = ., sv^2 ~ Dimension) + geom_line(col = col) + geom_point(col = &quot;white&quot;, fill = col, size = 2, shape = 21, stroke = 3) + labs(y = &quot;Inertia&quot;), biplot = { # We want to use the function plot.ca(), but without plotting the base plot # So, we place it in a specific environment where all base plot functions are # fake and do nothing (we just want to collect points coordinates at the end) env &lt;- new.env() env$plot_ca &lt;- ca:::plot.ca environment(env$plot_ca) &lt;- env env$plot &lt;- function(...) NULL env$box &lt;- function(...) NULL env$abline &lt;- function(...) NULL env$axis &lt;- function(...) NULL env$par &lt;- function(...) NULL env$points &lt;- function(...) NULL env$lines &lt;- function(...) NULL env$.arrows &lt;- function(...) NULL env$text &lt;- function(...) NULL env$strwidth &lt;- function(...) NULL env$strheight &lt;- function(...) NULL contribs &lt;- paste0(&quot;Dimension &quot;, 1:length(object$sv), &quot; (&quot;, round(object$sv^2 / sum(object$sv^2) * 100, 1), &quot;%)&quot;)[choices] res &lt;- env$plot_ca(object, dim = choices, ...) rows &lt;- as.data.frame(res$rows) rows$Type &lt;- &quot;rows&quot; rows$Labels &lt;- object$rownames cols &lt;- as.data.frame(res$cols) cols$Type &lt;- &quot;cols&quot; cols$Labels &lt;- object$colnames res &lt;- bind_rows(rows, cols) names(res) &lt;- c(&quot;x&quot;, &quot;y&quot;, &quot;type&quot;, &quot;labels&quot;) lims &lt;- scale_axes(res, aspect.ratio = aspect.ratio) nudge &lt;- (lims$x[2] - lims$x[1]) / 100 res &lt;- chart(data = res, y ~ x %col=% type %label=% labels) + geom_hline(yintercept = 0, col = &quot;gray&quot;) + geom_vline(xintercept = 0, col = &quot;gray&quot;) + coord_fixed(ratio = 1, xlim = lims$x, ylim = lims$y, expand = TRUE) + theme(legend.position = &quot;none&quot;) + labs(x = contribs[1], y = contribs[2]) if (isTRUE(repel)) { res &lt;- res + geom_point() + ggrepel::geom_text_repel() } else {# Use text res &lt;- res + geom_point() + geom_text(hjust = 0, vjust = 0, nudge_x = nudge, nudge_y = nudge) } res } ) res } chart.ca &lt;- function(data, choices = 1L:2L, ..., type = c(&quot;screeplot&quot;, &quot;altscreeplot&quot;, &quot;boiplot&quot;), env = parent.frame()) autoplot.ca(data, choices = choices, ..., type = type, env = env) class(chart.ca) &lt;- c(&quot;function&quot;, &quot;subsettable_type&quot;) "],
["afm.html", "Module 8 AFM", " Module 8 AFM Objectifs TODO Prérequis TODO "],
["analyse-factorielle-multiple-afm.html", "8.1 Analyse factorielle multiple (AFM)", " 8.1 Analyse factorielle multiple (AFM) L’analyse factorielle multiple (AFM) se nomme principal component analysis (PCA) en anglais. "],
["svbox.html", "A Installation de la SciViews Box", " A Installation de la SciViews Box Pour ce cours SDD 2, nous utiliserons la même SciViews Box que pour le cours 1… mais actualisée (version de l’année). Vous allez donc devoir installer la nouvelle version. La procédure n’a changé que sur des points de détails. Référez-vous à l’appendice A1 du cours SDD 1. Vous pouvez conserver l’ancienne SciViews Box en parallèle avec cette nouvelle version, mais vérifiez si vous avez assez d’espace sur le disque dur pour contenir les deux simultanément. Comptez par sécurité 20Go par version. Si vous manquez de place, vous pouvez éliminer l’ancienne version avant d’installer la nouvelle (vos projets ne seront pas effacés). "],
["migration-des-projets.html", "A.1 Migration des projets", " A.1 Migration des projets Concernant les projets réalisés dans une version précédente de la SciViews Box, ceux-ci restent disponibles, même si vous éliminez l’ancienne. Plusieurs cas de figure se présentent : Vous conserver deux ou plusieurs version de la SciViews Box en parallèle. Dans ce cas, nous conseillons fortement de garder chaque projet accessible à partir de la version dans laquelle il a été créé. Seulement les projets que vous décidez de migrer explicitement (voir ci-dessous) seront à déplacer dans le dossier shared de la nouvelle SciViews Box. Vous aurez à faire cette manipulation, par exemple, si vous devez recommencer un cours l’année suivante afin d’être en phase (même version de la svbox) par rapport à vos nouveaux collègues. Vous ne conservez que la dernière version de la SciViews Box, mais ne devez pas accéder fréquemment vos anciens projets, et dans ce cas, vous pouvez réinstaller temporairement l’ancienne version de svbox. Dans ce cas, ne migrez pas vos anciens projets. Éliminez simplement l’ancienne svbox, tout en laisant vos projets intacts dans son répertoire shared. Lors de la réinstallation de l’ancienne svbox, vous retrouverez alors tous vos anciens projets intactes. Vous ne conservez pas d’ancienne version de la svbox et vous ne souhaitez pas devoir la réinstaller. Il est possible de migrer vos anciens projets en les déplaçant de l’ancien répertoire shared vers le nouveau. Soyez toutefois conscients que vos documents R Markdown et scripts R ne fonctionneront pas forcément dans la nouvelle svbox et qu’une adaptation sera peut-être nécessaire ! "],
["configuration-git-et-github.html", "A.2 Configuration Git et Github", " A.2 Configuration Git et Github A chaque nouvelle installation de la SciViews Box, vous devez la reconfigurer via la boite de dialogue SciViews Box Configuration. En particulier, il est très important d’indiquer correctement votre identifiant et email Git (zone encadrée en rouge dans la copie d’écran ci-dessous). Assurez-vous (si ce n’est déjà fait) que vous possédez un compte Github valide. Vous pouvez cliquer sur le bouton Go to Github par facilté dans la même boite de dialogue. Choisissez de manière judicieuse votre login. Vous pourriez être amenés à l’utiliser bien plus longtemps que vous ne le pensez, y compris plus tard dans votre carrière. Donc, lisez les conseils ci-dessous (inspirés et adaptés de Happy Git and Github for the UseR - Register a Github Account : Incluez votre nom réel. Les gens aiment savoir à qui ils ont affaire. Rendez aussi votre nom/login facile à deviner et à retenir. Philippe Grosjean a comme login phgrosjean, par exemple. Vous pouvez réutiliser votre login d’autres contextes, par exemple Twitter ou Slack (ou Facebook). Choisissez un login que vous pourrez échanger de manière confortable avec votre futur boss. Un login plus court est préférable. Soyez unique dans votre login, mais à l’aide d’aussi peu de caractères que possible. Github propose parfois des logins en auto-complétion. Examinez ce qu’il propose. Rendez votre login invariable dans le temps. Par exemple, n’utilisez pas un login lié à votre université (numéro de matricule, ou nom de l’université inclue dans le login). Si tout va bien votre login vous suivra dans votre carrière, … donc, potentiellement loin de l’université où vous avez fait vos études. N’utilisez pas de logins qui sont aussi des mots ayant une signification particulière en programmation, par exemple, n’utilisez pas NA, même si c’est vos initiales ! Une fois votre compte Github créé, et votre login/email pour votre identification Git correctement enregistrés dans la SciViews Box, vous devez pouvoir travailler, faire des “pushs”, des “pulls” et des “commits”20. Cependant, RStudio vous demandera constamment vos logins et mots de passe… à la longue, c’est lassant ! La procédure ci-dessous vous enregistre une fois pour toutes sur votre compte Github dans RStudio. A.2.1 Compte Github dans RStudio RStudio offre la possibilité d’enregistrer une clé publique/privée dans votre SciViews Box afin de vous enregistrer sur Github de manière permanente. L’avantage, c’est que vous ne devrez plus constamment entrer votre login et mot de passe à chaque opération sur Github ! Nous vous le conseillons donc vivement. Entrez dans Rstudio Server, et allez dans le menu Tools -&gt; Global Options.... Ensuite, cliquez dans la rubrique Git/SVN dans la boite de dialogue. Ensuite, cliquez sur le bouton Create RSA key.... La phrase de passe n’est pas nécessaire (il est même préférable de la laisser vide si vous voulez utiliser Github sans rien devoir taper à chaque fois). Cliquez sur le bouton Create. Vous obtenez alors une fenêtre similaire à celle ci-dessous (bien sûr avec des données différentes). Ceci confirme que votre clé cryptographique a été créée localement. Fermez cette fenêtre pour revenir à la boite de dialogue de configuration de RStudio Server. Dans la boite de dialogue de configuration de RStudio Server, section Git/SVN cliquez sur le lien View public key qui apparait une fois la clé créée : La clé apparait dans une fenêtre, déjà présélectionnée. Copiez-là dans le presse-papier (Ctrl-C ou clic bouton droit et sélection de Copy dans le menu contextuel), puis fermez cette fenêtre. Dans votre navigateur web favori, naviguez vers https://github.com, loggez-vous, et accédez aux paramètres de votre compte Github (menu déroulant en haut à droite, entrée Settings) : Dans les paramètres de votre compte, cliquez sur la rubrique SSH and GPG keys, ensuite sur le bouton vert New SSH key Collez-y votre clé à partir du presse-papier dans la zone Key. Vous pouvez lui donner un nom évocateur dans le champ Title. Ensuite, cliquez sur Add SSH key. Déloggez, puis reloggez-vous dans RStudio Server pour que les changements soient pris en compte. La prochaine action sur Github depuis RStudio pourrait encore déclencher la demande de votre login et mot de passe, mais ensuite, les opérations devraient se faire directement. Si vous éprouvez toujours des difficultés à faire collaborer R et RStudio avec Git et Github, voyez https://happygitwithr.com (en anglais) qui explique les différentes procédures bien plus en détails. Vérifiez toujours lors de votre premier commit que Github vous reconnait bien. Pour cela, naviguez vers le dépôt où vous avez commité avec votre explorateur web, et vérifiez l’identité prise en compte lors de votre commit.↩ "],
["references.html", "Références", " Références "]
]
